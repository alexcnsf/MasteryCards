Chapter 
2Conditional Probability 
2.1 The Deﬁnition ofConditional Probability 
2.2Independent Events
2.3Bayes’ Theorem 2.4The Gambler’s Ruin Problem 
2.5SupplementaryExercises
2.1 The Deﬁnition of Conditional Probability 
Amajoruseofprobabilityinstatisticalinferenceistheupdatingofprobabilities 
when certain events are observed. The updated probability of event Aafter we 
learnthatevent Bhasoccurredistheconditionalprobabilityof AgivenB.
Example 
2.1.1 Lottery Ticket . Considerastatelotterygameinwhichsixnumbersaredrawnwithout 
replacementfromabincontainingthenumbers1–30.Eachplayertriestomatchthe 
setofsixnumbersthatwillbedrawnwithoutregardtotheorderinwhichthenumbers 
are drawn. Suppose that you hold a ticket in such a lottery with the numbers 1, 14, 
15,20,23,and27.Youturnonyourtelevisiontowatchthedrawingbutallyouseeis 
onenumber,15,beingdrawnwhenthepowersuddenlygoesoffinyourhouse.You 
don’t even know whether 15 was the ﬁrst, last, or some in-between draw. However, 
now that you know that 15 appears in the winning draw, the probability that your 
ticket is a winner must be higher than it was before you saw the draw. How do you 
calculatetherevised probability? ◭
Example 2.1.1 is typical of the following situation. An experiment is performed 
for which the sample space Sis given (or can be constructed easily) and the proba- 
bilitiesareavailableforalloftheeventsofinterest.Wethenlearnthatsomeevent B
hasoccuured,andwewanttoknowhowtheprobabilityofanotherevent Achanges
afterwelearnthat Bhasoccurred.InExample2.1.1,theeventthatwehavelearned 
is B={oneofthe numbers drawnis 15 }.Wearecertainlyinterestedintheprobabil- 
ityof 
A={thenumbers 1, 14, 15, 20, 23, and 27 aredrawn },
andpossibly otherevents.
If we know that the event Bhas occurred, then we know that the outcome of 
theexperimentisoneofthoseincludedin B.Hence,toevaluatetheprobabilitythat 
Awill occur, we must consider the set of those outcomes in Bthat also result in 
the occurrence of A. As sketched in Fig. 2.1, this set is precisely the set A∩B. It is 
thereforenaturaltocalculatetherevisedprobabilityof Aaccordingtothefollowing 
deﬁnition.
55 
56 Chapter 2 ConditionalProbability 
Figure 2.1 The outcomes in 
the event Bthat also belong
tothe event A.ABS
A/H20669B
Deﬁnition
2.1.1 Conditional Probability . Supposethatwelearnthatanevent Bhasoccurredandthat
we wish to compute the probability of another event Ataking into account that
we know that Bhas occurred. The new probability of Ais called the conditional
probabilityoftheevent Agiventhattheevent Bhasoccurred andisdenotedPr (A |B) .
IfPr (B)> 0, we compute this probability as 
Pr (A |B) =Pr (A ∩B) 
Pr (B) . (2.1.1)
The conditional probability Pr (A |B) is notdeﬁned ifPr (B) =0. 
For convenience, the notation in Deﬁnition 2.1.1 is read simply as the conditional 
probability of AgivenB. Eq. (2.1.1) indicates that Pr (A |B) is computed as the 
proportionofthetotalprobabilityPr (B) thatisrepresentedbyPr (A ∩B) ,intuitively 
theproportion of Bthat is alsopartof A.
Example 
2.1.2 Lottery Ticket . InExample 2.1.1, youlearned thatthe event 
B={oneofthe numbers drawn is 15 }
has occurred. You want to calculate the probability of the event Athat your ticket
isawinner.Bothevents AandBareexpressibleinthesamplespacethatconsistsof 
the/parenleftbig30 
6/parenrightbig
=30!/( 6!24! )possible combinations of 30 items taken six at a time, namely, 
theunordereddrawsofsixnumbersfrom1–30.Theevent Bconsistsofcombinations 
thatinclude15.Sincethereare29remainingnumbersfromwhichtochoosetheother 
ﬁvein the winning draw, there are /parenleftbig29 
5/parenrightbig
outcomesin B. Itfollows that 
Pr (B) =/parenleftbig29 
5/parenrightbig
/parenleftbig30 
6/parenrightbig=29!24!6! 
30!5!24! =0.2.
Theevent Athatyourticketisawinnerconsistsofasingleoutcomethatisalsoin B,
so A∩B=A, and 
Pr (A ∩B) =Pr (A)=1/parenleftbig30 
6/parenrightbig=6!24! 
30!=1.68 ×10 −6.
Itfollows thatthe conditional probability of AgivenBis 
Pr (A |B) =6!24! 
30!
0.2=8.4×10 −6.
This is ﬁve times as large as Pr (A)before youlearned that Bhadoccurred. ◭
Deﬁnition 2.1.1 for the conditional probability Pr (A |B) is worded in terms of 
the subjective interpretation of probability in Sec. 1.2. Eq. (2.1.1) also has a simple 
meaning in terms of the frequency interpretation of probability. According to the 
2.1 TheDeﬁnition ofConditionalProbability 57 
frequency interpretation, if an experimental process is repeated a large number of 
times, then the proportion of repetitions in which the event Bwill occur is approx- 
imately Pr (B) and the proportion of repetitions in which both the event Aand the
eventBwill occur is approximately Pr (A ∩B) . Therefore, among those repetitions 
in which the event Boccurs, the proportion of repetitions in which the event Awill
also occuris approximately equal to 
Pr (A |B) =Pr (A ∩B) 
Pr (B) .
Example 
2.1.3 Rolling Dice . Supposethattwodicewererolledanditwasobservedthatthesum Tof 
thetwonumberswasodd.Weshalldeterminetheprobabilitythat Twaslessthan8. 
If we let Abe the event that T < 8 and let Bbe the event that Tis odd, then 
A∩Bistheeventthat Tis3 ,5,or7.Fromtheprobabilitiesfortwodicegivenatthe 
endofSec. 1.6, we can evaluate Pr (A ∩B) andPr (B) as follows: 
Pr (A ∩B) =2
36 +4
36 +6
36 =12 
36 =1
3,
Pr (B) =2
36 +4
36 +6
36 +4
36 +2
36 =18 
36 =1
2.
Hence,
Pr (A |B) =Pr (A ∩B) 
Pr (B) =2
3. ◭
Example 
2.1.4 A Clinical Trial . It is very common for patients with episodes of depression to have 
a recurrence within two to three years. Prien et al. (1984) studied three treatments 
for depression: imipramine, lithium carbonate, and a combination. As is traditional 
insuchstudies(called clinicaltrials ),therewasalsoagroupofpatientswhoreceived 
a placebo. (A placebo is a treatment that is supposed to be neither helpful nor 
harmful. Some patients are given a placebo so that they will not know that they 
did not receive one of the other treatments. None of the other patients knew which 
treatment or placebo they received, either.) In this example, we shall consider 150 
patients who entered the study after an episode of depression that was classiﬁed 
as “unipolar” (meaning that there was no manic disorder). They were divided into 
the four groups (three treatments plus placebo) and followed to see how many had 
recurrencesofdepression.Table2.1summarizestheresults.Ifapatientwereselected 
at random from this study and it were found that the patient received the placebo 
treatment, what is the conditional probability that the patient had a relapse? Let 
Bbe the event that the patient received the placebo, and let Abe the event that 
Table 2.1 Resultsofthe clinicaldepression study in Example 2.1.4 
Treatmentgroup 
Response Imipramine Lithium Combination Placebo Total 
Relapse 18 13 22 24 77 
No relapse 22 25 16 10 73 
Total 40 38 38 34 150 
58 Chapter 2 ConditionalProbability 
thepatienthadarelapse.WecancalculatePr (B) =34 /150 andPr (A ∩B) =24 /150
directly from the table. Then Pr (A |B) =24 /34 =0.706. On the other hand, if the 
randomly selected patient is found to have received lithium (call this event C) then 
Pr (C) =38 /150,Pr (A ∩C) =13 /150,andPr (A |C) =13 /38 =0.342.Knowingwhich 
treatmentapatientreceivedseemstomakeadifferencetotheprobabilityofrelapse. 
In Chapter 10, we shall study methods for being more precise about how much of a 
difference itmakes. ◭
Example 
2.1.5 Rolling Dice Repeatedly . Suppose that two dice are to be rolled repeatedly and the 
sumTof the two numbers is to be observed for each roll. We shall determine the 
probability pthatthevalue T=7willbeobservedbeforethevalue T=8isobserved. 
The desired probability pcould be calculated directly as follows: We could 
assumethatthesamplespace Scontainsallsequencesofoutcomesthatterminateas 
soon as either the sum T=7 or the sum T=8 is obtained. Then we could ﬁnd the 
sum of the probabilities of all the sequences that terminate when the value T=7 is 
obtained.
However,thereisasimplerapproachinthisexample.Wecanconsiderthesimple 
experimentinwhichtwodicearerolled.Ifwerepeattheexperimentuntileitherthe 
sumT=7 or the sum T=8 is obtained, the effect is to restrict the outcome of the 
experimenttooneofthesetwovalues.Hence,theproblemcanberestatedasfollows: 
Given that the outcome of the experiment is either T=7 or T=8, determine the 
probability pthatthe outcomeis actually T=7. 
If we let Abe the event that T=7 and let Bbe the event that the value of Tis 
either 7 or 8, then A∩B=Aand
p=Pr (A |B) =Pr (A ∩B) 
Pr (B) =Pr (A)
Pr (B) .
From the probabilities for two dice given in Example 1.6.5, Pr (A)=6/36 and 
Pr (B) =(6/36 )+(5/36 )=11 /36. Hence, p=6/11. ◭
The Multiplication Rule for Conditional Probabilities 
In some experiments, certain conditional probabilities are relatively easy to assign 
directly.Intheseexperiments,itisthenpossibletocomputetheprobabilitythatboth 
oftwoeventsoccurbyapplyingthenextresultthatfollowsdirectlyfromEq.(2.1.1) 
andthe analogous deﬁnition ofPr (B |A) .
Theorem
2.1.1 Multiplication Rule for Conditional Probabilities . Let AandBbe events. If Pr (B)> 0, 
then
Pr (A ∩B) =Pr (B) Pr (A |B). 
IfPr (A)>0, then 
Pr (A ∩B) =Pr (A)Pr (B |A).
Example 
2.1.6 Selecting Two Balls . Suppose that two balls are to be selected at random, without 
replacement, from a box containing rred balls and bblue balls. We shall determine 
theprobability pthat theﬁrst ballwill be redand the second ball willbe blue. 
LetAbetheeventthattheﬁrstballisred,andlet Bbetheeventthatthesecond 
ballisblue.Obviously,Pr (A)=r/(r +b) .Furthermore,iftheevent Ahasoccurred,
then one red ball has been removed from the box on the ﬁrst draw. Therefore, the 
2.1 TheDeﬁnition ofConditionalProbability 59 
probability ofobtaining a blue ball on the second draw will be 
Pr (B |A) =b
r+b−1.
Itfollows that 
Pr (A ∩B) =r
r+b.b
r+b−1. ◭
Theprinciplethathasjustbeenappliedcanbeextendedtoanyﬁnitenumberof 
events, as statedin the following theorem. 
Theorem
2.1.2 Multiplication Rule for Conditional Probabilities . Suppose that A1, A 2, . . . , A nare
events such that Pr (A 1∩A2∩. . . ∩An−1) > 0. Then 
Pr (A 1∩A2∩. . . ∩An) (2.1.2)
=Pr (A 1)Pr (A 2|A1)Pr (A 3|A1∩A2). . . Pr (A n|A1∩A2∩. . . ∩An−1). 
Proof The productofprobabilities onthe rightside ofEq. (2.1.2) is equal to 
Pr (A 1).Pr (A 1∩A2)
Pr (A 1).Pr (A 1∩A2∩A3)
Pr (A 1∩A2). . . Pr (A 1∩A2∩. . . ∩An)
Pr (A 1∩A2. . . ∩An−1).
SincePr (A 1∩A2∩. . . ∩An−1) > 0,eachofthedenominatorsinthisproductmustbe 
positive.Allofthetermsintheproductcanceleachotherexcepttheﬁnalnumerator 
Pr (A 1∩A2∩. . . ∩An), which is the leftside ofEq. (2.1.2). 
Example 
2.1.7 Selecting Four Balls . Suppose that four balls are selected one at a time, without 
replacement, from a box containing rred balls and bblue balls ( r≥2, b≥2). We 
shalldeterminetheprobabilityofobtainingthesequenceofoutcomesred,blue,red, 
blue.
If we let Rjdenote the event that a red ball is obtained on the jth draw and let 
Bjdenotetheeventthatablueballisobtainedonthe jthdraw( j=1, . . . , 4),then 
Pr (R 1∩B2∩R3∩B4)=Pr (R 1)Pr (B 2|R1)Pr (R 3|R1∩B2)Pr (B 4|R1∩B2∩R3)
=r
r+b.b
r+b−1.r−1
r+b−2.b−1
r+b−3. ◭
Note: Conditional Probabilities Behave Just Like Probabil ities. In all of the sit- 
uations that we shall encounter in this text, every result that we can prove has a 
conditionalversiongivenanevent BwithPr (B) > 0.Justreplace allprobabilitiesby 
conditionalprobabilitiesgiven Bandreplaceallconditionalprobabilitiesgivenother
events Cby conditional probabilities given C∩B. For example, Theorem 1.5.3 says 
thatPr (A c)=1−Pr (A).ItiseasytoprovethatPr (A c|B) =1−Pr (A |B) ifPr (B) > 0. 
(See Exercises 11 and 12 in this section.) Another example is Theorem 2.1.3, which 
isaconditionalversionofthemultiplicationruleTheorem2.1.2.Althoughaproofis 
givenforTheorem2.1.3,weshallnotprovideproofsofallsuchconditionaltheorems, 
because their proofs are generally very similar to the proofs of the unconditional 
versions.
Theorem
2.1.3 Supposethat A1, A 2, . . . , A n, B areeventssuchthatPr (B) > 0andPr (A 1∩A2∩. . . ∩
An−1|B) > 0. Then 
Pr (A 1∩A2∩. . . ∩An|B) =Pr (A 1|B) Pr (A 2|A1∩B) . . . 
×Pr (A n|A1∩A2∩. . . ∩An−1∩B). (2.1.3)
60 Chapter 2 ConditionalProbability 
Proof The productofprobabilities onthe rightside ofEq.(2.1.3) is equal to 
Pr (A 1∩B) 
Pr (B) .Pr (A 1∩A2∩B) 
Pr (A 1∩B) . . . Pr (A 1∩A2∩. . . ∩AnB) 
Pr (A 1∩A2. . . ∩An−1∩B) .
SincePr (A 1∩A2∩. . . ∩An−1|B) > 0,eachofthedenominatorsinthisproductmust 
bepositive.Allofthetermsintheproductcanceleachotherexcepttheﬁrstdenom- 
inator and the ﬁnal numerator to yield Pr (A 1∩A2∩. . . ∩An∩B)/ Pr (B) , which is 
theleft sideofEq. (2.1.3). 
Conditional Probability and Partitions 
Theorem 1.4.11 shows how to calculate the probability of an event by partitioning 
the sample space into two events BandBc. This result easily generalizes to larger 
partitions, and when combined with Theorem 2.1.1 it leads to a very powerful tool 
for calculatingprobabilities.
Deﬁnition
2.1.2 Partition . Let Sdenotethesamplespaceofsomeexperiment,andconsider kevents
B1, . . . , B kin Ssuchthat B1, . . . , B karedisjointand/uniontextk
i=1Bi=S.Itissaidthatthese 
events form a partition of S.
Typically, the events that make up a partition are chosen so that an important 
sourceofuncertaintyintheproblemisreducedifwelearnwhicheventhasoccurred. 
Example 
2.1.8 Selecting Bolts . Two boxes contain long bolts and short bolts. Suppose that one box 
contains60longboltsand40shortbolts,andthattheotherboxcontains10longbolts 
and20shortbolts.Supposealsothatoneboxisselectedatrandomandaboltisthen 
selected at random from that box. We would like to determine the probability that 
this bolt is long. ◭
Partitionscan facilitatethe calculationsofprobabilities ofcertain events. 
Theorem
2.1.4 Law of total probability . Suppose that the events B1, . . . , B kform a partition of the 
space SandPr (B j) > 0 for j=1, . . . , k . Then, for everyevent Ain S,
Pr (A)=k/summationdisplay
j=1Pr (B j)Pr (A |Bj). (2.1.4)
Proof Theevents B1∩A, B 2∩A, . . . , B k∩Awillformapartitionof A,asillustrated 
in Fig. 2.2. Hence, we can write 
A=(B 1∩A) ∪(B 2∩A) ∪. . . ∪(B k∩A).
Figure 2.2 The inter- 
sections of Awith events
B1, . . . , B 5of a partition in 
the proofofTheorem2.1.4. A
B1
B5B4B3B2S
2.1 TheDeﬁnition ofConditionalProbability 61 
Furthermore, since the kevents on the rightside ofthis equation are disjoint, 
Pr (A)=k/summationdisplay
j=1Pr (B j∩A).
Finally, if Pr (B j) > 0 for j=1, . . . , k , then Pr (B j∩A) =Pr (B j)Pr (A |Bj)and it 
follows that Eq.(2.1.4) holds.
Example 
2.1.9 Selecting Bolts . In Example 2.1.8, let B1be the event that the ﬁrst box (the one with 
60 long and 40 short bolts) is selected, let B2be the event that the second box (the 
one with 10 long and 20 short bolts) is selected, and let Abe the event that a long 
bolt is selected. Then 
Pr (A)=Pr (B 1)Pr (A |B1)+Pr (B 2)Pr (A |B2). 
Since a box is selected at random, we know that Pr (B 1)=Pr (B 2)=1/2. Fur- 
thermore, the probability of selecting a long bolt from the ﬁrst box is Pr (A |B1)=
60 /100=3/5, and the probability of selecting a long bolt from the second box is 
Pr (A |B2)=10 /30 =1/3. Hence, 
Pr (A)=1
2.3
5+1
2.1
3=7
15 . ◭
Example 
2.1.10 Achieving a High Score . Supposethatapersonplaysagameinwhichhisscoremustbe 
oneofthe50numbers1 ,2, . . . , 50andthateachofthese50numbersisequallylikely 
tobehisscore.Theﬁrsttimeheplaysthegame,hisscoreis X.Hethencontinuesto 
playthegameuntilheobtainsanotherscore Ysuchthat Y≥X.Wewillassumethat, 
conditional on previous plays, the 50 scores remain equally likely on all subsequent 
plays. We shall determine theprobability ofthe event AthatY=50. 
For each i=1, . . . , 50, let Bibe the event that X=i. Conditional on Bi, the 
valueof Yisequallylikelytobeanyoneofthenumbers i, i +1, . . . , 50.Sinceeach 
ofthese (51 −i) possible values for Yis equally likely, itfollows that 
Pr (A |Bi)=Pr (Y =50 |Bi)=1
51 −i.
Furthermore,sincetheprobabilityofeachofthe50valuesof Xis1/50,itfollowsthat 
Pr (B i)=1/50 for all iand
Pr (A)=50 /summationdisplay
i=11
50 .1
51 −i=1
50 /parenleftbigg
1+1
2+1
3+. . . +1
50 /parenrightbigg
=0.0900. ◭
Note:ConditionalVersionofLawofTotalProbability. Thelawoftotalprobability 
hasan analog conditionalon another event C, namely, 
Pr (A |C) =k/summationdisplay
j=1Pr (B j|C) Pr (A |Bj∩C). (2.1.5)
The reader can prove this in Exercise 17. 
AugmentedExperiment Insomeexperiments,itmaynotbeclearfromtheinitial 
descriptionoftheexperimentthatapartitionexiststhatwillfacilitatethecalculation 
ofprobabilities.However,therearemanysuchexperimentsinwhichsuchapartition 
existsifweimaginethattheexperimenthassomeadditionalstructure.Considerthe 
following modiﬁcationofExamples 2.1.8and 2.1.9. 
62 Chapter 2 ConditionalProbability 
Example 
2.1.11 Selecting Bolts . There is one box of bolts that contains some long and some short 
bolts. A manager is unable to open the box at present, so she asks her employees 
whatisthecompositionofthebox.Oneemployeesaysthatitcontains60longbolts 
and 40 short bolts. Another says that it contains 10 long bolts and 20 short bolts. 
Unabletoreconciletheseopinions,themanagerdecidesthateachoftheemployees 
iscorrectwithprobability1/2.Let B1betheeventthattheboxcontains60longand 
40 short bolts, and let B2be the event that the box contains 10 long and 20 short 
bolts.Theprobabilitythattheﬁrstboltselectedislongisnowcalculatedpreciselyas 
in Example 2.1.9. ◭
In Example 2.1.11, there is only one box of bolts, but we believe that it has one 
of two possible compositions. We let the events B1andB2determine the possible
compositions. This typeofsituation is very common in experiments. 
Example 
2.1.12 A Clinical Trial . Consideraclinicaltrialsuchasthestudyoftreatmentsfordepression 
in Example 2.1.4. As in many such trials, each patient has two possible outcomes, 
in this case relapse and no relapse. We shall refer to relapse as “failure” and no 
relapse as “success.” For now, we shall consider only patients in the imipramine 
treatmentgroup.Ifweknewtheeffectivenessofimipramine,thatis,theproportion 
pof successes among all patients who might receive the treatment, then we might 
modelthepatientsinourstudyashavingprobability pofsuccess.Unfortunately,we 
do not know pat the start of the trial. In analogy to the box of bolts with unknown 
composition in Example 2.1.11, we can imagine that the collection of all available 
patients(fromwhichthe40imipraminepatientsinthistrialwereselected)hastwoor 
morepossiblecompositions.Wecanimaginethatthecompositionofthecollectionof 
patientsdeterminestheproportionthatwillbesuccess.Forsimplicity,inthisexample, 
we imagine that there are 11 different possible compositions of the collection of 
patients.Inparticular,weassumethattheproportionsofsuccessforthe11possible 
compositions are 0 ,1/10 , . . . , 9/10 ,1. (We shall be able to handle more realistic 
models for pin Chapter 3.) For example, if we knew that our patients were drawn 
from a collection with the proportion 3 /10 of successes, we would be comfortable 
saying that the patients in our sample each have success probability p=3/10. The 
valueof pisanimportantsourceofuncertaintyinthisproblem,andweshallpartition 
the sample space by the possible values of p. For j=1, . . . , 11, let Bjbe the event 
thatoursamplewasdrawnfromacollectionwithproportion (j −1)/ 10ofsuccesses. 
Wecan also identify Bjas theevent {p=(j −1)/ 10 }.
Now, let E1be the event that the ﬁrst patient in the imipramine group has a 
success. We deﬁned each event Bjso that Pr (E 1|Bj)=(j −1)/ 10. Supppose that, 
prior to starting the trial, we believe thatPr (B j)=1/11for each j. Itfollows that 
Pr (E 1)=11 /summationdisplay
j=11
11 j−1
10 =55 
110=1
2, (2.1.6)
wherethesecond equalityuses thefact that/summationtextn
j=1j=n(n+1)/ 2. ◭
The events B1, B 2, . . . , B 11 in Example 2.1.12 can be thought of in much the 
same way as the two events B1andB2that determine the mixture of long and short 
boltsinExample2.1.11.Thereisonlyoneboxofbolts,butthereisuncertaintyabout 
its composition. Similarly in Example 2.1.12, there is only one group of patients, 
butwebelievethatithasoneof11possiblecompositionsdeterminedbytheevents 
B1, B 2, . . . , B 11 .Tocalltheseevents,theymustbesubsetsofthesamplespaceforthe 
experiment in question. That will be the case in Example 2.1.12 if we imagine that 
2.1 TheDeﬁnition ofConditionalProbability 63 
theexperimentconsistsnotonlyofobservingthenumbersofsuccessesandfailures 
among the patients but also of potentially observing enough additional patients to 
be able to compute p, possibly at some time very far in the future. Similarly, in 
Example 2.1.11, the two events B1andB2are subsets of the sample space if we 
imaginethattheexperimentconsistsnotonlyofobservingonesampleboltbutalso 
ofpotentially observingthe entire composition ofthe box. 
Throughout the remainder of this text, we shall implicitly assume that experi- 
ments are augmented to include outcomes that determine the values of quantities 
suchas p.Weshallnotrequirethatweevergettoobservethecompleteoutcomeof 
the experiment so as to tell us precisely what pis, but merely that there is an exper- 
imentthatincludesalloftheeventsofinteresttous,includingthosethatdetermine 
quantities like p.
Deﬁnition
2.1.3 Augmented Experiment . Ifdesired,anyexperimentcanbeaugmentedtoincludethe 
potentialorhypotheticalobservationofasmuchadditionalinformationaswewould 
ﬁnd useful to help us calculate any probabilities thatwe desire. 
Deﬁnition 2.1.3 is worded somewhat vaguely because it is intended to cover a 
wide varietyofcases. Hereis an explicitapplication to Example 2.1.12. 
Example 
2.1.13 A Clinical Trial . In Example 2.1.12, we could explicitly assume that there exists an 
inﬁnite sequence of patients who could be treated with imipramine even though 
we will observe only ﬁnitely many of them. We could let the sample space consist 
of inﬁnite sequences of the two symbols SandFsuch as (S, S, F, S, F, F, F, . . .) .
Here Sin coordinate imeans that the ith patient is a success, and Fstands for
failure. So, the event E1in Example 2.1.12 is the event that the ﬁrst coordinate 
is S. The example sequence above is then in the event E1. To accommodate our 
interpretation of pas the proportion of successes, we can assume that, for every 
such sequence, the proportion of S’s among the ﬁrst ncoordinates gets close to one 
ofthenumbers0 ,1/10 , . . . , 9/10 ,1as nincreases.Inthisway, pisexplicitlythelimit 
of the proportion of successes we would observe if we could ﬁnd a way to observe 
indeﬁnitely.InExample2.1.12, B2istheeventconsistingofalltheoutcomesinwhich 
the limit of the proportion of S’s equals 1 /10, B3is the set of outcomes in which 
the limit is 2 /10, etc. Also, we observe only the ﬁrst 40 coordinates of the in ﬁnite 
sequence,butwestillbehaveasif pexistsandcouldbedeterminedifonlywecould 
observe forever. ◭
In the remainder of the text, there will be many experiments that we assume 
areaugmented.Insuchcases,wewillmentionwhichquantities(suchas pinExam- 
ple2.1.13)wouldbedeterminedbytheaugmentedpartoftheexperimentevenifwe 
do notexplicitly mention thatthe experimentis augmented. 
The Game of Craps 
We shall conclude this section by discussing a popular gambling game called craps. 
One version of this game is played as follows: A player rolls two dice, and the sum 
of the two numbers that appear is observed. If the sum on the ﬁrst roll is 7 or 11, 
the player wins the game immediately. If the sum on the ﬁrst roll is 2 ,3, or 12, the 
player loses the game immediately. If the sum on the ﬁrst roll is 4, 5, 6, 8, 9, or 10, 
then the two dice are rolled again and again until the sum is either 7 or the original 
value. If the original value is obtained a second time before 7 is obtained, then the 
64 Chapter 2 ConditionalProbability 
player wins. If the sum 7 is obtained before the original value is obtained a second 
time, then the player loses. 
We shall now compute the probability Pr (W) , where Wis the event that the 
playerwillwin.Letthesamplespace Sconsistofallpossiblesequencesofsumsfrom 
therollsofdicethatmightoccurinagame.Forexample,someoftheelementsof Sare
(4,7),(11 ),(4,3,4),(12 ),(10 ,8,2,12 ,6,7),etc.Weseethat (11 )∈Wbut(4,7)∈Wc,
etc..Webeginbynoticingthatwhetherornotanoutcomeisin Wdependsinacrucial 
way on the ﬁrst roll. For this reason, it makes sense to partition Waccording to the 
sumon the ﬁrstroll. Let Bibe the eventthatthe ﬁrstroll is ifori=2, . . . , 12. 
Theorem2.1.4tellsusthatPr (W) =/summationtext12 
i=2Pr (B i)Pr (W |Bi).SincePr (B i)foreach
iwas computed in Example 1.6.5, we need to determine Pr (W |Bi)for each i. We 
beginwith i=2.Becausetheplayerlosesiftheﬁrstrollis2,wehavePr (W |B2)=0. 
Similarly,Pr (W |B3)=0=Pr (W |B12 ).Also,Pr (W |B7)=1becausetheplayerwinsif 
theﬁrst rollis 7. Similarly, Pr (W |B11 )=1. 
For each ﬁrst roll i∈{4,5,6,8,9,10 }, Pr (W |Bi)is the probability that, in a 
sequence of dice rolls, the sum iwill be obtained before the sum 7 is obtained. As 
describedinExample2.1.5,thisprobabilityisthesameastheprobabilityofobtaining 
thesum iwhenthe summustbe either ior 7. Hence, 
Pr (W |Bi)=Pr (B i)
Pr (B i∪B7).
Wecomputethe necessary values here: 
Pr (W |B4)=3
36 
3
36 +6
36 =1
3, P(W |B5)=4
36 
4
36 +6
36 =2
5,
Pr (W |B6)=5
36 
5
36 +6
36 =5
11 ,Pr (W |B8)=5
36 
5
36 +6
36 =5
11 ,
Pr (W |B9)=4
36 
4
36 +6
36 =2
5,Pr (W |B10 )=3
36 
3
36 +6
36 =1
3.
Finally, we compute the sum /summationtext12 
i=2Pr (B i)Pr (W |Bi):
Pr (W) =12 /summationdisplay
i=2Pr (B i)Pr (W |Bi)=0+0+3
36 1
3+4
36 2
5+5
36 5
11 +6
36 
+5
36 5
11 +4
36 2
5+3
36 1
3+2
36 +0=2928
5940=0.493.
Thus, the probability ofwinningin the gameofcraps is slightly less than 1/2. 
Summary
The revised probability of an event Aafter learning that event B(with Pr (B) > 0) 
has occurred is the conditional probability of Agiven B, denoted by Pr (A |B) and
computed as Pr (A ∩B)/ Pr (B) . Often it is easy to assess a conditional probability, 
such as Pr (A |B) , directly. In such a case, we can use the multiplication rule for con- 
ditionalprobabilitiestocomputePr (A ∩B) =Pr (B) Pr (A |B) .Allprobabilityresults 
have versions conditional on an event Bwith Pr (B) > 0: Just change allprobabili-
ties so that they are conditional on Bin addition to anything else they were already 
2.1 TheDeﬁnition ofConditionalProbability 65 
conditionalon.Forexample,themultiplicationruleforconditionalprobabilitiesbe- 
comesPr (A 1∩A2|B) =Pr (A 1|B) Pr (A 2|A1∩B) .Apartitionisacollectionofdisjoint 
eventswhoseunionisthewholesamplespace.Tobemostuseful,apartitionischo- 
sen so that an important source of uncertainty is reduced if we learn which one of 
the partition events occurs. If the conditional probability of an event Ais available 
giveneacheventinapartition,thelawoftotalprobabilitytellshowtocombinethese 
conditional probabilities to getPr (A).
Exercises 
1. If A⊂BwithPr (B) > 0,whatisthevalueofPr (A |B) ?
2. If AandBare disjoint events and Pr (B) > 0, what is 
the value ofPr (A |B) ?
3. If Sis the sample space of an experiment and Ais any 
event in thatspace, whatis the value ofPr (A |S) ?
4. Each time a shopper purchases a tube of toothpaste, 
he chooses either brand A or brand B. Suppose that for 
eachpurchaseaftertheﬁrst,theprobabilityis1/3thathe 
willchoosethesamebrandthathechoseonhispreceding 
purchase and the probability is 2/3 that he will switch 
brands. If he is equally likely to choose either brand A 
or brand B on his ﬁrst purchase, what is the probability 
that both his ﬁrst and second purchases will be brand A 
and both his thirdand fourth purchaseswill bebrand B? 
5. A box contains rred balls and bblue balls. One ball 
is selected at random and its color is observed. The ball 
is then returned to the box and kadditional balls of the 
samecolorarealsoputintothebox.Asecondballisthen 
selectedatrandom,itscolorisobserved,anditisreturned 
to the box together with kadditional balls of the same 
color. Each time another ball is selected, the process is 
repeated.Iffourballsareselected,whatistheprobability 
thattheﬁrstthreeballswillberedandthefourthballwill 
be blue? 
6. A box contains three cards. One card is red on both 
sides, one card is green on both sides, and one card is red 
on one side and green on the other. One card is selected 
from the box at random, and the color on one side is 
observed.Ifthissideisgreen,whatistheprobabilitythat 
the other sideofthe card is also green? 
7. ConsideragaintheconditionsofExercise2ofSec.1.10. 
If a family selected at random from the city subscribes to 
newspaper A, what is the probability that the family also 
subscribes to newspaper B?
8. ConsideragaintheconditionsofExercise2ofSec.1.10. 
If a family selected at random from the city subscribes to 
at least one of the three newspapers A,B, and C, what is 
theprobabilitythatthefamilysubscribestonewspaper A?
9. Supposethataboxcontainsonebluecardandfourred 
cards,whicharelabeled A,B,C,and D.Supposealsothat two of these ﬁve cards are selected at random, without 
replacement.
a. If it is known that card Ahas been selected, what is 
the probabilitythat both cardsarered?
b. If it is known that at least one red card has been 
selected, what is the probability that both cards are 
red?
10.Consider the following version of the game of craps: 
The player rolls two dice. If the sum on the ﬁrst roll is 
7 or 11, the player wins the game immediately. If the 
sum on the ﬁrst roll is 2 ,3, or 12, the player loses the 
game immediately. However, if the sum on the ﬁrst roll 
is4 ,5,6,8,9,or10,thenthetwodicearerolledagainand 
againuntilthesumiseither7or11ortheoriginalvalue.If 
theoriginalvalueisobtainedasecondtimebeforeeither 
7 or 11 is obtained, then the player wins. If either 7 or 11 
isobtainedbeforetheoriginalvalueisobtainedasecond 
time,thentheplayerloses.Determinetheprobabilitythat 
theplayer willwin thisgame.
11.Foranytwoevents AandBwithPr (B) > 0,provethat 
Pr (A c|B) =1−Pr (A |B) .
12.Foranythreeevents A,B,and D,suchthatPr (D) > 0, 
prove that Pr (A ∪B|D) =Pr (A |D) +Pr (B |D) −Pr (A ∩
B|D) .
13.A box contains three coins with a head on each side, 
four coins with a tail on each side, and two fair coins. If 
one of these nine coins is selected at random and tossed 
once,whatistheprobabilitythataheadwillbeobtained? 
14.Amachineproducesdefectivepartswiththreediffer- 
ent probabilities depending on its state of repair. If the 
machine is in good working order, it produces defective 
parts with probability 0.02. If it is wearing down, it pro- 
ducesdefectivepartswithprobability0.1.Ifitneedsmain- 
tenance, it produces defective parts with probability 0.3. 
Theprobabilitythatthemachineisingoodworkingorder 
is0.8,theprobabilitythatitiswearingdownis0.1,andthe 
probabilitythatitneedsmaintenanceis0.1.Computethe 
probabilitythatarandomlyselectedpartwillbedefective. 
66 Chapter 2 ConditionalProbability 
15.ThepercentagesofvotersclassedasLiberalsinthree 
different election districts are divided as follows: in the 
ﬁrstdistrict,21percent;intheseconddistrict,45percent; 
andinthethirddistrict,75percent.Ifadistrictisselected 
at random and a voter is selected at random from that 
district,whatistheprobabilitythatshewillbeaLiberal? 
16.Consider again the shopper described in Exercise 4. 
Oneachpurchase,theprobabilitythathewillchoosethe same brand of toothpaste that he chose on his preced- 
ingpurchaseis1/3,andtheprobabilitythathewillswitch 
brandsis2/3.Supposethatonhisﬁrstpurchasetheproba- 
bilitythathewillchoosebrandAis1/4andtheprobability 
thathewillchoosebrandBis3/4.Whatistheprobability 
that hissecondpurchase willbebrand B? 
17.Provetheconditionalversionofthelawoftotalprob- 
ability (2.1.5).
2.2 Independent Events 
Iflearningthat Bhasoccurreddoesnotchangetheprobabilityof A,thenwesay 
thatAandBare independent. There are many cases in which events AandB
arenotindependent,buttheywouldbeindependentifwelearnedthatsomeother 
eventChadoccurred.Inthiscase, AandBareconditionallyindependentgiven C.
Example 
2.2.1Tossing Coins . Suppose that a fair coin is tossed twice. The experiment has four 
outcomes, HH, HT, TH, and TT, that tell us how the coin landed on each of the 
twotosses.Wecanassumethatthissamplespaceissimplesothateachoutcomehas 
probability 1/4. Suppose that we are interested in the second toss. In particular, we 
wanttocalculatetheprobabilityoftheevent A={Hon second toss }.Weseethat A=
{HH,TH},sothatPr (A)=2/4=1/2.IfwelearnthattheﬁrstcoinlandedT,wemight 
wish to compute the conditional probability Pr (A |B) where B={T on ﬁrsttoss }.
Using thedeﬁnition ofconditional probability, weeasily compute 
Pr (A |B) =Pr (A ∩B) 
Pr (B) =1/4
1/2=1
2,
because A∩B={T H }has probability 1/4. We see that Pr (A |B) =Pr (A); hence, we 
don’tchange the probability of Aeven afterwe learn that Bhasoccurred. ◭
Deﬁnition of Independence 
The conditional probability of the event Agiven that the event Bhas occurred is 
therevisedprobabilityof Aafterwelearnthat Bhasoccurred.Itmightbethecase, 
however,thatnorevisionisnecessarytotheprobabilityof Aevenafterwelearnthat 
Boccurs.ThisispreciselywhathappenedinExample2.2.1.Inthiscase,wesaythat 
AandBareindependentevents . As another example, if we toss a coin and then roll 
adie,wecouldlet Abetheeventthatthedieshows3andlet Bbetheeventthatthe 
coin lands with heads up. If the tossing of the coin is done in isolation of the rolling 
of the die, we might be quite comfortable assigning Pr (A |B) =Pr (A)=1/6. In this 
case, we say that AandBareindependent events.
Ingeneral,ifPr (B) > 0,theequationPr (A |B) =Pr (A)canberewrittenasPr (A ∩
B)/ Pr (B) =Pr (A).IfwemultiplybothsidesofthislastequationbyPr (B) ,weobtain 
theequationPr (A ∩B) =Pr (A)Pr (B) .InordertoavoidtheconditionPr (B) > 0,the 
mathematicaldeﬁnition ofthe independence oftwoevents is stated as follows: 
Deﬁnition
2.2.1Independent Events . Two events AandBareindependent if 
Pr (A ∩B) =Pr (A)Pr (B). 
2.2 IndependentEvents 67 
SupposethatPr (A) >0andPr (B) > 0.Thenitfollowseasilyfromthedeﬁnitions 
ofindependenceandconditionalprobabilitythat AandBareindependentifandonly 
ifPr (A |B) =Pr (A)andPr (B |A) =Pr (B) .
Independence of Two Events 
If two events AandBare considered to be independent because the events are 
physically unrelated, and if the probabilities Pr (A)and Pr (B) are known, then the
deﬁnition canbe used to assigna value to Pr (A ∩B) .
Example 
2.2.2Machine Operation . Supposethattwomachines1and2inafactoryareoperatedin- 
dependentlyofeachother.Let Abetheeventthatmachine1willbecomeinoperative 
duringagiven8-hourperiod,let Bbetheeventthatmachine2willbecomeinopera- 
tiveduringthesameperiod,andsupposethatPr (A)=1/3andPr (B) =1/4.Weshall 
determinetheprobabilitythatatleastoneofthemachineswillbecomeinoperative 
during thegiven period.
The probability Pr (A ∩B) that both machines will become inoperative during
the periodis 
Pr (A ∩B) =Pr (A)Pr (B) =/parenleftbigg1
3/parenrightbigg /parenleftbigg 1
4/parenrightbigg
=1
12 .
Therefore, the probability Pr (A ∪B) that at least one of the machines will become 
inoperativeduring the periodis 
Pr (A ∪B) =Pr (A)+Pr (B) −Pr (A ∩B) 
=1
3+1
4−1
12 =1
2. ◭
The next example shows that two events AandB, which are physically related, 
can,nevertheless, satisfy the deﬁnition ofindependence. 
Example 
2.2.3Rolling a Die . Suppose that a balanced die is rolled. Let Abe the event that an even 
number is obtained, and let Bbe the event that one of the numbers 1 ,2,3, or 4 is 
obtained.We shall showthatthe events AandBareindependent.
In this example, Pr (A)=1/2 and Pr (B) =2/3. Furthermore, since A∩Bis the 
eventthateitherthenumber2orthenumber4isobtained,Pr (A ∩B) =1/3.Hence, 
Pr (A ∩B) =Pr (A)Pr (B) .Itfollowsthattheevents AandBareindependentevents,
even thoughthe occurrenceofeach eventdepends on the same roll ofa die. ◭
Theindependenceoftheevents AandBinExample2.2.3canalsobeinterpreted 
as follows: Suppose that a person must bet on whether the number obtained on the 
diewillbeevenorodd,thatis,onwhetherornottheevent Awilloccur.Sincethree 
ofthepossibleoutcomesoftherollareevenandtheotherthreeareodd,theperson 
willtypicallyhavenopreferencebetweenbettingonanevennumberandbettingon 
an odd number. 
Supposealsothatafterthediehasbeenrolled,butbeforethepersonhaslearned
theoutcomeandbeforeshehasdecidedwhethertobetonanevenoutcomeoronan 
oddoutcome,sheisinformedthattheactualoutcomewasoneofthenumbers1 ,2,3, 
or4,i.e.,thattheevent Bhasoccurred.Thepersonnowknowsthattheoutcomewas 
1,2,3, or 4. However, since two of these numbers are even and two are odd, the 
person will typically still have no preference between betting on an even number 
andbettingonanoddnumber.Inotherwords,theinformationthattheevent Bhas
68 Chapter 2 ConditionalProbability 
occurredisofnohelptothepersonwhoistryingtodecidewhetherornottheevent 
Ahasoccurred.
IndependenceofComplements Intheforegoingdiscussionofindependentevents, 
westatedthatif AandBareindependent,thentheoccurrenceornonoccurrenceof 
Ashould not be related to the occurrence or nonoccurrence of B. Hence, if Aand
Bsatisfy the mathematical deﬁnition of independent events, then it should also be 
true that AandBcare independent events, that AcandBare independent events,
andthat AcandBcareindependentevents.Oneoftheseresultsisestablishedinthe 
nexttheorem.
Theorem
2.2.1If two events AandBare independent, then the events AandBcare also indepen-
dent.
Proof Theorem1.5.6 says that 
Pr (A ∩Bc)=Pr (A)−Pr (A ∩B). 
Furthermore, since AandBare independent events, Pr (A ∩B) =Pr (A)Pr (B) . It 
nowfollowsthat
Pr (A ∩Bc)=Pr (A)−Pr (A)Pr (B) =Pr (A)[1 −Pr (B) ]
=Pr (A)Pr (B c). 
Therefore, the events AandBcare independent.
Theproofoftheanalogousresultfortheevents AcandBissimilar,andtheproof 
for theevents AcandBcis required in Exercise 2 atthe endofthis section. 
Independence of Several Events 
The deﬁnition of independent events can be extended to any number of events, 
A1, . . . , A k.Intuitively,iflearningthatsomeoftheseeventsdoordonotoccurdoes 
notchangeourprobabilitiesforanyeventsthatdependonlyontheremainingevents, 
we would say that all kevents are independent. The mathematical deﬁnition is the 
following analogto Deﬁnition 2.2.1. 
Deﬁnition
2.2.2(Mutually) Independent Events . The kevents A1, . . . , A kareindependent (ormutually
independent ) if, for every subset Ai1, . . . , A ijof jofthese events (j =2,3, . . . , k) ,
Pr (A i1∩. . . ∩Aij)=Pr (A i1). . . Pr (A ij). 
Asanexample,inorderforthreeevents A,B,and Ctobeindependent,thefollowing 
fourrelationsmust be satisﬁed: 
Pr (A ∩B) =Pr (A)Pr (B), 
Pr (A ∩C) =Pr (A)Pr (C), 
Pr (B ∩C) =Pr (B) Pr (C), (2.2.1)
and
Pr (A ∩B∩C) =Pr (A)Pr (B) Pr (C). (2.2.2)
It is possible that Eq. (2.2.2) will be satisﬁed, but one or more of the three rela- 
tions(2.2.1)willnotbesatisﬁed.Ontheotherhand,asisshowninthenextexample, 
2.2 IndependentEvents 69 
itisalsopossiblethateachofthethreerelations(2.2.1)willbesatisﬁedbutEq.(2.2.2) 
will notbe satisﬁed. 
Example 
2.2.4Pairwise Independence . Suppose that a fair coin is tossed twice so that the sample 
space S={HH,HT,TH,TT }is simple. Deﬁne the following three events: 
A={Hon ﬁrsttoss }={HH,HT },
B={Hon second toss }={HH,TH },and
C={Both tossesthe same }={HH,TT }.
Then A∩B=A∩C=B∩C=A∩B∩C={HH }. Hence, 
Pr (A)=Pr (B) =Pr (C) =1/2
and
Pr (A ∩B) =Pr (A ∩C) =Pr (B ∩C) =Pr (A ∩B∩C) =1/4.
It follows that each of the three relations of Eq. (2.2.1) is satisﬁed but Eq. (2.2.2) is 
notsatisﬁed.Theseresultscanbesummarizedbysayingthattheevents A,B,and C
arepairwiseindependent , butallthree events are notindependent. ◭
Weshallnowpresentsomeexamplesthatwillillustratethepowerandscopeof 
the concept ofindependence in the solution ofprobability problems. 
Example 
2.2.5Inspecting Items . Supposethatamachineproducesadefectiveitemwithprobability 
p(0 < p < 1) and produces a nondefective item with probability 1 −p. Suppose 
furtherthatsixitemsproducedbythemachineareselectedatrandomandinspected, 
and that the results (defective or nondefective) for these six items are independent. 
Weshall determine the probability thatexactlytwoofthe six items are defective. 
It can be assumed that the sample space Scontains all possible arrangements
of six items, each one of which might be either defective or nondefective. For j=
1, . . . , 6,weshalllet Djdenotetheeventthatthe jthiteminthesampleisdefective 
so that Dc
jis the event that this item is nondefective. Since the outcomes for the six 
differentitemsareindependent,theprobabilityofobtaininganyparticularsequence 
of defective and nondefective items will simply be the product of the individual 
probabilities for theitems. For example, 
Pr (D c
1∩D2∩Dc
3∩Dc
4∩D5∩Dc
6)=Pr (D c
1)Pr (D 2)Pr (D c
3)Pr (D c
4)Pr (D 5)Pr (D c
6)
=(1−p)p( 1−p)( 1−p)p( 1−p) =p2(1−p) 4.
It can be seen that the probability of any other particular sequence in Scontaining
two defective items and four nondefective items will also be p2(1−p) 4. Hence, the 
probabilitythattherewillbeexactlytwodefectivesinthesampleofsixitemscanbe 
foundbymultiplyingtheprobability p2(1−p) 4ofanyparticularsequencecontaining 
twodefectivesbythepossiblenumberofsuchsequences.Sincethereare /parenleftbig6
2/parenrightbig
distinct
arrangementsoftwodefectiveitemsandfournondefectiveitems,theprobabilityof 
obtaining exactlytwo defectivesis /parenleftbig6
2/parenrightbig
p2(1−p) 4. ◭
Example 
2.2.6Obtaining a Defective Item . For the conditions of Example 2.2.5, we shall now deter- 
minetheprobabilitythatatleastoneofthesixitemsinthesamplewillbedefective. 
Sincetheoutcomesforthedifferentitemsareindependent,theprobabilitythat
all six items will be nondefective is (1−p) 6. Therefore, the probability that at least 
oneitemwill be defective is 1 −(1−p) 6. ◭
70 Chapter 2 ConditionalProbability 
Example 
2.2.7Tossing a Coin Until a Head Appears . Suppose that a fair coin is tossed until a head 
appearsfortheﬁrsttime,andassumethattheoutcomesofthetossesareindependent. 
Weshall determine the probability pnthat exactly ntosses will be required. 
The desired probability is equal to the probability of obtaining n−1 tails in 
succession and then obtaining a head on the next toss. Since the outcomes of the 
tosses are independent, the probability of this particular sequence of noutcomes is 
pn=(1/2)n.
The probability that a head will be obtained sooner or later (or, equivalently, 
that tailswillnot be obtained forever) is 
∞/summationdisplay
n=1pn=1
2+1
4+1
8+. . . =1.
Sincethesumoftheprobabilities pnis1,itfollowsthattheprobabilityofobtaining 
an inﬁnite sequence oftails withoutever obtaining a head mustbe 0. ◭
Example 
2.2.8Inspecting Items One at a Time . Consider again a machine that produces a defective 
item with probability pand produces a nondefective item with probability 1 −p.
Suppose that items produced by the machine are selected at random and inspected 
one at a time until exactly ﬁve defective items have been obtained. We shall deter- 
mine the probability pnthat exactly nitems (n ≥5)must be selected to obtain the 
ﬁve defectives.
Theﬁfthdefectiveitemwillbethe nthitemthatisinspectedifandonlyifthere 
are exactly four defectives among the ﬁrst n−1 items and then the nth item is 
defective. By reasoning similar to that given in Example 2.2.5, it can be shown that 
the probability of obtaining exactly four defectives and n−5 nondefectives among 
the ﬁrst n−1 items is /parenleftbign−1
4/parenrightbig
p4(1−p) n−5. The probability that the nth item will be 
defective is p. Since the ﬁrst event refers to outcomes for only the ﬁrst n−1items 
and the second event refers to the outcome for only the nth item, these two events 
are independent. Therefore, the probability that both events will occur is equal to 
theproduct oftheir probabilities. Itfollows that 
pn=/parenleftbiggn−1
4/parenrightbigg
p5(1−p) n−5. ◭
Example 
2.2.9People v. Collins . FinkelsteinandLevin(1990)describeacriminalcasewhoseverdict 
wasoverturnedbytheSupremeCourtofCaliforniainpartduetoaprobabilitycal- 
culation involving both conditional probability and independence. The case, People 
v.Collins ,68Cal.2d319,438P.2d33(1968),involvedapursesnatchinginwhichwit- 
nesses claimed to see a young woman with blond hair in a ponytail ﬂeeing from the 
scene in a yellow car driven by a black man with a beard. A couple meeting the de- 
scriptionwasarrestedafewdaysafterthecrime,butnophysicalevidencewasfound. 
A mathematician calculated the probability that a randomly selected couple would 
possess the described characteristics as about 8 .3×10 −8, or 1 in 12 million. Faced 
with such overwhelming odds and no physical evidence, the jury decided that the 
defendants must have been the only such couple and convicted them. The Supreme 
Court thought that a more useful probability should have been calculated. Based 
on the testimony of the witnesses, there was a couple that met the above descrip- 
tion. Given that there was already one couple who met the description, what is the 
conditionalprobability that therewas also a second couple such as the defendants? 
Letpbetheprobabilitythatarandomlyselectedcouplefromapopulationof n
coupleshascertaincharacteristics.Let Abetheeventthatatleastonecoupleinthe 
population has the characteristics, and let Bbe the event that at least two couples 
2.2 IndependentEvents 71 
have thecharacteristics. Whatwe seekis Pr (B |A) . Since B⊂A, itfollows that 
Pr (B |A) =Pr (B ∩A) 
Pr (A)=Pr (B) 
Pr (A).
We shall calculate Pr (B) and Pr (A)by breaking each event into more manageable 
pieces. Suppose that we number the ncouples in the population from 1 to n. Let Ai
betheeventthatcouplenumber ihasthecharacteristicsinquestionfor i=1, . . . , n ,
andlet Cbe the eventthatexactly one couple has the characteristics.Then 
A=(A c
1∩Ac
2. . . ∩Ac
n)c,
C=(A 1∩Ac
2. . . ∩Ac
n)∪(A c
1∩A2∩Ac
3. . . ∩Ac
n)∪. . . ∪(A c
1∩. . . ∩Ac
n−1∩An), 
B=A∩Cc.
Assuming that the ncouples are mutually independent, Pr (A c)=(1−p) n, and 
Pr (A)=1−(1−p) n. The nevents whose union is Care disjoint and each one has
probability p( 1−p) n−1, so Pr (C) =np( 1−p) n−1. Since A=B∪CwithBandC
disjoint, we have 
Pr (B) =Pr (A)−Pr (C) =1−(1−p) n−np( 1−p) n−1.
So, 
Pr (B |A) =1−(1−p) n−np( 1−p) n−1
1−(1−p) n. (2.2.3)
The Supreme Court of California reasoned that, since the crime occurred in a 
heavilypopulatedarea, nwouldbeinthemillions.Forexample,with p=8.3×10 −8
andn=8,000,000,thevalueof(2.2.3)is0.2966.Suchaprobabilitysugge ststhatthere 
is a reasonable chance that there was another couple meeting the same description 
asthewitnessesprovided.Ofcourse,thecourtdidnotknowhowlarge nwas,butthe 
factthat(2.2.3)couldeasilybesolargewasgroundsenoughtorulethatreasonable 
doubtremained as tothe guiltofthe defendants. ◭
Independence and Conditional Probability Two events AandBwith positive
probability are independent if and only if Pr (A |B) =Pr (A). Similar results hold for 
larger collections of independent events. The following theorem, for example, is 
straightforwardto prove based on the deﬁnitionofindependence. 
Theorem
2.2.2LetA1, . . . , A kbe events such that Pr (A 1∩. . . ∩Ak) > 0. Then A1, . . . , A kare
independentifandonlyif,foreverytwodisjointsubsets {i1, . . . , i m}and{j1, . . . , j ℓ}
of {1, . . . , k }, we have 
Pr (A i1∩. . . ∩Aim|Aj1∩. . . ∩Ajℓ)=Pr (A i1∩. . . ∩Aim). 
Theorem 2.2.2 says that kevents are independent if and only if learning that 
some of the events occur does not change the probability that any combination of 
the otherevents occurs.
The Meaning of Independence We have given a mathematical deﬁnition of inde- 
pendenteventsinDeﬁnition2.2.1.Wehavealsogivensomeinterpretationsforwhat 
itmeansforeventstobeindependent.Themostinstructiveinterpretationistheone 
basedonconditionalprobability.Iflearningthat Boccursdoesnotchangetheprob-
abilityof A,then AandBareindependent.Insimpleexamplessuchastossingwhat 
we believe to be a fair coin, we would generally not expect to change our minds 
72 Chapter 2 ConditionalProbability 
about what is likely to happen on later ﬂips after we observe earlier ﬂips; hence, we 
declaretheeventsthatconcerndifferentﬂipstobeindependent.However,consider 
a situation similar to Example 2.2.5 in which items produced by a machine are in- 
spectedtoseewhetherornottheyaredefective.InExample2.2.5,wedeclaredthat 
the different items were independent and that each item had probability pof being 
defective. This might make sense if we were conﬁdent that we knew how well the 
machine was performing. But if we were unsure of how the machine were perform- 
ing, we could easily imagine changing our mind about the probability that the 10th 
itemisdefectivedependingonhowmanyoftheﬁrstnineitemsaredefective.Tobe 
speciﬁc, suppose that we begin by thinking that the probability is 0.08 that an item 
willbedefective.Ifweobserveoneorzerodefectiveitemsintheﬁrstnine,wemight 
not make much revision to the probability that the 10th item is defective. On the 
otherhand,ifweobserveeightorninedefectivesintheﬁrstnineitems,wemightbe 
uncomfortablekeepingtheprobabilityat0.08thatthe10thitemwillbedefective.In 
summary,whendecidingwhethertomodeleventsasindependent,trytoanswerthe 
following question: “If I were to learn that some of these events occurred, would I 
change the probabilities of any of the others?” If we feel that we already know ev- 
erythingthatwecouldlearnfromtheseeventsabouthowlikelytheothersshouldbe, 
wecansafelymodelthemasindependent.If,ontheotherhand,wefeelthatlearning 
some of these events could change our minds about how likely some of the others 
are,thenweshouldbemorecarefulaboutdeterminingtheconditionalprobabilities 
andnot model theevents as independent. 
MutuallyExclusiveEventsandMutuallyIndependentEvents Twosimilar-sound- 
ing deﬁnitions have appeared earlier in this text. Deﬁnition 1.4.10 deﬁnes mutually 
exclusive events, and Deﬁnition 2.2.2 deﬁnes mutually independent events. It is 
almostneverthecasethatthesamesetofeventssatisﬁesbothdeﬁnitions.Thereason 
isthatifeventsaredisjoint(mutuallyexclusive),thenlearningthatoneoccursmeans 
thattheothersdeﬁnitelydidnotoccur.Hence,learningthatoneoccurswouldchange 
the probabilities for all the others to 0, unless the others already had probability 0. 
Indeed,thissuggeststheonlyconditioninwhichthetwodeﬁnitionswouldbothapply 
tothesamecollectionofevents.TheproofofthefollowingresultislefttoExercise24 
in this section. 
Theorem
2.2.3Letn > 1 and let A1, . . . , A nbe events that are mutually exclusive. The events are 
also mutually independent if and only if all the events except possibly one of them 
hasprobability0. 
Conditionally Independent Events 
Conditional probability and independence combine into one of the most versatile 
models of data collection. The idea is that, in many circumstances, we are unwilling 
tosaythatcertaineventsareindependentbecausewebelievethatlearningsomeof 
them will provide information about how likely the others are to occur. But if we 
knew the frequency with which such events would occur, we might then be willing 
to assume that they are independent. This model can be illustrated using one of the 
examples from earlierin this section. 
Example 
2.2.10 Inspecting Items . ConsideragainthesituationinExample2.2.5.Thistime,however, 
suppose that we believe that we would change our minds about the probabilities 
of later items being defective were we to learn that certain numbers of early items 
2.2 IndependentEvents 73 
were defective. Suppose that we think of the number pfrom Example 2.2.5 as the 
proportionofdefectiveitemsthatwewouldexpecttoseeifweweretoinspectavery 
largesampleofitems.Ifweknewthisproportion p,andifweweretosampleonlya 
few,say,sixor10itemsnow,wemightfeelconﬁdentmaintainingthattheprobability 
of a later item being defective remains peven after we inspect some of the earlier 
items.Ontheotherhand,ifwearenotsurewhatwouldbetheproportionofdefective 
itemsinalargesample,wemightnotfeelconﬁdentkeepingtheprobabilitythesame 
as wecontinue to inspect. 
To be precise, suppose that we treat the proportion pof defective items as 
unknown and that we are dealing with an augmented experiment as described in 
Deﬁnition2.1.3.Forsimplicity,supposethat pcantakeoneoftwovalues,either0.01 
or0.4,theﬁrstcorrespondingtonormaloperationandthesecondcorrespondingto 
a need for maintenance. Let B1be the event that p=0.01, and let B2be the event 
thatp=0.4. If we knew that B1had occurred, then we would proceed under the 
assumption that the events D1, D 2, . . . were independent with Pr (D i|B1)=0.01for 
alli. For example, we could do the same calculations as in Examples 2.2.5 and 2.2.8 
withp=0.01.Let Abetheeventthatweobserveexactlytwodefectivesinarandom 
sample of six items. Then Pr (A |B1)=/parenleftbig6
2/parenrightbig
0.01 20.99 4=1.44 ×10 −3. Similarly, if we 
knewthat B2hadoccurred,thenwewouldassumethat D1, D 2, . . . wereindependent
with Pr (D i|B2)=0.4. In this case, Pr (A |B2)=/parenleftbig6
2/parenrightbig
0.420.64=0.311. ◭
InExample2.2.10,thereisnoreasonthat pmustberequiredtoassumeatmost 
two different values. We could easily allow pto take a third value or a fourth value, 
etc.Indeed,inChapter3weshalllearnhowtohandlethecaseinwhicheverynumber 
between0and1isapossiblevalueof p.Thepointofthesimpleexampleistoillustrate 
the concept of assuming that events are independent conditional on another event, 
such as B1or B2in the example. 
The formal conceptillustrated in Example 2.2.10 is the following: 
Deﬁnition
2.2.3Conditional Independence . We say that events A1, . . . , A kareconditionally inde-
pendent given Bif, for every subcollection Ai1, . . . , A ijof jof these events ( j=
2,3, . . . , k ), 
Pr /parenleftBig
Ai1∩. . . ∩Aij/vextendsingle/vextendsingle/vextendsingleB/parenrightBig
=Pr (A i1|B) . . . Pr (A ij|B). 
Deﬁnition2.2.3isidenticaltoDeﬁnition2.2.2forindependenteventswiththemod- 
iﬁcation that allprobabilities in the deﬁnition are now conditional on B. As a note, 
even if we assume that events A1, . . . , A kare conditionally independent given B, it 
is notnecessarythattheybeconditionallyindependentgiven Bc.InExample2.2.10, 
the events D1, D 2, . . . were conditionally independent given both B1andB2=Bc
1,
whichisthetypicalsituation.Exercise16inSec.2.3isanexampleinwhicheventsare 
conditionallyindependentgivenoneevent Bbutarenotconditionallyindependent
giventhe complement Bc.
Recall that two events A1andA2(with Pr (A 1) > 0) are independent if and only 
ifPr (A 2|A1)=Pr (A 2). A similar resultholds for conditionally independentevents. 
Theorem
2.2.4Supposethat A1,A2,and BareeventssuchthatPr (A 1∩B) > 0.Then A1andA2are
conditionally independent given Bifand only ifPr (A 2|A1∩B) =Pr (A 2|B) .
Thisisanotherexampleoftheclaimwemadeearlierthateveryresultwecanprove 
has an analog conditional on an event B. The reader can prove this theorem in 
Exercise 22.
74 Chapter 2 ConditionalProbability 
The Collector’s Problem 
Suppose that nballs are thrown in a random manner into rboxes (r ≤n) . We shall 
assume that the nthrows are independent and that each of the rboxes is equally 
likely to receive any given ball. The problem is to determine the probability pthat
everyboxwillreceiveatleastoneball.Thisproblemcanbereformulatedintermsof 
acollector’sproblemasfollows:Supposethateachpackageofbubblegumcontains 
thepictureofabaseballplayer,thatthepicturesof rdifferentplayersareused,that
thepictureofeachplayerisequallylikelytobeplacedinanygivenpackageofgum, 
and that pictures are placed in different packages independently of each other. The 
problemnowistodeterminetheprobability pthatapersonwhobuys npackagesof 
gum(n ≥r) will obtain a complete setof rdifferent pictures.
For i=1, . . . , r , let Aidenote the event that the picture of player iis missing 
fromall npackages.Then /uniontextr
i=1Aiistheeventthatthepictureofatleastoneplayer 
is missing. We shall ﬁnd Pr (/uniontextr
i=1Ai)by applying Eq. (1.10.6). 
Since the picture of each of the rplayers is equally likely to be placed in any 
particularpackage,theprobabilitythatthepictureofplayer iwillnotbeobtainedin 
any particular package is (r −1)/r . Since the packages are ﬁlled independently, the 
probability that the picture of player iwill not be obtained in any of the npackages
is [ (r −1)/r ]n. Hence, 
Pr (A i)=/parenleftbiggr−1
r/parenrightbiggn
fori=1, . . . , r. 
Now consider any two players iandj. The probability that neither the picture of 
player inor the picture of player jwill be obtained in any particular package is 
(r −2)/r . Therefore, the probability that neither picture will be obtained in any of 
thenpackages is [ (r −2)/r ]n. Thus, 
Pr (A i∩Aj)=/parenleftbiggr−2
r/parenrightbiggn
.
Ifwe nextconsider any three players i,j, and k, we ﬁnd that 
Pr (A i∩Aj∩Ak)=/parenleftbiggr−3
r/parenrightbiggn
.
Bycontinuinginthisway,weﬁnallyarriveattheprobabilityPr (A 1∩A2∩. . . ∩Ar)
that the pictures of all rplayers are missing from the npackages. Of course, this 
probability is 0. Therefore, by Eq. (1.10.6) ofSec. 1.10, 
Pr /parenleftBiggr/uniondisplay
i=1Ai/parenrightBigg
=r/parenleftbiggr−1
r/parenrightbiggn
−/parenleftbiggr
2/parenrightbigg /parenleftbigg r−2
r/parenrightbiggn
+. . . +(−1)r/parenleftbiggr
r−1/parenrightbigg /parenleftbigg 1
r/parenrightbiggn
=r−1/summationdisplay
j=1(−1)j+1/parenleftbiggr
j/parenrightbigg /parenleftbigg 
1−j
r/parenrightbiggn
.
Sincetheprobability pofobtainingacompletesetof rdifferentpicturesisequalto 
1−Pr (/uniontextr
i=1Ai),itfollowsfromtheforegoingderivationthat pcanbewritteninthe 
form
p=r−1/summationdisplay
j=0(−1)j/parenleftbiggr
j/parenrightbigg /parenleftbigg 
1−j
r/parenrightbiggn
.
2.2 IndependentEvents 75 
Summary
Acollectionofeventsisindependentifandonlyiflearningthatsomeofthemoccur 
does not change the probabilities that any combination of the rest of them occurs. 
Equivalently,acollectionofeventsisindependentifandonlyiftheprobabilityofthe 
intersectionofeverysubcollectionistheproductoftheindividualprobabilities.The 
concept of independence has a version conditional on another event. A collection 
of events is independent conditional on Bif and only if the conditional probability 
of the intersection of every subcollection given Bis the product of the individual 
conditionalprobabilitiesgiven B.Equivalently,acollectionofeventsisconditionally 
independent given Bif and only if learning that some of them (and B) occur does 
notchangetheconditionalprobabilitiesgiven Bthatanycombinationoftherestof 
themoccur.Thefullpowerofconditionalindependencewillbecomemoreapparent 
after we introduce Bayes’ theoremin the nextsection. 
Exercises 
1. If AandBareindependenteventsandPr (B) < 1,what 
is the value ofPr (A c|Bc)?
2. Assumingthat AandBareindependentevents,prove 
thatthe events AcandBcarealso independent.
3. Supposethat AisaneventsuchthatPr (A)=0andthat 
Bisanyotherevent.Provethat AandBareindependent
events.
4. Suppose that a person rolls two balanced dice three 
times in succession. Determine the probability that on 
each of the three rolls, the sum of the two numbers that 
appear willbe 7. 
5. Suppose that the probability that the control system
used in a spaceship will malfunction on a given ﬂight is 
0.001.Supposefurtherthataduplicate,butcompletelyin- 
dependent,controlsystemisalsoinstalledinthespaceship 
to take control in case the ﬁrst system malfunctions. De- 
termine the probability that the spaceship will be under 
the control of either the original system or the duplicate 
system on a given ﬂight. 
6. Suppose that 10,000 tickets are sold in one lottery and 
5000 tickets are sold in another lottery. If a person owns 
100ticketsineachlottery,whatistheprobabilitythatshe 
will winatleastone ﬁrstprize? 
7. Twostudents AandBarebothregisteredforacertain 
course.Assumethatstudent Aattendsclass80percentof 
the time, student Battends class 60 percent of the time, 
and the absencesofthe two students areindependent. 
a. What is the probability that at least one of the two 
studentswill be in class on a given day? 
b. Ifatleastoneofthetwostudentsisinclassonagiven 
day,whatistheprobabilitythat Aisinclassthatday? 8. Ifthreebalanceddicearerolled,whatistheprobability 
thatall threenumberswill be the same? 
9. Consider an experiment in which a fair coin is tossed 
untilaheadisobtainedfortheﬁrsttime.Ifthisexperiment 
is performed three times, what is the probability that ex- 
actly the same number of tosses will be required for each 
ofthethree performances? 
10.The probability that any child in a certain family will 
haveblueeyesis1/4,andthisfeatureisinheritedindepen- 
dentlybydifferentchildreninthefamily.Ifthereareﬁve 
children in the family and it is known that at least one of 
these children has blue eyes, what is the probability that 
atleastthree ofthechildren have blue eyes? 
11.Consider the family with ﬁve children described in 
Exercise10.
a. Ifitisknownthattheyoungestchildinthefamilyhas 
blue eyes, what is the probability that at least three 
ofthechildren have blue eyes? 
b. Explain why the answer in part (a) is different from 
the answerinExercise10. 
12.Suppose that A,B, and Care three independent
events such that Pr (A)=1/4, Pr (B) =1/3, and Pr (C) =
1/2. (a)Determinetheprobabilitythatnoneofthesethree 
events will occur. (b)Determine the probability that ex-
actlyone ofthesethree events will occur. 
13.Supposethattheprobabilitythatanyparticleemitted
by a radioactive material will penetrate a certain shield 
is 0.01. If 10 particles are emitted, what is the probability 
thatexactlyoneoftheparticleswillpenetratetheshield? 
76 Chapter 2 ConditionalProbability 
14.Consider again the conditions of Exercise 13. If 10 
particles are emitted, what is the probability that at least 
one ofthe particles will penetrate the shield? 
15.Consider again the conditions of Exercise 13. How 
manyparticlesmustbeemittedinorderfortheprobability 
to be at least 0.8 that at least one particle will penetrate 
theshield?
16.In the World Series of baseball, two teams AandB
playasequenceofgamesagainsteachother,andtheﬁrst 
team that wins a total of four games becomes the winner 
of the World Series. If the probability that team Awill
winanyparticulargameagainstteam Bis1/3,whatisthe 
probabilitythatteam Awill wintheWorld Series? 
17.Two boys AandBthrow a ball at a target. Suppose 
that the probability that boy Awill hit the target on any 
throw is 1/3 and the probability that boy Bwill hit the
targetonanythrowis1/4.Supposealsothatboy Athrows
ﬁrstandthetwoboystaketurnsthrowing.Determinethe 
probability that the target will be hit for the ﬁrst time on 
thethird throwofboy A.
18.FortheconditionsofExercise17,determinetheprob- 
ability thatboy Awillhit thetarget beforeboy Bdoes.
19.A box contains 20 red balls, 30 white balls, and 50 
blue balls. Suppose that 10 balls are selected at random 
oneatatime,withreplacement;thatis,eachselectedball 
is replaced in the box before the next selection is made. 
Determine the probability that at least one color will be 
missing fromthe 10 selected balls. 20.Suppose that A1, . . . , A kform a sequence of kinde-
pendent events. Let B1, . . . , B kbe another sequence of k
events such that for each value of j (j =1, . . . , k) , either 
Bj=Ajor Bj=Ac
j. Prove that B1, . . . , B kare also inde-
pendent events. Hint:Use an induction argument based 
onthe number ofevents Bjfor which Bj=Ac
j.
21.Prove Theorem 2.2.2 on page 71. Hint:The “only if” 
directionisdirectfromthedeﬁnitionofindependenceon 
page68.Forthe“if”direction,useinductiononthevalue 
of jin the deﬁnition of independence. Let m=j−1and 
letℓ=1with j1=ij.
22.ProveTheorem2.2.4on page 73. 
23.A programmer is about to attempt to compile a se- 
ries of 11 similar programs. Let Aibe the event that the 
ithprogramcompilessuccessfullyfor i=1, . . . , 11.When 
the programming task is easy, the programmer expects 
that 80 percent of programs should compile. When the 
programmingtaskisdifﬁcult,sheexpectsthatonly40per- 
centoftheprogramswillcompile.Let Bbetheeventthat 
theprogrammingtaskwaseasy.Theprogrammerbelieves 
thattheevents A1, . . . , A 11 areconditionallyindependent
given Bandgiven Bc.
a. Compute the probability that exactly 8 out of 11 
programs willcompile given B.
b. Compute the probability that exactly 8 out of 11 
programs willcompile given Bc.
24.Prove Theorem2.2.3on page 72. 
2.3 Bayes’ Theorem 
Supposethatweareinterestedinwhichofseveraldisjointevents B1, . . . , B kwill 
occurandthatwewillgettoobservesomeotherevent A.If Pr (A |Bi)isavailable 
foreach i,thenBayes’theoremisausefulformulaforcomputingtheconditional 
probabilitiesofthe Bieventsgiven A.
Webegin with a typical example. 
Example 
2.3.1Test for a Disease . Suppose that you are walking down the street and notice that the 
Department of Public Health is giving a free medical test for a certain disease. The 
testis90percentreliableinthefollowingsense:Ifapersonhasthedisease,thereisa 
probabilityof0.9thatthetestwillgiveapositiveresponse;whereas,ifapersondoes 
nothavethedisease,thereisaprobabilityofonly0.1thatthetestwillgiveapositive 
response.
Data indicate that your chances of having the disease are only 1 in 10,000. 
However, since the test costs you nothing, and is fast and harmless, you decide to 
stopandtakethetest.Afewdayslateryoulearnthatyouhadapositiveresponseto 
thetest.Now, whatis the probability thatyouhave thedisease? ◭
2.3 Bayes’ Theorem 77 
ThelastquestioninExample2.3.1isaprototypeofthequestionforwhichBayes’ 
theoremwasdesigned.Wehaveatleasttwodisjointevents(“youhavethedisease” 
and “you do not have the disease”) about which we are uncertain, and we learn a 
pieceofinformation(theresultofthetest)thattellsussomethingabouttheuncertain 
events.Thenweneedtoknowhowtorevisetheprobabilitiesoftheeventsinthelight 
ofthe informationwe learned. 
WenowpresentthegeneralstructureinwhichBayes’theoremoperatesbefore 
returning to theexample. 
Statement, Proof, and Examples of Bayes’ Theorem 
Example 
2.3.2Selecting Bolts . Consider again the situation in Example 2.1.8, in which a bolt is 
selected at random from one of two boxes. Suppose that we cannot tell without 
makingafurthereffortfromwhichofthetwoboxestheoneboltisbeingselected.For 
example, the boxes may be identical in appearance or somebody else may actually 
select the box, but we only get to see the bolt. Prior to selecting the bolt, it was 
equallylikelythateachofthetwoboxeswouldbeselected.However,ifwelearnthat 
event Ahasoccurred,thatis,alongboltwasselected,wecancomputetheconditional 
probabilitiesofthetwoboxesgiven A.Toremindthereader, B1istheeventthatthe 
boxisselectedcontaining60longboltsand40shortbolts,while B2istheeventthat 
the box is selected containing 10 long bolts and 20 short bolts. In Example 2.1.9, we 
computedPr (A)=7/15,Pr (A |B1)=3/5,Pr (A |B2)=1/3,andPr (B 1)=Pr (B 2)=1/2. 
So, for example, 
Pr (B 1|A) =Pr (A ∩B1)
Pr (A)=Pr (B 1)Pr (A |B1)
Pr (A)=1
2×3
5
7
15 =9
14 .
Sincetheﬁrstboxhasahigherproportionoflongboltsthanthesecondbox,itseems 
reasonable that the probability of B1should rise after we learn that a long bolt was 
selected.ItmustbethatPr (B 2|A) =5/14sinceoneortheotherboxhadtobeselected. 
◭
In Example 2.3.2, we started with uncertainty about which of two boxes would 
bechosenandthenweobservedalongboltdrawnfromthechosenbox.Becausethe 
two boxes have different chances of having a long bolt drawn, the observation of a 
longboltchangedtheprobabilitiesofeachofthetwoboxeshavingbeenchosen.The 
precisecalculationofhowtheprobabilitieschangeisthepurposeofBayes’theorem. 
Theorem
2.3.1Bayes’ theorem . Let the events B1, . . . , B kform a partition of the space Ssuch that
Pr (B j) > 0 for j=1, . . . , k , and let Abe an event such that Pr (A) >0. Then, for 
i=1, . . . , k ,
Pr (B i|A) =Pr (B i)Pr (A |Bi)/summationtextk
j=1Pr (B j)Pr (A |Bj). (2.3.1)
Proof By the deﬁnition ofconditional probability, 
Pr (B i|A) =Pr (B i∩A) 
Pr (A).
ThenumeratorontherightsideofEq.(2.3.1)isequaltoPr (B i∩A) byTheorem2.1.1. 
The denominator is equal to Pr (A)accordingto Theorem2.1.4. 
78 Chapter 2 ConditionalProbability 
Example 
2.3.3Test for a Disease . Let us return to the example with which we began this section. 
We have just received word that we have tested positive for a disease. The test was 
90percentreliableinthesensethatwedescribedinExample2.3.1.Wewanttoknow 
the probability that we have the disease after we learn that the result of the test is 
positive. Some readers may feel that this probability should be about 0.9. However, 
thisfeelingcompletelyignoresthesmallprobabilityof0.0001thatyouhadthedisease 
beforetakingthetest.Weshalllet B1denotetheeventthatyouhavethedisease,and 
letB2denotetheeventthatyoudonothavethedisease.Theevents B1andB2form
a partition. Also, let Adenote the event that the response to the test is positive. 
The event Ais information we will learn that tells us something about the partition 
elements. Then, by Bayes’ theorem, 
Pr (B 1|A) =Pr (A |B1)Pr (B 1)
Pr (A |B1)Pr (B 1)+Pr (A |B2)Pr (B 2)
=(0.9)( 0.0001)
(0.9)( 0.0001)+(0.1)( 0.9999)=0.00090 .
Thus, the conditional probability that you have the disease given the test result 
is approximately only 1 in 1000. Of course, this conditional probability is approxi- 
mately 9 times as great as the probability was before you were tested, but even the 
conditionalprobability is quite small. 
Anotherwaytoexplainthisresultisasfollows:Onlyonepersoninevery10,000 
actuallyhasthedisease,butthetestgivesapositiveresponseforapproximatelyone 
person in every 10. Hence, the number of positive responses is approximately 1000 
times the number of persons who actually have the disease. In other words, out of 
every 1000 persons for whom the test gives a positive response, only one person 
actuallyhasthedisease.ThisexampleillustratesnotonlytheuseofBayes’theorem 
but also the importance of taking into account all of the information available in a 
problem. ◭
Example 
2.3.4Identifying the Source of a Defective Item . Three different machines M1,M2, and M3
were used for producing a large batch of similar manufactured items. Suppose that 
20 percent of the items were produced by machine M1, 30 percent by machine M2,
and50percentbymachine M3.Supposefurtherthat1percentoftheitemsproduced 
by machine M1are defective, that 2 percent of the items produced by machine M2
aredefective,andthat3percentoftheitemsproducedbymachine M3aredefective.
Finally, suppose that one item is selected at random from the entire batch and it is 
foundtobedefective.Weshalldeterminetheprobabilitythatthisitemwasproduced 
by machine M2.
LetBibe the event that the selected item was produced by machine Mi(i=
1,2,3), and let Abe the event that the selected item is defective. We must evaluate 
theconditionalprobability Pr (B 2|A) .
TheprobabilityPr (B i)thatanitemselectedatrandomfromtheentirebatchwas 
produced by machine Miis as follows, for i=1,2,3: 
Pr (B 1)=0.2,Pr (B 2)=0.3,Pr (B 3)=0.5.
Furthermore,theprobabilityPr (A |Bi)thatanitemproducedbymachine Miwillbe 
defectiveis 
Pr (A |B1)=0.01 ,Pr (A |B2)=0.02 ,Pr (A |B3)=0.03 .
Itnow follows fromBayes’ theoremthat 
2.3 Bayes’ Theorem 79 
Pr (B 2|A) =Pr (B 2)Pr (A |B2)
/summationtext3
j=1Pr (B j)Pr (A |Bj)
=(0.3)( 0.02 )
(0.2)( 0.01 )+(0.3)( 0.02 )+(0.5)( 0.03 )=0.26 . ◭
Example 
2.3.5Identifying Genotypes . Consider a gene that has two alleles (see Example 1.6.4 on 
page 23) Aanda. Suppose that the gene exhibits itself through a trait (such as 
hair color or blood type) with two versions. We call Adominant andarecessive
if individuals with genotypes AA andAa have the same version of the trait and 
the individuals with genotype aa have the other version. The two versions of the 
trait are called phenotypes . We shall call the phenotype exhibited by individuals 
with genotypes AA andAa thedominant trait, and the other trait will be called the 
recessivetrait .Inpopulationgeneticsstudies,itiscommontohaveinformationonthe 
phenotypesofindividuals,butitisratherdifﬁculttodeterminegenotypes.However, 
some information about genotypes can be obtained by observing phenotypes of 
parents andchildren.
Assume that the allele Ais dominant, that individuals mate independently of 
genotype,andthatthegenotypes AA ,Aa ,and aa occurinthepopulationwithprob- 
abilities 1/4, 1/2, and 1/4, respectively. We are going to observe an individual whose 
parentsarenotavailable,andweshallobservethephenotypeofthisindividual.Let 
Ebe the event that the observed individual has the dominant trait. We would like 
toreviseouropinionofthepossiblegenotypesoftheparents.Therearesixpossible 
genotypecombinations, B1, . . . , B 6,fortheparentspriortomakinganyobservations, 
andthesearelisted in Table 2.2. 
Theprobabilitiesofthe Biwerecomputedusingtheassumptionthattheparents
matedindependentlyofgenotype.Forexample, B3occursifthefatheris AA andthe
motheris aa (probability1/16)orifthefatheris aa andthemotheris AA (probability
1/16).ThevaluesofPr (E |Bi)werecomputedassumingthatthetwoavailablealleles
arepassedfromparentstochildrenwithprobability1/2eachandindependentlyfor 
the two parents. For example, given B4, the event Eoccurs if and only if the child 
does not get two a’s. The probability of getting afrom both parents given B4is 1/4, 
so Pr (E |B4)=3/4. 
Now we shall compute Pr (B 1|E) and Pr (B 5|E) . We leave the other calculations 
tothereader.ThedenominatorofBayes’theoremisthesameforbothcalculations, 
namely,
Pr (E) =5/summationdisplay
i=1Pr (B i)Pr (E |Bi)
=1
16 ×1+1
4×1+1
8×1+1
4×3
4+1
4×1
2+1
16 ×0=3
4.
Table2.2 Parental genotypesfor Example2.3.5
(AA, AA) (AA, Aa) (AA, aa) (Aa, Aa) (Aa, aa) (aa, aa) 
Name ofevent B1 B2 B3 B4 B5 B6
Probability of Bi1/16 1/4 1/8 1/4 1/4 1/16
Pr (E |Bi) 1 1 1 3/4 1/2 0 
80 Chapter 2 ConditionalProbability 
Applying Bayes’theorem,we get 
Pr (B 1|E) =1
16 ×1
3
4=1
12 ,Pr (B 5|E) =1
4×1
2
3
4=1
6. ◭
Note: Conditional Version of Bayes’ Theorem. There is also a version of Bayes’ 
theoremconditional on anevent C:
Pr (B i|A∩C) =Pr (B i|C) Pr (A |Bi∩C) /summationtextk
j=1Pr (B j|C) Pr (A |Bj∩C) . (2.3.2)
Prior and Posterior Probabilities 
In Example 2.3.4, a probability like Pr (B 2)is often called the prior probability that
the selected item will have been produced by machine M2, because Pr (B 2)is the 
probability of this event before the item is selected and before it is known whether 
the selected item is defective or nondefective. A probability like Pr (B 2|A) is then 
calledthe posteriorprobability thattheselecteditemwasproducedbymachine M2,
because it is the probability of this event after it is known that the selected item is 
defective.
Thus,inExample2.3.4,thepriorprobabilitythattheselecteditemwillhavebeen 
producedbymachine M2is0.3.Afteranitemhasbeenselectedandhasbeenfound 
to be defective, the posterior probability that the item was produced by machine 
M2is 0.26. Since this posterior probability is smaller than the prior probability that 
the item was produced by machine M2, the posterior probability that the item was 
producedbyoneoftheothermachinesmustbelargerthanthepriorprobabilitythat 
it was produced by one of those machines (see Exercises 1 and 2 at the end of this 
section).
Computation of Posterior Probabilities in More Than One Sta ge 
Suppose that a box contains one fair coin and one coin with a head on each side. 
Supposealsothatonecoinisselectedatrandomandthatwhenitistossed,aheadis 
obtained.We shall determine the probabilitythatthecoin is the fair coin. 
LetB1betheeventthatthecoinisfair,let B2betheeventthatthecoinhastwo 
heads,andlet H1betheeventthataheadisobtainedwhenthecoinistossed.Then, 
by Bayes’ theorem, 
Pr (B 1|H1)=Pr (B 1)Pr (H 1|B1)
Pr (B 1)Pr (H 1|B1)+Pr (B 2)Pr (H 1|B2)
=(1/2)( 1/2)
(1/2)( 1/2)+(1/2)( 1)=1
3. (2.3.3)
Thus, after the ﬁrsttoss, the posterior probability thatthe coin is fair is 1/3. 
Now suppose that the same coin is tossed again and we assume that the two 
tosses are conditionally independent given both B1andB2. Suppose that another 
head is obtained. There are two ways of determining the new value of the posterior 
probabilitythatthe coinis fair. 
The ﬁrst way is to return to the beginning of the experiment and assume again 
thatthepriorprobabilitiesarePr (B 1)=Pr (B 2)=1/2.Weshalllet H1∩H2denotethe
eventinwhichheadsareobtainedontwotossesofthecoin,andweshallcalculatethe 
posteriorprobabilityPr (B 1|H1∩H2)thatthecoinisfairafterwehaveobservedthe 
2.3 Bayes’ Theorem 81 
event H1∩H2. The assumption that the tosses are conditionally independent given 
B1means thatPr (H 1∩H2|B1)=1/2×1/2=1/4. By Bayes’ theorem, 
Pr (B 1|H1∩H2)=Pr (B 1)Pr (H 1∩H2|B1)
Pr (B 1)Pr (H 1∩H2|B1)+Pr (B 2)Pr (H 1∩H2|B2)
=(1/2)( 1/4)
(1/2)( 1/4)+(1/2)( 1)=1
5. (2.3.4)
The second way of determining this same posterior probability is to use the 
conditional version of Bayes’ theorem (2.3.2) given the event H1. Given H1, the 
conditionalprobabilityof B1is1/3,andtheconditionalprobabilityof B2istherefore 
2/3. These conditional probabilities can now serve as the prior probabilities for the 
next stage of the experiment, in which the coin is tossed a second time. Thus, we 
can apply (2.3.2) with C=H1, Pr (B 1|H1)=1/3, and Pr (B 2|H1)=2/3. We can then 
compute the posterior probability Pr (B 1|H1∩H2)that the coin is fair after we have 
observed a head on the second toss and a head on the ﬁrst toss. We shall need 
Pr (H 2|B1∩H1),whichequalsPr (H 2|B1)=1/2byTheorem2.2.4since H1andH2are
conditionally independent given B1. Since the coin is two-headed when B2occurs,
Pr (H 2|B2∩H1)=1. So we obtain 
Pr (B 1|H1∩H2)=Pr (B 1|H1)Pr (H 2|B1∩H1)
Pr (B 1|H1)Pr (H 2|B1∩H1)+Pr (B 2|H1)Pr (H 2|B2∩H1)
=(1/3)( 1/2)
(1/3)( 1/2)+(2/3)( 1)=1
5. (2.3.5)
Theposteriorprobabilityoftheevent B1obtainedinthesecondwayisthesame 
asthatobtainedintheﬁrstway.Wecanmakethefollowinggeneralstatement:Ifan 
experiment is carried out in more than one stage, then the posterior probability of 
everyeventcanalsobecalculatedinmorethanonestage.Aftereachstagehasbeen 
carried out, the posterior probability calculated for the event after that stage serves
as the prior probability for the next stage. The reader should look back at (2.3.2) 
to see that this interpretation is precisely what the conditional version of Bayes’ 
theorem says. The example we have been doing with coin tossing is typical of many 
applicationsofBayes’theoremanditsconditionalversionbecauseweareassuming 
that the observable events are conditionally independent given each element of the 
partition B1, . . . , B k(in this case, k=2). The conditional independence makes the 
probability of Hi(head on ith toss) given B1(or given B2) the same whether or not 
we also conditiononearlier tosses (see Theorem2.2.4). 
Conditionally Independent Events 
Thecalculationsthatledto(2.3.3)and(2.3.5)togetherwithExample2.2.10illustrate 
simple cases of a very powerful statistical model for observable events. It is very 
common to encounter a sequence of events that we believe are similar in that they 
allhavethesameprobabilityofoccurring.Itisalsocommonthattheorderinwhich 
the events are labeled does not affect the probabilities that we assign. However, 
we often believe that these events are not independent, because, if we were to 
observe some of them, we would change our minds about the probability of the 
ones we had not observed depending on how many of the observed events occur. 
For example, in the coin-tossing calculation leading up to Eq. (2.3.3), before any 
tosses occur, the probability of H2is the same as the probability of H1, namely, the 
82 Chapter 2 ConditionalProbability 
denominator of (2.3.3), 3/4, as Theorem 2.1.4 says. However, after observing that 
theevent H1occurs,theprobabilityof H2isPr (H 2|H1),whichisthedenominatorof 
(2.3.5), 5/6, as computed by the conditional version of the law of total probability 
(2.1.5). Even though we might treat the coin tosses as independent conditional 
on the coin being fair, and we might treat them as independent conditional on 
the coin being two-headed (in which case we know what will happen every time 
anyway),wecannottreatthemasindependentwithouttheconditioninginformation. 
The conditioning information removes an important source of uncertainty from 
the problem, so we partition the sample space accordingly. Now we can use the 
conditional independence of the tosses to calculate joint probabilities of various 
combinationsofeventsconditionallyonthepartitionevents.Finally,wecancombine 
theseprobabilitiesusingTheorem2.1.4and(2.1.5).Twomoreexampleswillhelpto 
illustratethese ideas.
Example 
2.3.6Learning about a Proportion . In Example 2.2.10 on page 72, a machine produced 
defectivepartsinoneoftwoproportions, p=0.01or p=0.4.Supposethattheprior 
probabilitythat p=0.01is0.9.Aftersamplingsixpartsatrandom,supposethatwe 
observetwodefectives. Whatis the posterior probabilitythat p=0.01? 
LetB1={p=0.01 }andB2={p=0.4}asinExample2.2.10.Let Abetheevent 
that two defectives occur in a random sample of size six. The prior probability of 
B1is 0.9, and the prior probability of B2is 0.1. We already computed Pr (A |B1)=
1.44 ×10 −3andPr (A |B2)=0.311in Example 2.2.10. Bayes’ theoremtells us that 
Pr (B 1|A) =0.9×1.44 ×10 −3
0.9×1.44 ×10 −3+0.1×0.311=0.04 .
Even though we thought originally that B1had probability as high as 0.9, after we 
learned that there were two defective items in a sample as small as six, we changed 
our minds dramatically and now we believe that B1has probability as small as 0.04. 
The reason for this major change is that the event Athat occurred has much higher
probability if B2is true thanif B1is true. ◭
Example 
2.3.7A Clinical Trial . Consider the same clinical trial described in Examples 2.1.12 and 
2.1.13. Let Eibe the event that the ith patient has success as her outcome. Recall 
thatBjis the event that p=(j −1)/ 10 for j=1, . . . , 11, where pis the proportion 
of successes among all possible patients. If we knew which Bjoccurred, we would 
say that E1, E 2, . . . were independent. That is, we are willing to model the patients 
asconditionallyindependentgiveneachevent Bj,andwesetPr (E i|Bj)=(j −1)/ 10 
for all i, j . We shall still assume that Pr (B j)=1/11for all jprior to the start of the 
trial.Wearenowinpositiontoexpresswhatwelearnabout pbycomputingposterior 
probabilities for the Bjevents aftereachpatientﬁnishes thetrial.
For example, consider the ﬁrst patient. We calculated Pr (E 1)=1/2 in (2.1.6). If 
E1occurs, we apply Bayes’ theoremto get 
Pr (B j|E1)=Pr (E 1|Bj)Pr (B j)
1/2=2(j −1)
10 ×11 =j−1
55 . (2.3.6)
Afterobservingonesuccess,theposteriorprobabilitiesoflargevaluesof parehigher
than their prior probabilities and the posterior probabilities of low values of pare
lowerthantheirpriorprobabilitiesaswewouldexpect.Forexample,Pr (B 1|E1)=0, 
because p=0isruledoutafteronesuccess.Also,Pr (B 2|E1)=0.0182,whichismuch 
smaller than its prior value 0 .0909, and Pr (B 11 |E1)=0.1818, which is larger than its 
prior value0 .0909. 
2.3 Bayes’ Theorem 83 
0.10.40.5
0.3
0.2
0B1B2B3B4B5B6B7B8B9B10 B11 
Figure 2.3 The posterior probabilities of partition 
elementsafter40patients inExample2.3.7.
We could check how the posterior probabilities behave after each patient is 
observed. However, we shall skip ahead to the point at which all 40 patients in the 
imipramine column of Table 2.1 have been observed. Let Astand for the observed
eventthat22ofthemaresuccessesand18arefailures.Wecanusethesamereasoning 
as in Example 2.2.5 to compute Pr (A |Bj). There are /parenleftbig40 
22 /parenrightbig
possible sequences of 40 
patients with 22 successes, and, conditional on Bj, the probability of each sequence 
is ([j−1] /10 )22 (1−[j−1] /10 )18 .
So, 
Pr (A |Bj)=/parenleftbigg40 
22 /parenrightbigg
([j−1] /10 )22 (1−[j−1] /10 )18 , (2.3.7)
for each j. ThenBayes’ theoremtells us that 
Pr (B j|A) =1
11 /parenleftbig40 
22 /parenrightbig
([j−1] /10 )22 (1−[j−1] /10 )18 
/summationtext11 
i=11
11 /parenleftbig40 
22 /parenrightbig
([i−1] /10 )22 (1−[i−1] /10 )18 .
Figure2.3showstheposteriorprobabilitiesofthe11partitionelementsafterobserv- 
ingA.Noticethattheprobabilitiesof B6andB7arethehighest,0.42.Thiscorresponds 
to the fact that the proportion of successes in the observed sample is 22 /40 =0.55, 
halfway between (6−1)/ 10 and (7−1)/ 10. 
Wecanalsocomputetheprobabilitythatthenextpatientwillbeasuccessboth 
before the trial and after the 40 patients. Before the trial, Pr (E 41 )=Pr (E 1), which 
equals 1/2, as computed in (2.1.6). After observing the 40 patients, we can compute 
Pr (E 41 |A) usingthe conditionalversion ofthe law oftotalprobability, (2.1.5): 
Pr (E 41 |A) =11 /summationdisplay
j=1Pr (E 41 |Bj∩A) Pr (B j|A). (2.3.8)
UsingthevaluesofPr (B j|A) inFig.2.3andthefactthatPr (E 41 |Bj∩A) =Pr (E 41 |Bj)
=(j −1)/ 10(conditionalindependenceofthe Eigiventhe Bj),wecompute(2.3.8) 
to be 0.5476.This is alsovery close tothe observed frequencyof success. ◭
The calculation at the end of Example 2.3.7 is typical of what happens after ob- 
servingmanyconditionallyindependenteventswiththesameconditionalprobability
of occurrence. The conditional probability of the next event given those that were 
observed tends to be close to the observed frequency of occurrence among the ob- 
servedevents.Indeed,whenthereissubstantialdata,thechoiceofpriorprobabilities 
becomesfarlessimportant.
84 Chapter 2 ConditionalProbability 
0.10.40.5
0.3
0.2
0B1B2B3B4B5B6B7B8B9B10 B11 X X X XXX X 
X
X X X 
Figure 2.4 The posterior probabilities of partition 
elements after 40 patients in Example 2.3.8. The X 
characters mark the values of the posterior probabilities 
calculatedinExample2.3.7.
Example 
2.3.8The Effect of Prior Probabilities . Consider the same clinical trial as in Example 2.3.7. 
Thistime,supposethatadifferentresearcherhasadifferentprioropinionaboutthe 
value of p, the probability of success. This researcher believes the following prior 
probabilities:
Event B1B2B3B4B5B6B7B8B9B10 B11 
p 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Prior prob. 0.00 0.19 0.19 0.17 0.14 0.11 0.09 0.06 0.04 0.01 0.00 
We can recalculate the posterior probabilities using Bayes’ theorem, and we get 
the values pictured in Fig. 2.4. To aid comparison, the posterior probabilities from 
Example 2.3.7 are also plotted in Fig. 2.4 using the symbol X. One can see how 
closethetwosetsofposteriorprobabilitiesaredespitethelargedifferencesbetween 
thepriorprobabilities.Iftherehadbeenfewerpatientsobserved,therewouldhave 
been larger differences between the two sets of posterior probabilites because the 
observed events would have provided less information. (See Exercise 12 in this 
section.) ◭
Summary
Bayes’theoremtellsushowtocomputetheconditionalprobabilityofeacheventina 
partitiongivenanobservedevent A.Amajoruseofpartitionsistodividethesample 
space into small enough pieces so that a collection of events of interest become 
conditionally independentgiven each eventin the partition. 
Exercises 
1. Suppose that kevents B1, . . . , B kform a partition of 
thesamplespace S.For i=1, . . . , k ,letPr (B i)denotethe
prior probability of Bi. Also, for each event Asuch that
Pr (A) >0, let Pr (B i|A) denote the posterior probabilityof Bigiven that the event Ahas occurred. Prove that if 
Pr (B 1|A) < Pr (B 1),thenPr (B i|A) > Pr (B i)foratleastone 
valueof i(i=2, . . . , k ). 
2.3 Bayes’ Theorem 85 
2. Consider again the conditions of Example 2.3.4 in this 
section, in which an item was selected at random from 
a batch of manufactured items and was found to be de- 
fective. For which values of i(i=1,2,3) is the posterior 
probability that the item was produced by machine Mi
larger than the prior probability that the item was pro-
duced by machine Mi?
3. SupposethatinExample2.3.4inthissection,theitem 
selectedatrandomfromtheentirelotisfoundtobenon- 
defective.Determinetheposteriorprobabilitythatitwas 
producedby machine M2.
4. A new test has been devised for detecting a particular 
typeofcancer.Ifthetestisappliedtoapersonwhohasthis 
typeofcancer,theprobabilitythatthepersonwillhavea 
positivereactionis0.95andtheprobabilitythattheperson 
willhaveanegativereactionis0.05.Ifthetestisappliedto 
apersonwhodoesnothavethistypeofcancer,theprob- 
abilitythatthepersonwillhaveapositivereactionis0.05 
andtheprobabilitythatthepersonwillhaveanegativere- 
actionis0.95.Supposethatinthegeneralpopulation,one 
person out of every 100,000 people has this type of can- 
cer.Ifapersonselectedatrandomhasapositivereaction 
to the test, what is the probability that he has this type of 
cancer?
5. In a certain city, 30 percent of the people are Conser- 
vatives, 50 percent are Liberals, and 20 percent are Inde- 
pendents. Records show that in a particular election, 65 
percentoftheConservativesvoted,82percentoftheLib- 
erals voted, and 50 percent of the Independents voted. If 
apersoninthecityisselectedatrandomanditislearned 
thatshedidnotvoteinthelastelection,whatistheprob- 
abilitythatshe is a Liberal? 
6. Suppose that when a machine is adjusted properly, 50 
percent of the items produced by it are of high quality 
andtheother50percentareofmediumquality.Suppose, 
however, that the machine is improperly adjusted during 
10percentofthetimeandthat,undertheseconditions,25 
percentoftheitemsproducedbyitareofhighqualityand 
75 percentare ofmediumquality. 
a. Suppose that ﬁve items produced by the machine at 
acertaintimeareselectedatrandomandinspected. 
Iffouroftheseitemsareofhighqualityandoneitem 
isofmediumquality,whatistheprobabilitythatthe 
machine wasadjustedproperlyatthattime? 
b. Suppose that one additional item, which was pro-
duced by the machine at the same time as the other 
ﬁve items, is selected and found to be of medium 
quality. What is the new posterior probability that 
themachine was adjustedproperly?
7. Suppose that a box contains ﬁve coins and that for 
each coin there is a different probability that a head will 
be obtained when the coin is tossed. Let pidenote the
probability of a head when the ith coin is tossed ( i=1, . . . , 5), and suppose that p1=0, p2=1/4, p3=1/2, 
p4=3/4, and p5=1. 
a. Supposethatonecoinisselectedatrandomfromthe 
box and when it is tossed once, a head is obtained. 
Whatistheposteriorprobabilitythatthe ithcoinwas 
selected( i=1, . . . , 5)? 
b. If the same coin were tossed again, what would be 
theprobabilityofobtaining another head? 
c. If a tail had been obtained on the ﬁrst toss of the 
selected coin and the same coin were tossed again,
what would be the probability of obtaining a head 
onthe secondtoss? 
8. Consider again the box containing the ﬁve different
coins described in Exercise 7. Suppose that one coin is 
selectedatrandomfromtheboxandistossedrepeatedly 
until a head is obtained. 
a. If the ﬁrst head is obtained on the fourth toss, what 
is the posterior probability that the ith coin was se- 
lected( i=1, . . . , 5)? 
b. If we continue to toss the same coin until another 
headisobtained,whatistheprobabilitythatexactly 
three additionaltosseswill be required? 
9. ConsideragaintheconditionsofExercise14inSec.2.1. 
Suppose that several parts will be observed and that the 
different parts are conditionally independent given each
ofthethreestatesofrepairofthemachine.Ifsevenparts 
are observed and exactly one is defective, compute the 
posterior probabilities ofthe three states ofrepair. 
10.Consider again the conditions of Example 2.3.5, in 
which the phenotype of an individual was observed and 
found to be the dominant trait. For which values of i
(i=1, . . . , 6)istheposteriorprobabilitythattheparents 
have the genotypes of event Bismaller than the prior
probability that the parents have the genotyes of event 
Bi?
11.SupposethatinExample2.3.5theobservedindividual 
has the recessive trait. Determine the posterior probabil-
itythat theparentshavethe genotypesofevent B4.
12.In the clinical trial in Examples 2.3.7 and 2.3.8, sup- 
posethatwehaveonlyobservedtheﬁrstﬁvepatientsand 
threeoftheﬁvehadbeensuccesses.Usethetwodifferent 
sets of prior probabilities from Examples 2.3.7 and 2.3.8 
to calculate two sets of posterior probabilities. Are these 
two sets of posterior probabilities as close to each other 
aswerethetwoinExamples2.3.7and2.3.8?Whyorwhy 
not?
13.Supposethataboxcontainsonefaircoinandonecoin 
with a head on each side. Suppose that a coin is drawn at 
random from this box and that we begin to ﬂip the coin. 
In Eqs. (2.3.4) and (2.3.5), we computed the conditional 
86 Chapter 2 ConditionalProbability 
probability that the coin was fair given that the ﬁrst two
ﬂips both produceheads.
a. Suppose that the coin is ﬂipped a third time and 
another head is obtained. Compute the probability 
thatthecoinisfairgiventhatallthreeﬂipsproduced 
heads.
b. Supposethatthecoinisﬂippedafourthtimeandthe 
resultistails.Computetheposteriorprobabilitythat 
thecoin is fair. 
14.Consider again the conditions of Exercise 23 in Sec. 
2.2. Assume that Pr (B) =0.4. Let Abe the event that ex- 
actly8outof11programscompiled.Computethecondi- 
tionalprobability of Bgiven A.
15.Use the prior probabilities in Example 2.3.8 for the 
events B1, . . . , B 11 . Let E1be the event that the ﬁrst pa- 
tient is a success. Compute the probability of E1and ex-
plain why it is so much less than the value computed in 
Example 2.3.7.
16.Consideramachinethatproducesitemsinsequence. 
Under normal operating conditions, the items are independent with probability 0.01 of being defective. 
However, it is possible for the machine to develop a 
“memory” in the following sense: After each defective 
item,andindependentofanythingthathappenedearlier, 
the probability that the next item is defective is 2/5. Af- 
ter each nondefective item, and independent of anything 
that happened earlier, the probability that the next item 
is defectiveis 1/165. 
Assumethatthemachineiseitheroperatingnormally 
for the whole time we observe or has a memory for the 
whole time that we observe. Let Bbe the event that the 
machine is operating normally, and assume that Pr (B) =
2/3. Let Dibe the event that the ith item inspected is 
defective. Assume that D1is independentof B.
a. Prove that Pr (D i)=0.01 for all i.Hint:Use induc-
tion.
b. Assume that we observe the ﬁrst six items and the 
event that occurs is E=Dc
1∩Dc
2∩D3∩D4∩Dc
5∩
Dc
6. That is, the third and fourth items are defective, 
but theotherfour are not.ComputePr (B |D) .
⋆2.4 The Gambler’s Ruin Problem 
Consider two gamblers with ﬁnite resources who repeatedly play the same game 
againsteachother.Usingthetoolsofconditionalprobability,wecancalculatethe 
probability that each of the gamblers will eventually lose all of his money to the 
opponent.
Statement of the Problem 
Suppose that two gamblers AandBare playing a game against each other. Let p
be a given number (0 < p < 1), and suppose that on each play of the game, the 
probabilitythatgambler Awillwinonedollarfromgambler Bis pandtheprobability
that gambler Bwill win one dollar from gambler Ais 1 −p. Suppose also that the 
initial fortune of gambler Ais idollars and the initial fortune of gambler Bis k−i
dollars, where iandk−iare given positive integers. Thus, the total fortune of the 
twogamblersis kdollars.Finally,supposethatthegamblersplaythegamerepeatedly 
and independently until the fortune of one of them has been reduced to 0 dollars. 
Anotherwaytothinkaboutthisproblemisthat Bisacasinoand Aisagamblerwho 
is determined to quit as soon he wins k−idollars from the casino or when he goes 
broke, whichever comes ﬁrst. 
Weshallnowconsiderthisgamefromthepointofviewofgambler A.Hisinitial 
fortuneis idollarsandoneachplayofthegamehisfortunewilleitherincreasebyone 
dollar with a probability of por decrease by one dollar with a probability of 1 −p.
If p > 1/2, the game is favorable to him; if p < 1/2, the game is unfavorable to him; 
andif p=1/2,thegameisequallyfavorabletobothgamblers.Thegameendseither 
when the fortune of gambler Areaches kdollars, in which case gambler Bwill have
nomoneyleft,orwhenthefortuneofgambler Areaches0dollars.Theproblemisto 
2.4 TheGambler’s RuinProblem 87 
determine the probability that the fortune of gambler Awill reach kdollars before
it reaches 0 dollars. Because one of the gamblers will have no money left at the end 
ofthe game, this problemis called the Gambler’sRuin problem.
Solution of the Problem 
Weshallcontinuetoassumethatthetotalfortuneofthegamblers AandBis kdollars,
and we shall let aidenote the probability that the fortune of gambler Awill reach k
dollarsbeforeitreaches0dollars,giventhathisinitialfortuneis idollars.Weassume 
thatthegameisthesameeachtimeitisplayedandtheplaysareindependentofeach 
other.Itfollowsthat,aftereachplay,theGambler’sRuinproblemessentiallystarts 
over with the only change being that the initial fortunes of the two gamblers have 
changed. In particular, for each j=0, . . . , k , each time that we observe a sequence 
ofplaysthatleadtogambler A’sfortunebeing jdollars,theconditionalprobability, 
givensuchasequence,thatgambler Awinsis aj.Ifgambler A’sfortuneeverreaches 
0, then gambler Ais ruined, hence a0=0. Similarly, if his fortune ever reaches k,
then gambler Ahas won, hence ak=1. We shall now determine the value of aifor
i=1, . . . , k −1. 
LetA1denote the event that gambler Awins one dollar on the ﬁrst play of the 
game,let B1denotetheeventthatgambler Alosesonedollarontheﬁrstplayofthe 
game, and let Wdenote the event that the fortune of gambler Aultimately reaches
kdollarsbefore itreaches 0 dollars. Then 
Pr (W) =Pr (A 1)Pr (W |A1)+Pr (B 1)Pr (W |B1)
=pPr (W |A1)+(1−p) Pr (W |B1). (2.4.1)
Sincetheinitialfortuneofgambler Ais idollars( i=1, . . . , k −1),thenPr (W) =ai.
Furthermore, if gambler Awins one dollar on the ﬁrst play of the game, then his 
fortune becomes i+1 dollars and the conditional probability Pr (W |A1)that his
fortune will ultimately reach kdollars is therefore ai+1. If Aloses one dollar on the 
ﬁrst play of the game, then his fortune becomes i−1 dollars and the conditional 
probabilityPr (W |B1)thathisfortunewillultimatelyreach kdollarsistherefore ai−1.
Hence, by Eq. (2.4.1), 
ai=pa i+1+(1−p)a i−1. (2.4.2)
We shall let i=1, . . . , k −1 in Eq. (2.4.2). Then, since a0=0 and ak=1, we 
obtain thefollowing k−1equations: 
a1=pa 2,
a2=pa 3+(1−p)a 1,
a3=pa 4+(1−p)a 2,
...
ak−2=pa k−1+(1−p)a k−3,
ak−1=p+(1−p)a k−2.(2.4.3)
If the value of aion the left side of the ith equation is rewritten in the form pa i+
(1−p)a iandsomeelementaryalgebraisperformed,thenthese k−1equationscan 
88 Chapter 2 ConditionalProbability 
be rewritten as follows: 
a2−a1=1−p
pa1,
a3−a2=1−p
p(a 2−a1)=/parenleftbigg1−p
p/parenrightbigg2
a1,
a4−a3=1−p
p(a 3−a2)=/parenleftbigg1−p
p/parenrightbigg3
a1,
...
ak−1−ak−2=1−p
p(a k−2−ak−3)=/parenleftbigg1−p
p/parenrightbiggk−2
a1,
1−ak−1=1−p
p(a k−1−ak−2)=/parenleftbigg1−p
p/parenrightbiggk−1
a1.(2.4.4)
By equating the sum of the left sides of these k−1 equations with the sum of the 
rightsides, we obtainthe relation 
1−a1=a1k−1/summationdisplay
i=1/parenleftbigg1−p
p/parenrightbiggi
. (2.4.5)
Solution for a Fair Game Suppose ﬁrst that p=1/2. Then (1−p)/p =1, and it 
followsfromEq.(2.4.5)that1 −a1=(k −1)a 1,fromwhich a1=1/k .Inturn,itfollows 
fromtheﬁrstequationin(2.4.4)that a2=2/k ,itfollowsfromthesecondequationin 
(2.4.4)that a3=3/k ,andsoon.Inthisway,weobtainthefollowingcompletesolution 
when p=1/2: 
ai=i
kfori=1, . . . , k −1. (2.4.6)
Example 
2.4.1The Probability of Winning in a Fair Game . Suppose that p=1/2, in which case the 
game is equally favorable to both gamblers; and suppose that the initial fortune of 
gambler Ais 98 dollars and the initial fortune of gambler Bis just two dollars. In 
this example, i=98 and k=100. Therefore, it follows from Eq. (2.4.6) that there 
is a probability of 0.98 that gambler Awill win two dollars from gambler Bbefore
gambler Bwins 98 dollars fromgambler A. ◭
Solution for an Unfair Game Suppose now that p≈negationslash=1/2. Then Eq. (2.4.5) can be 
rewritten in the form 
1−a1=a1/parenleftbigg
1−p
p/parenrightbiggk
−/parenleftbigg
1−p
p/parenrightbigg
/parenleftbigg
1−p
p/parenrightbigg
−1. (2.4.7)
Hence,
a1=/parenleftbigg
1−p
p/parenrightbigg
−1
/parenleftbigg
1−p
p/parenrightbiggk
−1. (2.4.8)
2.4 TheGambler’s RuinProblem 89 
Each of the other values of aifori=2, . . . , k −1 can now be determined in turn 
fromtheequationsin(2.4.4).Inthisway,weobtainthefollowingcompletesolution: 
ai=/parenleftbigg
1−p
p/parenrightbiggi
−1
/parenleftbigg
1−p
p/parenrightbiggk
−1fori=1, . . . , k −1. (2.4.9)
Example 
2.4.2The Probability of Winning in an Unfavorable Game . Suppose that p=0.4, in which 
case the probability that gambler Awill win one dollar on any given play is smaller 
thantheprobabilitythathewillloseonedollar.Supposealsothattheinitialfortune 
ofgambler Ais99dollarsandtheinitialfortuneofgambler Bisjustonedollar.We 
shall determine the probability that gambler Awill win one dollar from gambler B
beforegambler Bwins99 dollars fromgambler A.
In this example, the required probability aiis given by Eq. (2.4.9), in which 
(1−p)/p =3/2, i=99, and k=100. Therefore, 
ai=/parenleftBig3
2/parenrightBig99 
−1
/parenleftBig3
2/parenrightBig100
−1≈1
3/2=2
3.
Hence,althoughtheprobabilitythatgambler Awillwinonedollaronanygivenplay 
is only 0.4, the probability that he will win one dollar before he loses 99 dollars is 
approximately 2/3. ◭
Summary
We considered a gambler and an opponent who each start with ﬁnite amounts of 
money.Thetwothenplayasequenceofgamesagainsteachotheruntiloneofthem 
runsoutofmoney.Wewereabletocalculatetheprobabilitythateachofthemwould 
betheﬁrsttorunoutasafunctionoftheprobabilityofwinningthegameandofhow 
muchmoney eachhas atthe start. 
Exercises 
1. ConsidertheunfavorablegameinExample2.4.2.This 
time, suppose that the initial fortune of gambler Ais i
dollars with i≤98. Suppose that the initial fortune of 
gambler Bis 100 −idollars. Show that the probability 
is greater than 1/2 that gambler Alosses idollars before
winning 100−idollars.
2. Consider the following three different possible condi-
tions in the gambler’s ruin problem: 
a. The initial fortune of gambler Ais two dollars, and 
the initialfortune ofgambler Bis one dollar. 
b. Theinitialfortuneofgambler Ais20dollars,andthe 
initial fortuneofgambler Bis 10 dollars. 
c. The initial fortune of gambler Ais 200 dollars, and 
the initial fortuneofgambler Bis 100 dollars. Supposethat p=1/2.Forwhichofthesethreecondi- 
tions is there the greatest probability that gambler Awill
win the initial fortune of gambler Bbefore he loses his 
owninitial fortune?
3. Consider again the three different conditions (a), (b),
and(c)giveninExercise2,butsupposenowthat p < 1/2. 
For which of these three conditions is there the greatest 
probability that gambler Awill win the initial fortune of 
gambler Bbeforehe loses his own initialfortune? 
4. Consider again the three different conditions (a), (b),
and(c)giveninExercise2,butsupposenowthat p > 1/2. 
For which of these three conditions is there the greatest 
probability that gambler Awill win the initial fortune of 
gambler Bbeforehe loses his own initialfortune? 
90 Chapter 2 ConditionalProbability 
5. Supposethatoneachplayofacertaingame,apersonis 
equallylikelytowinonedollarorloseonedollar.Suppose 
alsothattheperson’sgoalistowintwodollarsbyplaying 
this game. How large an initial fortune must the person 
haveinorderfortheprobabilitytobeatleast0.99thatshe 
willachievehergoalbeforeshelosesherinitialfortune?
6. Suppose that on each play of a certain game, a person 
will either win one dollar with probability 2/3 or lose one 
dollarwithprobability1/3.Supposealsothattheperson’s
goalistowintwodollarsbyplayingthisgame.Howlarge 
an initial fortune must the person have in order for the 
probabilitytobeatleast0.99thathewillachievehisgoal 
beforehe loses his initialfortune? 
7. Suppose that on each play of a certain game, a person 
will either win one dollar with probability 1/3 or lose one 
dollarwithprobability2/3.Supposealsothattheperson’s
goalistowintwodollarsbyplayingthisgame.Showthat 
nomatterhowlargetheperson’sinitialfortunemightbe, the probability that she will achieve her goal before she
loses her initialfortune is less than1/4. 
8. Suppose that the probability of a head on any toss of 
a certain coin is p(0 < p < 1), and suppose that the coin 
is tossed repeatedly. Let Xndenote the total number of 
heads that have been obtained on the ﬁrst ntosses, and 
letYn=n−Xndenote the total number of tails on the 
ﬁrstntosses. Suppose that the tosses are stopped as soon 
as a number nis reached such that either Xn=Yn+3 or 
Yn=Xn+3. Determine the probability that Xn=Yn+3
when the tosses are stopped.
9. Supposethatacertainbox Acontainsﬁveballsandan-
other box Bcontains 10 balls. One of these two boxes is 
selected at random, and one ball from the selected box is 
transferred to the other box. If this process of selecting a 
box at random and transferring one ball from that box to 
theotherboxisrepeatedindeﬁnitely,whatistheprobabil- 
ity that box Awill become empty before box Bbecomes
empty?
2.5 Supplementary Exercises 
1. Supposethat A,B,and Dareanythreeeventssuchthat
Pr (A |D) ≥Pr (B |D) andPr (A |Dc)≥Pr (B |Dc).Provethat 
Pr (A)≥Pr (B) .
2. Suppose that a fair coin is tossed repeatedly and inde- 
pendently until both a head and a tail have appeared at 
least once. (a)Describe the sample space of this experi- 
ment.(b)Whatistheprobabilitythatexactlythreetosses 
will be required? 
3. Suppose that AandBare events such that Pr (A)=
1/3,Pr (B) =1/5,andPr (A |B) +Pr (B |A) =2/3.Evaluate 
Pr (A c∪Bc).
4. Supposethat AandBareindependenteventssuchthat
Pr (A)=1/3 and Pr (B) > 0. What is the value of Pr (A ∪
Bc|B) ?
5. Supposethatin10rollsofabalanceddie,thenumber6 
appearedexactlythreetimes.Whatistheprobabilitythat 
the ﬁrstthreerollseach yielded thenumber6? 
6. Suppose that A,B, and Dare events such that Aand
Bareindependent,Pr (A ∩B∩D) =0.04,Pr (D |A∩B) =
0.25, andPr (B) =4Pr (A). Evaluate Pr (A ∪B) .
7. Suppose that the events A,B, and Care mutually in-
dependent. Under what conditions are Ac,Bc, and Cc
mutually independent?
8. Suppose that the events AandBare disjoint and that
each haspositive probability. Are AandBindependent?
9. Suppose that A,B, and Care three events such that A
andBare disjoint, AandCare independent, and BandCareindependent.Supposealsothat4Pr (A)=2Pr (B) =
Pr (C) > 0 and Pr (A ∪B∪C) =5Pr (A). Determine the 
valueofPr (A).
10.Suppose that each of two dice is loaded so that when 
either die is rolled, the probability that the number kwill
appearis0.1for k=1,2,5,or6andis0.3for k=3or4.If 
the two loaded dice are rolled independently, what is the 
probability that the sum of the two numbers that appear 
will be 7? 
11.Suppose that there is a probability of 1/50 that you 
will win a certain game. If you play the game 50 times, 
independently,whatistheprobabilitythatyouwillwinat 
least once?
12.Supposethatabalanceddieisrolledthreetimes,and 
letXidenote the number that appears on the ith roll 
(i =1,2,3).EvaluatePr (X 1> X 2> X 3).
13.Three students A,B, and Care enrolled in the same 
class.Supposethat Aattendsclass30percentofthetime, 
Battends class 50 percent of the time, and Cattends
class 80 percent of the time. If these students attend class 
independently of each other, what is (a) the probability 
that at least one of them will be in class on a particular 
day and (b) the probability that exactly one of them will 
bein class ona particular day? 
14.ConsidertheWorldSeriesofbaseball,asdescribedin 
Exercise 16 of Sec. 2.2. If there is probability pthat team
Awill win any particular game, what is the probability 
2.5 SupplementaryExercises 91 
that it will be necessary to play seven games in order to 
determine thewinnerofthe Series? 
15.Supposethatthreeredballsandthreewhiteballsare
thrownatrandomintothreeboxesandandthatallthrows 
are independent. What is the probability that each box 
contains one red balland one white ball?
16.Ifﬁveballsarethrownatrandominto nboxes,andall 
throws are independent, what is the probability that no 
box contains morethan twoballs?
17.Bus tickets in a certain city contain four numbers, U,
V,W, and X. Each of these numbers is equally likely to 
be any of the 10 digits 0 ,1, . . . , 9, and the four numbers 
arechosenindependently.Abusriderissaidtobeluckyif 
U+V=W+X.Whatproportionoftheridersarelucky? 
18.Acertaingrouphaseightmembers.InJanuary,three 
members are selected at random to serve on a commit- 
tee. In February, four members are selected at random 
and independently of the ﬁrst selection to serve on an- 
other committee. In March, ﬁve members are selected at 
randomandindependentlyoftheprevioustwoselections 
to serve on a third committee. Determine the probability 
that each of the eight members serves on at least one of 
the three committees.
19.FortheconditionsofExercise18,determinetheprob- 
ability that two particular members AandBwill serve
together on atleastone ofthe three committees. 
20.Supposethattwoplayers AandBtaketurnsrollinga
pairofbalanceddiceandthatthewinneristheﬁrstplayer 
who obtains the sum of 7 on a given roll of the two dice. 
If Arolls ﬁrst,what is the probability that Bwillwin?
21.Three players A,B, and Ctake turns tossing a fair 
coin.Supposethat Atossesthecoinﬁrst, Btossessecond,
andCtossesthird;andsupposethatthiscycleisrepeated 
indeﬁnitely until someone wins by being the ﬁrst player 
to obtain a head. Determine the probability that each of 
three players willwin.
22.Supposethatabalanceddieisrolledrepeatedlyuntil 
the same number appears on two successive rolls, and let 
Xdenotethenumberofrollsthatarerequired.Determine 
the value ofPr (X =x) , for x=2,3, . . . . 
23.Suppose that 80 percent of all statisticians are shy, 
whereasonly15percentofalleconomistsareshy.Suppose 
alsothat90percentofthepeopleatalargegatheringare 
economists and the other 10 percent are statisticians. If 
youmeetashypersonatrandomatthegathering,whatis 
the probabilitythat thepersonis a statistician? 
24.Dreamboat cars are produced at three different fac- 
tories A,B, and C. Factory Aproduces 20 percent of the 
total output of Dreamboats, Bproduces 50 percent, and 
Cproduces 30 percent. However, 5 percent of the cars 
produced at Aare lemons, 2 percent of those produced at Bare lemons, and 10 percent of those produced at C
arelemons.IfyoubuyaDreamboatanditturnsouttobe 
a lemon, what is the probability that it was produced at 
factory A?
25.Suppose that 30 percent of the bottles produced in 
a certain plant are defective. If a bottle is defective, the 
probability is 0.9 that an inspector will notice it and re- 
move it from the ﬁlling line. If a bottle is not defective, 
theprobabilityis0.2thattheinspectorwillthinkthatitis 
defectiveand removeitfromtheﬁlling line. 
a. Ifabottleisremovedfromtheﬁllingline,whatisthe 
probabilitythat itis defective? 
b. Ifacustomerbuysabottlethathasnotbeenremoved 
from the ﬁlling line, what is the probability that it is 
defective?
26.Suppose that a fair coin is tossed until a head is ob- 
tained and that this entire experiment is then performed 
independentlyasecondtime.Whatistheprobabilitythat 
thesecondexperimentrequiresmoretossesthantheﬁrst
experiment?
27.Suppose that a family has exactly nchildren ( n≥2). 
Assume that the probability that any child will be a girl 
is 1/2 and that all births are independent. Given that the 
familyhasatleastonegirl,determinetheprobabilitythat 
thefamilyhas atleastoneboy. 
28.Suppose that a fair coin is tossed independently n
times.Determinetheprobabilityofobtainingexactly n−
1 heads, given (a)that at least n−2 heads are obtained 
and(b)thatheads are obtained onthe ﬁrst n−2 tosses. 
29.Suppose that 13 cards are selected at random from a 
regular deckof52 playingcards. 
a. Ifitisknownthatatleastoneacehasbeenselected, 
what is the probability that at least two aces have 
been selected?
b. Ifitisknownthattheaceofheartshasbeenselected, 
what is the probability that at least two aces have 
been selected?
30.Suppose that nletters are placed at random in nen-
velopes,asinthematchingproblemofSec.1.10,andlet qn
denote the probability that no letter is placed in the cor- 
rect envelope. Show that the probability that exactly one 
letter is placed inthe correctenvelope is qn−1.
31.Consider again the conditions of Exercise 30. Show 
that the probability that exactly two letters are placed in 
thecorrectenvelopes is (1/2)q n−2.
32.ConsideragaintheconditionsofExercise7ofSec.2.2. 
If exactly one of the two students AandBis in class on a 
givenday, whatis theprobabilitythatitis A?
33.Consider again the conditions of Exercise 2 of Sec. 
1.10. If a family selected at random from the city 
92 Chapter 2 ConditionalProbability 
subscribes to exactly one of the three newspapers A,B,
andC, whatis the probability thatitis A?
34.Three prisoners A,B, and Con death row know that 
exactlytwoofthemaregoingtobeexecuted,buttheydo 
notknowwhichtwo.Prisoner Aknowsthatthejailerwill
nottellhimwhetherornotheisgoingtobeexecuted.He 
therefore asks the jailer to tell him the name of one pris- 
onerotherthan Ahimselfwhowillbeexecuted.Thejailer 
responds that Bwill be executed. Upon receiving this re- 
sponse,Prisoner Areasonsasfollows:Beforehespoketo 
thejailer,theprobabilitywas2/3thathewouldbeoneof 
the two prisoners executed. After speaking to the jailer, 
he knows that either he or prisoner Cwill be the other 
onetobeexecuted.Hence,theprobabilitythathewillbe 
executedisnowonly1/2.Thus,merelybyaskingthejailer 
hisquestion,theprisonerreducedtheprobabilitythathe 
would be executed from 2/3 to 1/2, because he could go 
through exactly this same reasoning regardless of which 
answerthejailergave.Discusswhatiswrongwithprisoner 
A’s reasoning. 
35.Suppose that each of two gamblers AandBhas an 
initial fortune of 50 dollars, and that there is probability 
pthat gambler Awill win on any single play of a game 
againstgambler B.Also,supposeeitherthatonegambler 
canwinonedollarfromtheotheroneachplayofthegame 
or that they can double the stakes and one can win two 
dollars from the other on each play of the game. Under 
which of these two conditions does Ahave the greater
probabilityofwinningtheinitialfortuneof Bbeforelosing
herownforeachofthefollowingconditions: (a)p < 1/2; 
(b)p > 1/2; (c)p=1/2? 
36.A sequence of njob candidates is prepared to inter- 
view for a job. We would like to hire the best candidate, 
but we have no information to distinguish the candidates beforeweinterviewthem.Weassumethatthebestcandi- 
dateisequallylikelytobeeachofthe ncandidatesinthe 
sequencebeforetheinterviewsstart.Aftertheinterviews
start, we are able to rank those candidates we have seen, 
but we have no information about where the remaining 
candidatesrankrelativetothosewehaveseen.Aftereach 
interview,itisrequiredthateitherwehirethecurrentcan- 
didateimmediatelyandstoptheinterviews,orwemustlet 
thecurrentcandidategoandwenevercancallthemback. 
We choose to interview as follows: We select a number 
0≤r < n and we interview the ﬁrst rcandidates without
any intention of hiring them. Starting with the next can- 
didate r+1, we continue interviewing until the current 
candidate is the best we have seen so far. We then stop 
and hire the current candidate. If none of the candidates 
fromr+1 to nis the best, we just hire candidate n. We 
wouldliketocomputetheprobabilitythatwehirethebest 
candidateandwewouldliketochoose rtomakethisprob- 
abilityaslargeaspossible.Let Abetheeventthatwehire 
the best candidate, and let Bibe the event that the best 
candidate is in position iin the sequenceofinterviews. 
a. Leti > r .Findtheprobabilitythatthecandidatewho 
is relatively the best among the ﬁrst iinterviewed
appears in the ﬁrst rinterviews.
b. Prove that Pr (A |Bi)=0 for i≤rand Pr (A |Bi)=
r/(i −1)fori > r .
c. For ﬁxed r, let prbe the probability of Ausing that
value of r.Prove that pr=(r/n) /summationtextn
i=r+1(i −1)−1.
d. Letqr=pr−pr−1forr=1, . . . , n −1, and prove 
thatqris a strictly decreasing function of r.
e. Showthatavalueof rthatmaximizes pristhelast r
suchthat qr>0.( Hint:Write pr=p0+q1+. . . +qr
forr > 0.) 
f. For n=10,ﬁndthevalueof rthatmaximizes pr,and 
ﬁndthe corresponding prvalue.
Chapter 
3Random V ariables 
and Distributions 
3.1 Random Variables and Discrete Distributions 
3.2 Continuous Distributions
3.3 The Cumulative Distribution Function 
3.4 Bivariate Distributions
3.5 Marginal Distributions
3.6 Conditional Distributions3.7 Multivariate Distributions
3.8 Functions of a Random Variable 
3.9 Functions of Two or More Random Variables 
3.10 Markov Chains
3.11 Supplementary Exercises
3.1 Random Variables and Discrete Distributions 
Arandomvariableisareal-valuedfunctiondeﬁnedonasamplespace.Random 
variables are the main tools used for modeling unknown quantities in statistical 
analyses. For each random variable Xand each set Cof real numbers, we could 
calculatetheprobabilitythat Xtakesitsvaluein C.Thecollectionofallofthese 
probabilitiesisthedistributionof X.Therearetwomajorclassesofdistributions 
and random variables: discrete (this section) and continuous (Sec. 3.2). Discrete 
distributionsarethosethatassignpositiveprobabilitytoatmostcountablymany 
different values. A discrete distribution can be characterized by its probability 
function(p.f.),whichspeciﬁestheprobabilitythattherandomvariabletakeseach 
ofthedifferentpossiblevalues.Arandomvariablewithadiscretedistributionwill 
becalledadiscreterandomvariable.
Deﬁnition of a Random Variable 
Example 
3.1.1 Tossing a Coin . Consider an experiment in which a fair coin is tossed 10 times. In this 
experiment, the sample space Scan be regarded as the set of outcomes consisting of 
the 210 different sequences of 10 heads and/or tails that are possible. We might be 
interested in the number of heads in the observed outcome. We can let Xstand for the
real-valued function deﬁned on Sthat counts the number of heads in each outcome. 
For example, if sis the sequence HHTTTHTTTH, then X(s)=4. For each possible 
sequencesconsisting of 10 heads and/or tails, the value X(s) equals the number of 
heads in the sequence. The possible values for the function Xare 0,1,..., 10. ◭
Deﬁnition
3.1.1 Random Variable . Let Sbe the sample space for an experiment. A real-valued func- 
tion that is deﬁned on Sis called a randomvariable .
For example, in Example 3.1.1, the number Xof heads in the 10 tosses is a random 
variable. Another random variable in that example is Y=10 −X, the number of 
tails.
93 
94 Chapter 3 Random Variables and Distributions 
Figure 3.1 The event that 
at least one utility demand is 
high in Example 3.1.3. 
1150
115
04 100 Water Electric
200A/H20668B
Example 
3.1.2 Measuring a Person’s Height . Consider an experiment in which a person is selected at 
random from some population and her height in inches is measured. This height is a 
random variable. ◭
Example 
3.1.3 Demands for Utilities . Consider the contractor in Example 1.5.4 on page 19 who is 
concerned about the demands for water and electricity in a new ofﬁce complex. The 
sample space was pictured in Fig. 1.5 on page 12, and it consists of a collection of 
points of the form (x,y) , where xis the demand for water and yis the demand 
for electricity. That is, each point s∈Sis a pair s=(x,y) . One random variable 
that is of interest in this problem is the demand for water. This can be expressed 
as X(s)=xwhens=(x,y) . The possible values of Xare the numbers in the interval 
[4 ,200]. Another interesting random variable is Y, equal to the electricity demand, 
which can be expressed as Y(s)=ywhens=(x,y) . The possible values of Yare the
numbers in the interval [1 ,150]. A third possible random variable Zis an indicator of 
whether or not at least one demand is high. Let AandBbe the two events described 
in Example 1.5.4. That is, Ais the event that water demand is at least 100, and Bis 
the event that electric demand is at least 115. Deﬁne 
Z(s)=/braceleftbigg1 if s∈A∪B,
0 if s/negationslash∈A∪B.
The possible values of Zare the numbers 0 and 1. The event A∪Bis indicated in 
Fig. 3.1. ◭
The Distribution of a Random Variable 
When a probability measure has been speciﬁed on the sample space of an experiment, 
we can determine probabilities associated with the possible values of each random 
variableX. Let Cbe a subset of the real line such that {X∈C}is an event, and let 
Pr (X ∈C)denote the probability that the value of Xwill belong to the subset C.
Then Pr (X ∈C)is equal to the probability that the outcome sof the experiment will 
be such that X(s)∈C. In symbols, 
Pr (X ∈C)=Pr ({s:X(s)∈C}). (3.1.1)
Deﬁnition
3.1.2 Distribution . Let Xbe a random variable. The distribution of Xis the collection of all 
probabilities of the form Pr (X ∈C)for all setsCof real numbers such that {X∈C}
is an event. 
It is a straightforward consequence of the deﬁnition of the distribution of Xthat
this distribution is itself a probability measure on the set of real numbers. The set 
3.1 Random Variables and Discrete Distributions 95 
Figure 3.2 The event that 
water demand is between 50 
and 175 in Example 3.1.5. 
1150
115
04 100 Water Electric
200175
{X∈C}will be an event for every set Cof real numbers that most readers will be 
able to imagine. 
Example 
3.1.4 Tossing a Coin . Consider again an experiment in which a fair coin is tossed 10 times, 
and letXbe the number of heads that are obtained. In this experiment, the possible 
values of Xare 0,1,2,..., 10. For each x, Pr (X =x)is the sum of the probabilities 
of all of the outcomes in the event {X=x}. Because the coin is fair, each outcome 
has the same probability 1 /210 , and we need only count how many outcomes shave
X(s)=x. We know that X(s)=xif and only if exactly xof the 10 tosses are H. Hence, 
the number of outcomes swithX(s)=xis the same as the number of subsets of size 
x(to be the heads) that can be chosen from the 10 tosses, namely, /parenleftbig10 
x/parenrightbig
, according to 
Deﬁnitions 1.8.1 and 1.8.2. Hence,
Pr (X =x)=/parenleftbigg10 
x/parenrightbigg1
210 forx=0,1,2,..., 10 . ◭
Example 
3.1.5 Demands for Utilities . In Example 1.5.4, we actually calculated some features of the 
distributions of the three random variables X,Y, and Zdeﬁned in Example 3.1.3. 
For example, the event A, deﬁned as the event that water demand is at least 100, can 
be expressed as A={X≥100}, and Pr (A)=0.5102. This means that Pr (X ≥100)=
0.5102. The distribution of Xconsists of all probabilities of the form Pr (X ∈C)for all
setsCsuch that{X∈C}is an event. These can all be calculated in a manner similar 
to the calculation of Pr (A)in Example 1.5.4. In particular, if Cis a subinterval of the 
interval [4 ,200], then 
Pr (X ∈C)=(150−1)×(length of interval C)
29 ,204. (3.1.2)
For example, if Cis the interval [50 ,175], then its length is 125, and Pr (X ∈C)=
149×125/29 ,204=0.6378. The subset of the sample space whose probability was 
just calculated is drawn in Fig. 3.2. ◭
The general deﬁnition of distribution in Deﬁnition 3.1.2 is awkward, and it will 
be useful to ﬁnd alternative ways to specify the distributions of random variables. In 
the remainder of this section, we shall introduce a few such alternatives. 
Discrete Distributions 
Deﬁnition
3.1.3 Discrete Distribution/Random Variable . We say that a random variable Xhas adiscrete
distribution or that Xis a discreterandomvariable if Xcan take only a ﬁnite number 
kof different values x1,...,x kor, at most, an inﬁnite sequence of different values 
x1,x 2,....
96 Chapter 3 Random Variables and Distributions 
Random variables that can take every value in an interval are said to have continuous
distributions and are discussed in Sec. 3.2. 
Deﬁnition
3.1.4 Probability Function/p.f./Support . If a random variable Xhas a discrete distribution, 
theprobabilityfunction (abbreviated p.f.) of Xis deﬁned as the function fsuch that
for every real number x,
f(x)=Pr (X =x).
The closure of the set {x:f(x)> 0}is called the supportof(thedistributionof) X.
Some authors refer to the probability function as the probability mass function, or 
p.m.f. We will not use that term again in this text. 
Example 
3.1.6 Demands for Utilities . The random variable Zin Example 3.1.3 equals 1 if at least one 
of the utility demands is high, and Z=0 if neither demand is high. Since Ztakes only
two different values, it has a discrete distribution. Note that {s:Z(s) =1}= A∪B,
whereAandBare deﬁned in Example 1.5.4. We calculated Pr (A ∪B) =0.65253 in 
Example 1.5.4. If Zhas p.f. f, then 
f(z) =

0.65253 if z=1, 
0.34747 if z=0, 
0 otherwise. 
The support of Zis the set {0,1}, which has only two elements. ◭
Example 
3.1.7 Tossing a Coin . The random variable Xin Example 3.1.4 has only 11 different possible 
values. Its p.f. fis given at the end of that example for the values x=0,..., 10 that 
constitute the support of X;f(x) =0 for all other values of x. ◭
Here are some simple facts about probability functions
Theorem
3.1.1 LetXbe a discrete random variable with p.f. f. If xis not one of the possible values 
of X, then f(x) =0. Also, if the sequence x1,x 2,... includes all the possible values
of X, then /summationtext∞
i=1f(x i)=1. 
A typical p.f. is sketched in Fig. 3.3, in which each vertical segment represents 
the value of f(x) corresponding to a possible value x. The sum of the heights of the 
vertical segments in Fig. 3.3 must be 1. 
Figure 3.3 An example of 
a p.f. 
x30 x2x1x4 xf(x)
3.1 Random Variables and Discrete Distributions 97 
Theorem 3.1.2 shows that the p.f. of a discrete random variable characterizes its 
distribution, and it allows us to dispense with the general deﬁnition of distribution 
when we are discussing discrete random variables. 
Theorem
3.1.2 If Xhas a discrete distribution, the probability of each subset Cof the real line can 
be determined from the relation 
Pr (X ∈C) =/summationdisplay
xi∈Cf(x i).
Some random variables have distributions that appear so frequently that the 
distributions are given names. The random variable Zin Example 3.1.6 is one such. 
Deﬁnition
3.1.5 Bernoulli Distribution/Random Variable . A random variable Zthat takes only two
values 0 and 1 with Pr (Z =1)=phas theBernoullidistributionwithparameter p.
We also say that Zis a Bernoullirandomvariablewithparameter p.
The Zin Example 3.1.6 has the Bernoulli distribution with parameter 0 .65252. It 
is easy to see that the name of each Bernoulli distribution is enough to allow us to 
compute the p.f., which, in turn, allows us to characterize its distribution. 
We conclude this section with illustrations of two additional families of discrete 
distributions that arise often enough to have names. 
Uniform Distributions on Integers 
Example 
3.1.8 Daily Numbers . A popular state lottery game requires participants to select a three- 
digit number (leading 0s allowed). Then three balls, each with one digit, are chosen at 
random from well-mixed bowls. The sample space here consists of all triples (i1,i2,i3)
whereij∈{0,..., 9}forj=1,2,3. If s=(i1,i2,i3), deﬁne X(s) =100i1+10 i2+i3.
For example, X( 0,1,5)=15. It is easy to check that Pr (X =x) =0.001 for each
integerx∈{0,1,..., 999}. ◭
Deﬁnition
3.1.6 Uniform Distribution on Integers . Let a≤bbe integers. Suppose that the value of a 
random variable Xis equally likely to be each of the integers a,...,b . Then we say 
thatXhas theuniformdistributionontheintegers a,...,b .
The Xin Example 3.1.8 has the uniform distribution on the integers 0 ,1,..., 999. 
A uniform distribution on a set of kintegers has probability 1 /k on each integer. 
If b>a , there are b−a+1 integers from ato bincludingaandb. The next result 
follows immediately from what we have just seen, and it illustrates how the name of 
the distribution characterizes the distribution.
Theorem
3.1.3 If Xhas the uniform distribution on the integers a,...,b , the p.f. of Xis 
f(x) =

1
b−a+1forx=a,...,b ,
0 otherwise. 
The uniform distribution on the integers a,...,b represents the outcome of an 
experiment that is often described by saying that one of the integers a,...,b is chosen 
atrandom . In this context, the phrase “at random” means that each of the b−a+1
integers is equally likely to be chosen. In this same sense, it is not possible to choose 
an integer at random from the set of allpositive integers, because it is not possible 
98 Chapter 3 Random Variables and Distributions 
to assign the same probability to every one of the positive integers and still make the 
sum of these probabilities equal to 1. In other words, a uniform distribution cannot 
be assigned to an inﬁnite sequence of possible values, but such a distribution can be 
assigned to any ﬁnite sequence. 
Note: Random Variables Can Have the Same Distribution withou t Being the 
Same Random Variable. Consider two consecutive daily number draws as in Ex- 
ample 3.1.8. The sample space consists of all 6-tuples (i1,...,i 6), where the ﬁrst 
three coordinates are the numbers drawn on the ﬁrst day and the last three are the 
numbers drawn on the second day (all in the order drawn). If s=(i1,...,i 6), let 
X1(s) =100i1+10 i2+i3and letX2(s) =100i4+10 i5+i6. It is easy to see that X1
andX2are different functions of sand are not the same random variable. Indeed, 
there is only a small probability that they will take the same value. But they have 
the same distribution because they assume the same values with the same probabil-
ities. If a businessman has 1000 customers numbered 0 ,..., 999, and he selects one 
at random and records the number Y, the distribution of Ywill be the same as the 
distribution of X1and of X2, but Yis not like X1or X2in any other way. 
Binomial Distributions
Example 
3.1.9 Defective Parts . Consider again Example 2.2.5 from page 69. In that example, a ma- 
chine produces a defective item with probability p(0 <p< 1) and produces a non- 
defective item with probability 1 −p. We assumed that the events that the different 
items were defective were mutually independent. Suppose that the experiment con-
sists of examining nof these items. Each outcome of this experiment will consist of 
a list of which items are defective and which are not, in the order examined. For ex- 
ample, we can let 0 stand for a nondefective item and 1 stand for a defective item. 
Then each outcome is a string of ndigits, each of which is 0 or 1. To be speciﬁc, if, 
say,n=6, then some of the possible outcomes are 
010010,100100,000011,110000,100001,000000,etc. (3.1.3) 
We will let Xdenote the number of these items that are defective. Then the random 
variableXwill have a discrete distribution, and the possible values of Xwill be 
0,1,2,...,n . For example, the ﬁrst four outcomes listed in Eq. (3.1.3) all have 
X(s) =2. The last outcome listed has X(s) =0. ◭
Example 3.1.9 is a generalization of Example 2.2.5 with nitems inspected rather
than just six, and rewritten in the notation of random variables. For x=0,1,...,n ,
the probability of obtaining each particular ordered sequence of nitems containing
exactlyxdefectives and n−xnondefectives is px(1−p) n−x, just as it was in Ex- 
ample 2.2.5. Since there are/parenleftbign
x/parenrightbig
different ordered sequences of this type, it follows 
that
Pr (X =x) =/parenleftbiggn
x/parenrightbigg
px(1−p) n−x.
Therefore, the p.f. of Xwill be as follows: 
f(x) =/braceleftBigg/parenleftBign
x/parenrightBig
px(1−p) n−xforx=0,1,...,n ,
0 otherwise. (3.1.4)
Deﬁnition
3.1.7 Binomial Distribution/Random Variable . The discrete distribution represented by the 
p.f. in (3.1.4) is called the binomialdistributionwithparameters nandp. A random 
3.1 Random Variables and Discrete Distributions 99 
variable with this distribution is said to be a binomialrandomvariablewithparame-
tersnandp.
The reader should be able to verify that the random variable Xin Example 3.1.4, 
the number of heads in a sequence of 10 independent tosses of a fair coin, has the 
binomial distribution with parameters 10 and 1/2. 
Since the name of each binomial distribution is sufﬁcient to construct its p.f., it 
follows that the name is enough to identify the distribution. The name of each distri- 
bution includes the two parameters. The binomial distributions are very important in 
probability and statistics and will be discussed further in later chapters of this book. 
A short table of values of certain binomial distributions is given at the end 
of this book. It can be found from this table, for example, that if Xhas the bino-
mial distribution with parameters n=10 and p=0.2, then Pr (X =5)=0.0264 and
Pr (X ≥5)=0.0328. 
As another example, suppose that a clinical trial is being run. Suppose that the 
probability that a patient recovers from her symptoms during the trial is pand that
the probability is 1 −pthat the patient does not recover. Let Ydenote the number of 
patients who recover out of nindependent patients in the trial. Then the distribution 
of Yis also binomial with parameters nandp. Indeed, consider a general experiment 
that consists of observing nindependent repititions (trials) with only two possible
results for each trial. For convenience, call the two possible results “success” and 
“failure.” Then the distribution of the number of trials that result in success will be 
binomial with parameters nandp, where pis the probability of success on each trial. 
Note: Names of Distributions. In this section, we gave names to several families 
of distributions. The name of each distribution includes any numerical parameters 
that are part of the deﬁnition. For example, the random variable Xin Example 3.1.4 
has the binomial distribution with parameters 10 and 1/2. It is a correct statement to 
say thatXhas a binomial distribution or that Xhas a discrete distribution, but such 
statements are only partial descriptions of the distribution of X. Such statements 
arenotsufﬁcient to name the distribution of X, and hence they are not sufﬁcient as 
answers to the question “What is the distribution of X?” The same considerations 
apply to all of the named distributions that we introduce elsewhere in the book. When 
attempting to specify the distribution of a random variable by giving its name, one 
must give the full name, including the values of any parameters. Only the full name 
is sufﬁcient for determining the distribution. 
Summary
A random variable is a real-valued function deﬁned on a sample space. The distri- 
bution of a random variable Xis the collection of all probabilities Pr (X ∈C) for all
subsetsCof the real numbers such that {X∈C}is an event. A random variable Xis 
discrete if there are at most countably many possible values for X. In this case, the 
distribution of Xcan be characterized by the probability function (p.f.) of X, namely, 
f(x) =Pr (X =x) forxin the set of possible values. Some distributions are so famous 
that they have names. One collection of such named distributions is the collection of 
uniform distributions on ﬁnite sets of integers. A more famous collection is the col- 
lection of binomial distributions whose parameters are nandp, where nis a positive 
integer and 0 <p< 1, having p.f. (3.1.4). The binomial distribution with parameters 
n=1 and pis also called the Bernoulli distribution with parameter p. The names of 
these distributions also characterize the distributions.
100 Chapter 3 Random Variables and Distributions 
Exercises 
1. Suppose that a random variable Xhas the uniform dis-
tribution on the integers 10 ,..., 20. Find the probability 
thatXis even. 
2. Suppose that a random variable Xhas a discrete distri- 
bution with the following p.f.: 
f(x) =/braceleftbigg
cx forx=1,..., 5, 
0 otherwise. 
Determine the value of the constant c.
3. Suppose that two balanced dice are rolled, and let X
denote the absolute value of the difference between the 
two numbers that appear. Determine and sketch the p.f. 
of X.
4. Suppose that a fair coin is tossed 10 times indepen- 
dently. Determine the p.f. of the number of heads that will 
be obtained. 
5. Suppose that a box contains seven red balls and three 
blue balls. If ﬁve balls are selected at random, without 
replacement, determine the p.f. of the number of red balls 
that will be obtained. 
6. Suppose that a random variable Xhas the binomial dis-
tribution with parameters n=15 and p=0.5. Find Pr (X< 
6).
7. Suppose that a random variable Xhas the binomial dis-
tribution with parameters n=8 and p=0.7. Find Pr (X ≥
5)by using the table given at the end of this book. Hint:Use the fact that Pr (X ≥5)=Pr (Y ≤3), where Yhas the
binomial distribution with parameters n=8 and p=0.3. 
8. If 10 percent of the balls in a certain box are red, and 
if 20 balls are selected from the box at random, with re- 
placement, what is the probability that more than three 
red balls will be obtained? 
9. Suppose that a random variable Xhas a discrete distri- 
bution with the following p.f.: 
f(x) =/braceleftbiggc
2xforx=0,1,2,...,
0 otherwise. 
Find the value of the constant c.
10.A civil engineer is studying a left-turn lane that is 
long enough to hold seven cars. Let Xbe the number 
of cars in the lane at the end of a randomly chosen red 
light. The engineer believes that the probability that X=
xis proportional to (x +1)( 8−x) forx=0,..., 7 (the 
possible values of X). 
a. Find the p.f. of X.
b. Find the probability that Xwill be at least 5. 
11.Show that there does not exist any number csuch that
the following function would be a p.f.: 
f(x) =/braceleftbiggc
xforx=1,2,...,
0 otherwise. 
3.2 Continuous Distributions 
Next,wefocusonrandomvariablesthatcanassumeeveryvalueinaninterval
(boundedorunbounded).Ifarandomvariable Xhasassociatedwithitafunction 
fsuchthattheintegralof fovereachintervalgivestheprobabilitythat Xisinthe 
interval,thenwecall ftheprobabilitydensityfunction(p.d.f.)of Xandwesay 
thatXhasacontinuousdistribution.
The Probability Density Function 
Example 
3.2.1Demands for Utilities . In Example 3.1.5, we determined the distribution of the de- 
mand for water, X. From Fig. 3.2, we see that the smallest possible value of Xis 4 
and the largest is 200. For each interval C=[c0,c 1]⊂[4 ,200], Eq. (3.1.2) says that 
Pr (c 0≤X≤c1)=149(c 1−c0)
29204=c1−c0
196=/integraldisplayc1
c01
196dx.
3.2 Continuous Distributions 101
So, if we deﬁne 
f(x) =

1
196if 4 ≤x≤200, 
0 otherwise, (3.2.1)
we have that 
Pr (c 0≤X≤c1)=/integraldisplayc1
c0f(x)dx. (3.2.2)
Because we deﬁned f(x) to be 0 for xoutside of the interval [4 ,200], we see that Eq. 
(3.2.2) holds for all c0≤c1, even if c0=−∞ and/orc1=∞ . ◭
The water demand Xin Example 3.2.1 is an example of the following. 
Deﬁnition
3.2.1Continuous Distribution/Random Variable . We say that a random variable Xhas a
continuousdistribution or that Xis a continuousrandomvariable if there exists a 
nonnegative function f, deﬁned on the real line, such that for every interval of real 
numbers (bounded or unbounded), the probability that Xtakes a value in the interval 
is the integral of fover the interval.
For example, in the situation described in Deﬁnition 3.2.1, for each bounded closed 
interval [a,b ], 
Pr (a ≤X≤b) =/integraldisplayb
af(x)dx. (3.2.3)
Similarly, Pr (X ≥a) =/integraltext∞
af(x)dx and Pr (X ≤b) =/integraltextb
−∞f(x)dx .
We see that the function fcharacterizes the distribution of a continuous ran- 
dom variable in much the same way that the probability function characterizes the 
distribution of a discrete random variable. For this reason, the function fplays an 
important role, and hence we give it a name. 
Deﬁnition
3.2.2Probability Density Function/p.d.f./Support . If Xhas a continuous distribution, the 
functionfdescribed in Deﬁnition 3.2.1 is called the probability density function
(abbreviated p.d.f. ) of X. The closure of the set {x:f(x)> 0}is called the support
of(thedistributionof) X.
Example 3.2.1 demonstrates that the water demand Xhas p.d.f. given by (3.2.1). 
Every p.d.f. fmust satisfy the following two requirements:
f(x) ≥0,for all x, (3.2.4)
and/integraldisplay∞
−∞f(x)dx =1. (3.2.5)
A typical p.d.f. is sketched in Fig. 3.4. In that ﬁgure, the total area under the curve 
must be 1, and the value of Pr (a ≤X≤b) is equal to the area of the shaded region. 
Note: Continuous Distributions Assign Probability 0 to Ind ividual Values. The 
integral in Eq. (3.2.3) also equals Pr (a<X ≤b) as well as Pr (a<X<b) and Pr (a ≤
X<b) . Hence, it follows from the deﬁnition of continuous distributions that, if X
has a continuous distribution, Pr (X =a) =0 for each number a. As we noted on 
page 20, the fact that Pr (X =a) =0 does not imply that X=ais impossible. If it did, 
102 Chapter 3 Random Variables and Distributions 
Figure3.4 An example of a 
p.d.f. 
a b xf(x)
all values of Xwould be impossible and Xcouldn’t assume any value. What happens 
is that the probability in the distribution of Xis spread so thinly that we can only see 
it on sets like nondegenerate intervals. It is much the same as the fact that lines have 
0 area in two dimensions, but that does not mean that lines are not there. The two 
vertical lines indicated under the curve in Fig. 3.4 have 0 area, and this signiﬁes that 
Pr (X =a) =Pr (X =b) =0. However, for each ǫ> 0 and each asuch thatf(a)> 0, 
Pr (a −ǫ≤X≤a+ǫ) ≈2ǫf(a)> 0. 
Nonuniqueness of the p.d.f. 
If a random variable Xhas a continuous distribution, then Pr (X =x) =0 for every 
individual value x. Because of this property, the values of each p.d.f. can be changed 
at a ﬁnite number of points, or even at certain inﬁnite sequences of points, without 
changing the value of the integral of the p.d.f. over any subset A. In other words, 
the values of the p.d.f. of a random variable Xcan be changed arbitrarily at many 
points without affecting any probabilities involving X, that is, without affecting the 
probability distribution of X. At exactly which sets of points we can change a p.d.f. 
depends on subtle features of the deﬁnition of the Riemann integral. We shall not 
deal with this issue in this text, and we shall only contemplate changes to p.d.f.’s at 
ﬁnitely many points.
To the extent just described, the p.d.f. of a random variable is not unique. In many 
problems, however, there will be one version of the p.d.f. that is more natural than 
any other because for this version the p.d.f. will, wherever possible, be continuous on 
the real line. For example, the p.d.f. sketched in Fig. 3.4 is a continuous function over 
the entire real line. This p.d.f. could be changed arbitrarily at a few points without 
affecting the probability distribution that it represents, but these changes would 
introduce discontinuities into the p.d.f. without introducing any apparent advantages. 
Throughout most of this book, we shall adopt the following practice: If a random 
variableXhas a continuous distribution, we shall give only one version of the p.d.f. 
of Xand we shall refer to that version as thep.d.f. of X, just as though it had been 
uniquely determined. It should be remembered, however, that there is some freedom 
in the selection of the particular version of the p.d.f. that is used to represent each 
continuous distribution. The most common place where such freedom will arise is 
in cases like Eq. (3.2.1) where the p.d.f. is required to have discontinuities. Without 
making the function fany less continuous, we could have deﬁned the p.d.f. in that 
example so that f( 4)=f( 200)=0 instead of f( 4)=f( 200)=1/196. Both of these 
choices lead to the same calculations of all probabilities associated with X, and they 
3.2 Continuous Distributions 103
are both equally valid. Because the support of a continuous distribution is the closure 
of the set where the p.d.f. is strictly positive, it can be shown that the support is unique. 
A sensible approach would then be to choose the version of the p.d.f. that was strictly 
positive on the support whenever possible. 
The reader should note that “continuous distribution” is notthe name of a 
distribution, just as “discrete distribution” is not the name of a distribution. There are 
many distributions that are discrete and many that are continuous. Some distributions 
of each type have names that we either have introduced or will introduce later. 
We shall now present several examples of continuous distributions and their 
p.d.f.’s. 
Uniform Distributions on Intervals 
Example 
3.2.2Temperature Forecasts . Television weather forecasters announce high and low tem- 
perature forecasts as integer numbers of degrees. These forecasts, however, are the 
results of very sophisticated weather models that provide more precise forecasts that 
the television personalities round to the nearest integer for simplicity. Suppose that 
the forecaster announces a high temperature of y. If we wanted to know what tem- 
peratureXthe weather models actually produced, it might be safe to assume that X
was equally likely to be any number in the interval from y−1/2 to y+1/2. ◭
The distribution of Xin Example 3.2.2 is a special case of the following. 
Deﬁnition
3.2.3Uniform Distribution on an Interval . Let aandbbe two given real numbers such that 
a<b . Let Xbe a random variable such that it is known that a≤X≤band, for
every subinterval of [ a,b ], the probability that Xwill belong to that subinterval is 
proportional to the length of that subinterval. We then say that the random variable 
Xhas theuniformdistributionontheinterval [a,b ]. 
A random variable Xwith the uniform distribution on the interval [ a,b ] represents 
the outcome of an experiment that is often described by saying that a point is chosen 
atrandom from the interval [ a,b ]. In this context, the phrase “at random” means 
that the point is just as likely to be chosen from any particular part of the interval as 
from any other part of the same length. 
Theorem
3.2.1Uniform Distribution p.d.f . If Xhas the uniform distribution on an interval [ a,b ], then 
the p.d.f. of Xis 
f(x) =/braceleftBigg1
b−afora≤x≤b,
0 otherwise. (3.2.6)
Proof Xmust take a value in the interval [ a,b ]. Hence, the p.d.f. f(x) of Xmust
be 0 outside of [ a,b ]. Furthermore, since any particular subinterval of [ a,b ] having 
a given length is as likely to contain Xas is any other subinterval having the same 
length, regardless of the location of the particular subinterval in [ a,b ], it follows that 
f(x) must be constant throughout [ a,b ], and that interval is then the support of the 
distribution. Also,
/integraldisplay∞
−∞f(x)dx =/integraldisplayb
af(x)dx =1. (3.2.7)
Therefore, the constant value of f(x) throughout [ a,b ] must be 1 /(b−a) , and the 
p.d.f. of Xmust be (3.2.6). 
104 Chapter 3 Random Variables and Distributions 
Figure3.5 The p.d.f. for the 
uniform distribution on the 
interval [a,b ]. 
a b xf(x)
Th p.d.f. (3.2.6) is sketched in Fig. 3.5. As an example, the random variable X(demand
for water) in Example 3.2.1 has the uniform distribution on the interval [4 ,200].
Note:DensityIsNotProbability. The reader should note that the p.d.f. in (3.2.6) can 
be greater than 1, particularly if b−a< 1. Indeed, p.d.f.’s can be unbounded, as we 
shall see in Example 3.2.6. The p.d.f. of X,f(x) , itself does not equal the probability 
thatXis near x. The integral of fover values near xgives the probability that Xis 
nearx, and the integral is never greater than 1. 
It is seen from Eq. (3.2.6) that the p.d.f. representing a uniform distribution on 
a given interval is constant over that interval, and the constant value of the p.d.f. 
is the reciprocal of the length of the interval. It is not possible to deﬁne a uniform 
distribution over an unbounded interval, because the length of such an interval is 
inﬁnite.
Consider again the uniform distribution on the interval [ a,b ]. Since the proba- 
bility is 0 that one of the endpoints aor bwill be chosen, it is irrelevant whether the 
distribution is regarded as a uniform distribution on the closed intervala≤x≤b, or 
as a uniform distribution on the open intervala<x<b , or as a uniform distribution 
on the half-open and half-closed interval (a,b ] in which one endpoint is included and 
the other endpoint is excluded. 
For example, if a random variable Xhas the uniform distribution on the interval 
[−1,4], then the p.d.f. of Xis 
f(x) =/braceleftbigg1/5 for −1≤x≤4, 
0 otherwise. 
Furthermore,
Pr (0≤X< 2)=/integraldisplay2
0f(x)dx =2
5.
Notice that we deﬁned the p.d.f. of Xto be strictly positive on the closed interval 
[−1,4] and 0 outside of this closed interval. It would have been just as sensible to 
deﬁne the p.d.f. to be strictly positive on the open interval (−1,4)and 0 outside of this 
open interval. The probability distribution would be the same either way, including 
the calculation of Pr (0≤X< 2)that we just performed. After this, when there are 
several equally sensible choices for how to deﬁne a p.d.f., we will simply choose one 
of them without making any note of the other choices. 
Other Continuous Distributions 
Example 
3.2.3Incompletely Speciﬁed p.d.f . Suppose that the p.d.f. of a certain random variable X
has the following form:
3.2 Continuous Distributions 105
f(x) =/braceleftbiggcx for 0<x< 4, 
0 otherwise, 
wherecis a given constant. We shall determine the value of c.
For every p.d.f., it must be true that /integraltext∞
−∞f(x) =1. Therefore, in this example, 
/integraldisplay4
0cxdx =8c=1.
Hence,c=1/8. ◭
Note: Calculating Normalizing Constants. The calculation in Example 3.2.3 illus- 
trates an important point that simpliﬁes many statistical results. The p.d.f. of Xwas
speciﬁed without explicitly giving the value of the constant c. However, we were able 
to ﬁgure out what was the value of cby using the fact that the integral of a p.d.f. must 
be 1. It will often happen, especially in Chapter 8 where we ﬁnd sampling distribu- 
tions of summaries of observed data, that we can determine the p.d.f. of a random 
variable except for a constant factor. That constant factor must be the unique value 
such that the integral of the p.d.f. is 1, even if we cannot calculate it directly. 
Example 
3.2.4Calculating Probabilities from a p.d.f . Suppose that the p.d.f. of Xis as in Example 3.2.3, 
namely,
f(x) =/braceleftBiggx
8for 0<x< 4, 
0 otherwise. 
We shall now determine the values of Pr (1≤X≤2)and Pr (X> 2). Apply Eq. (3.2.3) 
to get 
Pr (1≤X≤2)=/integraldisplay2
11
8xdx =3
16 
and
Pr (X> 2)=/integraldisplay4
21
8xdx =3
4. ◭
Example 
3.2.5Unbounded Random Variables . It is often convenient and useful to represent a con- 
tinuous distribution by a p.d.f. that is positive over an unbounded interval of the real 
line. For example, in a practical problem, the voltage Xin a certain electrical system 
might be a random variable with a continuous distribution that can be approximately 
represented by the p.d.f. 
f(x) =

0 for x≤0, 
1
(1+x) 2forx> 0. (3.2.8)
It can be veriﬁed that the properties (3.2.4) and (3.2.5) required of all p.d.f.’s are 
satisﬁed by f(x) .
Even though the voltage Xmay actually be bounded in the real situation, the 
p.d.f. (3.2.8) may provide a good approximation for the distribution of Xover its full
range of values. For example, suppose that it is known that the maximum possible 
value of Xis 1000, in which case Pr (X> 1000)=0. When the p.d.f. (3.2.8) is used, 
we compute Pr (X> 1000)=0.001. If (3.2.8) adequately represents the variability 
of Xover the interval (0,1000), then it may be more convenient to use the p.d.f. 
(3.2.8) than a p.d.f. that is similar to (3.2.8) for x≤1000, except for a new normalizing 
106 Chapter 3 Random Variables and Distributions 
constant, and is 0 for x> 1000. This can be especially true if we do not know for sure 
that the maximum voltage is only 1000. ◭
Example 
3.2.6Unbounded p.d.f.’s . Since a value of a p.d.f. is a probability density, rather than a 
probability, such a value can be larger than 1. In fact, the values of the following 
p.d.f. are unbounded in the neighborhood of x=0: 
f(x) =/braceleftBigg
2
3x−1/3for 0<x< 1, 
0 otherwise. (3.2.9)
It can be veriﬁed that even though the p.d.f. (3.2.9) is unbounded, it satisﬁes the 
properties (3.2.4) and (3.2.5) required of a p.d.f. ◭
Mixed Distributions 
Most distributions that are encountered in practical problems are either discrete or 
continuous. We shall show, however, that it may sometimes be necessary to consider a 
distribution that is a mixture of a discrete distribution and a continuous distribution. 
Example 
3.2.7Truncated Voltage . Suppose that in the electrical system considered in Example 3.2.5, 
the voltageXis to be measured by a voltmeter that will record the actual value of 
Xif X≤3 but will simply record the value 3 if X> 3. If we let Ydenote the value
recorded by the voltmeter, then the distribution of Ycan be derived as follows. 
First, Pr (Y =3)=Pr (X ≥3)=1/4. Since the single value Y=3 has probability 
1/4, it follows that Pr (0<Y < 3)=3/4. Furthermore, since Y=Xfor 0<X< 3, this 
probability 3/4 for Yis distributed over the interval (0,3)according to the same p.d.f. 
(3.2.8) as that of Xover the same interval. Thus, the distribution of Yis speciﬁed by 
the combination of a p.d.f. over the interval (0,3)and a positive probability at the 
pointY=3. ◭
Summary
A continuous distribution is characterized by its probability density function (p.d.f.). 
A nonnegative function fis the p.d.f. of the distribution of Xif, for every interval 
[a,b ], Pr (a ≤X≤b) =/integraltextb
af(x) dx . Continuous random variables satisfy Pr (X =x) =
0 for every value x. If the p.d.f. of a distribution is constant on an interval [ a,b ] and 
is 0 off the interval, we say that the distribution is uniform on the interval [ a,b ]. 
Exercises 
1. LetXbe a random variable with the p.d.f. speciﬁed in 
Example 3.2.6. Compute Pr (X ≤8/27 ).
2. Suppose that the p.d.f. of a random variable Xis as 
follows:
f(x) =/braceleftBigg
4
3(1−x3)for 0<x< 1, 
0 otherwise. Sketch this p.d.f. and determine the values of the fol- 
lowing probabilities: a. Pr /parenleftbigg
X< 1
2/parenrightbigg
b. Pr /parenleftbigg1
4<X< 3
4/parenrightbigg
c. Pr /parenleftBig
X> 1
3/parenrightBig
.
3. Suppose that the p.d.f. of a random variable Xis as 
follows:
3.3 The Cumulative Distribution Function 107
f(x) =/braceleftBigg
1
36 (9−x2)for−3≤x≤3, 
0 otherwise. 
Sketch this p.d.f. and determine the values of the following 
probabilities: a. Pr (X< 0)b. Pr (−1≤X≤1)
c. Pr (X> 2).
4. Suppose that the p.d.f. of a random variable Xis as 
follows:
f(x) =/braceleftbiggcx 2for 1≤x≤2, 
0 otherwise. 
a. Find the value of the constant cand sketch the p.d.f. 
b. Find the value of Pr (X> 3/2).
5. Suppose that the p.d.f. of a random variable Xis as 
follows:
f(x) =/braceleftBigg
1
8xfor 0≤x≤4, 
0 otherwise. 
a. Find the value of tsuch that Pr (X ≤t) =1/4. 
b. Find the value of tsuch that Pr (X ≥t) =1/2. 
6. LetXbe a random variable for which the p.d.f. is as 
given in Exercise 5. After the value of Xhas been ob-
served, letYbe the integer closest to X. Find the p.f. of 
the random variable Y.
7. Suppose that a random variable Xhas the uniform
distribution on the interval [ −2,8]. Find the p.d.f. of Xand
the value of Pr (0<X< 7).
8. Suppose that the p.d.f. of a random variable Xis as 
follows:
f(x) =/braceleftbiggce −2xforx> 0, 
0 otherwise. a. Find the value of the constant cand sketch the p.d.f. 
b. Find the value of Pr (1<X< 2).
9. Show that there does not exist any number csuch that
the following function f(x) would be a p.d.f.: 
f(x) =/braceleftbiggc
1+xforx> 0, 
0 otherwise. 
10.Suppose that the p.d.f. of a random variable Xis as 
follows:
f(x) =/braceleftbiggc
(1−x) 1/2for 0<x< 1, 
0 otherwise. 
a. Find the value of the constant cand sketch the p.d.f. 
b. Find the value of Pr (X ≤1/2).
11.Show that there does not exist any number csuch that
the following function f(x) would be a p.d.f.: 
f(x) =/braceleftbiggc
xfor 0<x< 1, 
0 otherwise. 
12.In Example 3.1.3 on page 94, determine the distri- 
bution of the random variable Y, the electricity demand. 
Also, ﬁnd Pr (Y < 50 ).
13.An ice cream seller takes 20 gallons of ice cream in 
her truck each day. Let Xstand for the number of gallons 
that she sells. The probability is 0.1 that X=20. If she 
doesn’t sell all 20 gallons, the distribution of Xfollows a
continuous distribution with a p.d.f. of the form 
f(x) =/braceleftbiggcx for 0<x< 20, 
0 otherwise, 
wherecis a constant that makes Pr (X< 20 )=0.9. Find the 
constantcso that Pr (X< 20 )=0.9 as described above. 
3.3 The Cumulative Distribution Function 
Althoughadiscretedistributionischaracterizedbyitsp.f.andacontinuousdistri-
butionischaracterizedbyitsp.d.f.,everydistributionhasacommoncharacteriza-
tionthroughits(cumulative)distributionfunction(c.d.f.).Theinverseofthec.d.f.
iscalledthequantilefunction,anditisusefulforindicatingwheretheprobability 
islocatedinadistribution.
Example 
3.3.1Voltage . Consider again the voltage Xfrom Example 3.2.5. The distribution of X
is characterized by the p.d.f. in Eq. (3.2.8). An alternative characterization that is 
more directly related to probabilities associated with Xis obtained from the following 
function:
108 Chapter 3 Random Variables and Distributions 
F(x) =Pr (X ≤x) =/integraldisplayx
−∞f(y)dy =

0 for x≤0, /integraldisplayx
0dy 
(1+y) 2forx> 0, 
=/braceleftBigg0 for x≤0, 
1−1
1+xforx> 0. (3.3.1)
So, for example, Pr (X ≤3)=F( 3)=3/4. ◭
Deﬁnition and Basic Properties 
Deﬁnition
3.3.1(Cumulative) Distribution Function . The distributionfunction or cumulativedistribu-
tionfunction (abbreviated c.d.f. )Fof a random variable Xis the function 
F(x) =Pr (X ≤x) for−∞<x< ∞. (3.3.2)
It should be emphasized that the cumulative distribution function is deﬁned as above 
for every random variable X, regardless of whether the distribution of Xis discrete, 
continuous, or mixed. For the continuous random variable in Example 3.3.1, the c.d.f. 
was calculated in Eq. (3.3.1). Here is a discrete example: 
Example 
3.3.2Bernoulli c.d.f . Let Xhave the Bernoulli distribution with parameter pdeﬁned in 
Deﬁnition 3.1.5. Then Pr (X =0)=1−pand Pr (X =1)=p. Let Fbe the c.d.f. of X.
It is easy to see that F(x) =0 for x< 0 because X≥0 for sure. Similarly, F(x) =1 for 
x≥1 because X≤1 for sure. For 0 ≤x< 1, Pr (X ≤x) =Pr (X =0)=1−pbecause
0 is the only possible value of Xthat is in the interval (−∞,x ]. In summary, 
F(x) =

0 for x< 0, 
1−pfor 0≤x< 1, 
1 for x≥1. ◭
We shall soon see (Theorem 3.3.2) that the c.d.f. allows calculation of all interval 
probabilities; hence, it characterizes the distribution of a random variable. It follows 
from Eq. (3.3.2) that the c.d.f. of each random variable Xis a function Fdeﬁned on 
the real line. The value of Fat every point xmust be a number F(x) in the interval 
[0 ,1] because F(x) is the probability of the event {X≤x}. Furthermore, it follows 
from Eq. (3.3.2) that the c.d.f. of every random variable Xmust have the following
three properties.
Property 
3.3.1Nondecreasing .Thefunction F(x) isnondecreasingas xincreases;thatis,if x1<x 2,
thenF(x 1)≤F(x 2).
Proof If x1<x 2, then the event {X≤x1}is a subset of the event {X≤x2}. Hence, 
Pr {X≤x1}≤ Pr {X≤x2}according to Theorem 1.5.4. 
An example of a c.d.f. is sketched in Fig. 3.6. It is shown in that ﬁgure that 0 ≤
F(x) ≤1 over the entire real line. Also, F(x) is always nondecreasing as xincreases,
althoughF(x) is constant over the interval x1≤x≤x2and forx≥x4.
Property 
3.3.2Limits at ±∞. lim x→−∞F(x) =0and limx→∞F(x) =1. 
Proof As in the proof of Property 3.3.1, note that {X≤x1}⊂{X≤x2}wheneverx1<
x2. The fact that Pr (X ≤x) approaches 0 as x→−∞ now follows from Exercise 13 in 
3.3 The Cumulative Distribution Function 109
Figure3.6 An example of a 
c.d.f. 
1
z3
z2
z1
z0
0 x1x2x3x4xF(x)
Section 1.10. Similarly, the fact that Pr (X ≤x) approaches 1 as x→∞ follows from
Exercise 12 in Sec. 1.10. 
The limiting values speciﬁed in Property 3.3.2 are indicated in Fig. 3.6. In this 
ﬁgure, the value of F(x) actually becomes 1 at x=x4and then remains 1 for x>x 4.
Hence, it may be concluded that Pr (X ≤x4)=1 and Pr (X>x 4)=0. On the other 
hand, according to the sketch in Fig. 3.6, the value of F(x) approaches 0 as x→−∞ ,
but does not actually become 0 at any ﬁnite point x. Therefore, for every ﬁnite value 
of x, no matter how small, Pr (X ≤x)> 0. 
A c.d.f. need not be continuous. In fact, the value of F(x) may jump at any 
ﬁnite or countable number of points. In Fig. 3.6, for instance, such jumps or points 
of discontinuity occur where x=x1andx=x3. For each ﬁxed value x, we shall let 
F(x −)denote the limit of the values of F(y) as yapproachesxfrom the left, that is, 
as yapproachesxthrough values smaller than x. In symbols, 
F(x −)=limy→x
y<x F(y).
Similarly, we shall deﬁne F(x +)as the limit of the values of F(y) as yapproaches x
from the right. Thus, 
F(x +)=limy→x
y>x F(y).
If the c.d.f. is continuous at a given point x, then F(x −)=F(x +)=F(x) at that point. 
Property 
3.3.3Continuity from the Right .Ac.d.f.isalwayscontinuousfromtheright;thatis ,F(x) =
F(x +)ateverypoint x.
Proof Lety1>y 2>...be a sequence of numbers that are decreasing such that 
limn→∞yn=x. Then the event {X≤x}is the intersection of all the events {X≤yn}
forn=1,2,.... Hence, by Exercise 13 of Sec. 1.10, 
F(x) =Pr (X ≤x) =limn→∞Pr (X ≤yn)=F(x +).
It follows from Property 3.3.3 that at every point xat which a jump occurs, 
F(x +)=F(x) andF(x −)<F(x).
110 Chapter 3 Random Variables and Distributions 
In Fig. 3.6 this property is illustrated by the fact that, at the points of discontinuity 
x=x1andx=x3, the value of F(x 1)is taken as z1and the value of F(x 3)is taken as 
z3.
Determining Probabilities from the Distribution Function
Example 
3.3.3Voltage . In Example 3.3.1, suppose that we want to know the probability that Xlies
in the interval [2 ,4]. That is, we want Pr (2≤X≤4). The c.d.f. allows us to compute 
Pr (X ≤4)and Pr (X ≤2). These are related to the probability that we want as follows: 
LetA={2<X ≤4},B={X≤2}, and C={X≤4}. Because Xhas a continuous 
distribution, Pr (A)is the same as the probability that we desire. We see that A∪B=
C, and it is clear that AandBare disjoint. Hence, Pr (A)+Pr (B) =Pr (C) . It follows 
that
Pr (A)=Pr (C) −Pr (B) =F( 4)−F( 2)=4
5−3
4=1
20 . ◭
The type of reasoning used in Example 3.3.3 can be extended to ﬁnd the prob- 
ability that an arbitrary random variable Xwill lie in any speciﬁed interval of the 
real line from the c.d.f. We shall derive this probability for four different types of 
intervals.
Theorem
3.3.1For every value x,
Pr (X>x) =1−F(x). (3.3.3)
Proof The events {X>x }and{X≤x}are disjoint, and their union is the whole 
sample space Swhose probability is 1. Hence, Pr (X>x) +Pr (X ≤x) =1. Now, 
Eq. (3.3.3) follows from Eq. (3.3.2).
Theorem
3.3.2For all values x1andx2such thatx1<x 2,
Pr (x 1<X ≤x2)=F(x 2)−F(x 1). (3.3.4)
Proof LetA={x1<X ≤x2},B={X≤x1}, and C={X≤x2}. As in Example 3.3.3, 
AandBare disjoint, and their union is C, so 
Pr (x 1<X ≤x2)+Pr (X ≤x1)=Pr (X ≤x2).
Subtracting Pr (X ≤x1)from both sides of this equation and applying Eq. (3.3.2) 
yields Eq. (3.3.4).
For example, if the c.d.f. of Xis as sketched in Fig. 3.6, then it follows from 
Theorems 3.3.1 and 3.3.2 that Pr (X>x 2)=1−z1and Pr (x 2<X ≤x3)=z3−z1. Also, 
sinceF(x) is constant over the interval x1≤x≤x2, then Pr (x 1<X ≤x2)=0. 
It is important to distinguish carefully between the strict inequalities and the 
weak inequalities that appear in all of the preceding relations and also in the next 
theorem. If there is a jump in F(x) at a given value x, then the values of Pr (X ≤x) 
and Pr (X<x) will be different. 
Theorem
3.3.3For each value x,
Pr (X<x) =F(x −). (3.3.5)
3.3 The Cumulative Distribution Function 111
Proof Lety1<y 2<...be an increasing sequence of numbers such that lim n→∞yn=
x. Then it can be shown that 
{X<x }= ∞/uniondisplay
n=1{X≤yn}.
Therefore, it follows from Exercise 12 of Sec. 1.10 that 
Pr (X<x) =lim
n→∞Pr (X ≤yn)
=limn→∞F(y n)=F(x −).
For example, for the c.d.f. sketched in Fig. 3.6, Pr (X<x 3)=z2and Pr (X<x 4)
=1. 
Finally, we shall show that for every value x, Pr (X =x) is equal to the amount 
of the jump that occurs in Fat the point x. If Fis continuous at the point x, that is, 
if there is no jump in Fat x, then Pr (X =x) =0. 
Theorem
3.3.4For every value x,
Pr (X =x) =F(x) −F(x −). (3.3.6)
Proof It is always true that Pr (X =x) =Pr (X ≤x) −Pr (X<x) . The relation (3.3.6) 
follows from the fact that Pr (X ≤x) =F(x) at every point and from Theorem 3.3.3. 
In Fig. 3.6, for example, Pr (X =x1)=z1−z0, Pr (X =x3)=z3−z2, and the 
probability of every other individual value of Xis 0. 
The c.d.f. of a Discrete Distribution 
From the deﬁnition and properties of a c.d.f. F(x) , it follows that if a<b and
if Pr (a<X<b) =0, then F(x) will be constant and horizontal over the interval 
a<x<b . Furthermore, as we have just seen, at every point xsuch that Pr (X =x)> 0, 
the c.d.f. will jump by the amount Pr (X =x) .
Suppose that Xhas a discrete distribution with the p.f. f(x) . Together, the prop- 
erties of a c.d.f. imply that F(x) must have the following form: F(x) will have a jump 
of magnitude f(x i)at each possible value xiof X, and F(x) will be constant between 
every pair of successive jumps. The distribution of a discrete random variable Xcan
be represented equally well by either the p.f. or the c.d.f. of X.
The c.d.f. of a Continuous Distribution 
Theorem
3.3.5LetXhave a continuous distribution, and let f(x) andF(x) denote its p.d.f. and the 
c.d.f., respectively. Then Fis continuous at every x,
F(x) =/integraldisplayx
−∞f(t)dt, (3.3.7)
and
dF(x) 
dx =f(x), (3.3.8)
at all xsuch thatfis continuous. 
112 Chapter 3 Random Variables and Distributions 
Proof Since the probability of each individual point xis 0, the c.d.f. F(x) will have
no jumps. Hence, F(x) will be a continuous function over the entire real line. 
By deﬁnition, F(x) =Pr (X ≤x) . Since fis the p.d.f. of X, we have from the 
deﬁnition of p.d.f. that Pr (X ≤x) is the right-hand side of Eq. (3.3.7). 
It follows from Eq. (3.3.7) and the relation between integrals and derivatives 
(the fundamental theorem of calculus) that, for every xat which fis continuous, 
Eq. (3.3.8) holds.
Thus, the c.d.f. of a continuous random variable Xcan be obtained from the p.d.f. 
and vice versa. Eq. (3.3.7) is how we found the c.d.f. in Example 3.3.1. Notice that 
the derivative of the Fin Example 3.3.1 is 
F′(x) =

0 for x< 0, 
1
(1+x) 2forx> 0, 
andF′does not exist at x=0. This veriﬁes Eq (3.3.8) for Example 3.3.1. Here, we 
have used the popular shorthand notation F′(x) for the derivative of Fat the point x.
Example 
3.3.4Calculating a p.d.f. from a c.d.f . Let the c.d.f. of a random variable be 
F(x) =

0 for x< 0, 
x2/3for 0≤x≤1, 
1 for x> 1. 
This function clearly satisﬁes the three properties required of every c.d.f., as given 
earlier in this section. Furthermore, since this c.d.f. is continuous over the entire real 
line and is differentiable at every point except x=0 and x=1, the distribution of X
is continuous. Therefore, the p.d.f. of Xcan be found at every point other than x=0
andx=1 by the relation (3.3.8). The value of f(x) at the points x=0 and x=1 can 
be assigned arbitrarily. When the derivative F′(x) is calculated, it is found that f(x) 
is as given by Eq. (3.2.9) in Example 3.2.6. Conversely, if the p.d.f. of Xis given by 
Eq. (3.2.9), then by using Eq. (3.3.7) it is found that F(x) is as given in this example. 
◭
The Quantile Function 
Example 
3.3.5Fair Bets . Suppose that Xis the amount of rain that will fall tomorrow, and Xhas
c.d.f. F. Suppose that we want to place an even-money bet on Xas follows: If X≤x0,
we win one dollar and if X>x 0we lose one dollar. In order to make this bet fair, we 
need Pr (X ≤x0)=Pr (X>x 0)=1/2. We could search through all of the real numbers 
xtrying to ﬁnd one such that F(x) =1/2, and then we would let x0equal the value we 
found. If Fis a one-to-one function, then Fhas an inverse F−1andx0=F−1(1/2).
◭
The value x0that we seek in Example 3.3.5 is called the 0.5 quantile of Xor the 
50thpercentile of Xbecause 50% of the distribution of Xis at or below x0.
Deﬁnition
3.3.2Quantiles/Percentiles . Let Xbe a random variable with c.d.f. F. For each pstrictly
between 0 and 1, deﬁne F−1(p) to be the smallest value xsuch thatF(x) ≥p. Then 
F−1(p) is called the pquantile of Xor the 100 ppercentile of X. The function F−1
deﬁned here on the open interval (0,1)is called the quantilefunction of X.
3.3 The Cumulative Distribution Function 113
Example 
3.3.6Standardized Test Scores . Many universities in the United States rely on standardized 
test scores as part of their admissions process. Thousands of people take these tests 
each time that they are offered. Each examinee’s score is compared to the collection 
of scores of all examinees to see where it ﬁts in the overall ranking. For example, if 
83% of all test scores are at or below your score, your test report will say that you 
scored at the 83rd percentile. ◭
The notation F−1(p) in Deﬁnition 3.3.2 deserves some justiﬁcation. Suppose ﬁrst 
that the c.d.f. Fof Xis continuous and one-to-one over the whole set of possible 
values of X. Then the inverse F−1of Fexists, and for each 0 <p< 1, there is one 
and only one xsuch thatF(x) =p. That xis F−1(p) . Deﬁnition 3.3.2 extends the 
concept of inverse function to nondecreasing functions (such as c.d.f.’s) that may be 
neither one-to-one nor continuous.
QuantilesofContinuousDistributions When the c.d.f. of a random variable Xis 
continuous and one-to-one over the whole set of possible values of X, the inverse 
F−1of Fexists and equals the quantile function of X.
Example 
3.3.7Value at Risk . The manager of an investment portfolio is interested in how much 
money the portfolio might lose over a ﬁxed time horizon. Let Xbe the change 
in value of the given portfolio over a period of one month. Suppose that Xhas
the p.d.f. in Fig. 3.7. The manager computes a quantity known in the world of risk 
management as ValueatRisk (denoted by VaR). To be speciﬁc, let Y=− Xstand
for the loss incurred by the portfolio over the one month. The manager wants to 
have a level of conﬁdence about how large Ymight be. In this example, the manager 
speciﬁes a probability level, such as 0.99 and then ﬁnds y0, the 0 .99 quantile of Y. The 
manager is now 99% sure that Y≤y0, and y0is called the VaR. If Xhas a continuous 
distribution, then it is easy to see that y0is closely related to the 0.01 quantile of 
the distribution of X. The 0.01 quantile x0has the property that Pr (X<x 0)=0.01. 
But Pr (X<x 0)=Pr (Y > −x0)=1−Pr (Y ≤− x0). Hence, −x0is a 0 .99 quantile of 
Y. For the p.d.f. in Fig. 3.7, we see that x0=− 4.14, as the shaded region indicates. 
Then y0=4.14 is VaR for one month at probability level 0.99. ◭
Figure 3.7 The p.d.f. of the 
change in value of a portfolio 
with lower 1% indicated. 
0.02
00.12
0.10
0.08
0.06
0.04
/H110024.14Change in valueDensity
20 10 0.99
0.01
0/H1100210 /H1100220 
114 Chapter 3 Random Variables and Distributions 
Figure 3.8 The c.d.f. of a 
uniform distribution indi-
cating how to solve for a 
quantile.
0.21.0
0.8
0.6p
0.4
01 2 F/H110021(p)Cumulative distribution function 
4 3 x
Example 
3.3.8Uniform Distribution on an Interval . Let Xhave the uniform distribution on the 
interval [a,b ]. The c.d.f. of Xis 
F(x) =Pr (X ≤x) =

0 if x≤a,/integraldisplayx
a1
b−adu if a<x ≤b,
1 if x>b .
The integral above equals (x −a)/(b −a) . So, F(x) =(x −a)/(b −a) for alla<x<b ,
which is a strictly increasing function over the entire interval of possible values of X.
The inverse of this function is the quantile function of X, which we obtain by setting 
F(x) equal to pand solving for x:
x−a
b−a=p,
x−a=p(b −a),
x=a+p(b −a) =pb +(1−p)a.
Figure 3.8 illustrates how the calculation of a quantile relates to the c.d.f. 
The quantile function of Xis F−1(p) =pb +(1−p)a for 0<p< 1. In particular, 
F−1(1/2)=(b +a)/ 2. ◭
Note:Quantiles,Likec.d.f.’s,DependontheDistribution Only. Any two random
variables with the same distribution have the same quantile function. When we refer 
to a quantile of X, we mean a quantile of the distribution of X.
QuantilesofDiscreteDistributions It is convenient to be able to calculate quantiles 
for discrete distributions as well. The quantile function of Deﬁnition 3.3.2 exists for all 
distributions whether discrete, continuous, or otherwise. For example, in Fig. 3.6, let 
z0≤p≤z1. Then the smallest xsuch thatF(x) ≥pis x1. For every value of x<x 1,
we have F(x)<z 0≤pandF(x 1)=z1. Notice that F(x) =z1for allxbetweenx1
andx2, but since x1is the smallest of all those numbers, x1is the pquantile. Because 
distribution functions are continuous from the right, the smallest xsuch thatF(x) ≥p
exists for all 0 <p< 1. For p=1, there is no guarantee that such an xwill exist. For 
example, in Fig. 3.6, F(x 4)=1, but in Example 3.3.1, F(x)< 1 for all x. For p=0, 
there is never a smallest xsuch thatF(x) =0 because lim x→−∞F(x) =0. That is, if 
F(x 0)=0, then F(x) =0 for all x<x 0. For these reasons, we never talk about the 0 
or 1 quantiles. 
3.3 The Cumulative Distribution Function 115
Table 3.1 Quantile function
for Example 3.3.9
p F −1(p)
(0,0.1681] 0
(0.1681,0.5283] 1
(0.5283,0.8370] 2
(0.8370,0.9693] 3
(0.9693,0.9977] 4
(0.9977,1) 5
Example 
3.3.9Quantiles of a Binomial Distribution . Let Xhave the binomial distribution with pa-
rameters 5 and 0.3. The binomial table in the back of the book has the p.f. fof X,
which we reproduce here together with the c.d.f. F:
x 0 1 2 3 4 5 
f(x) 0.1681 0.3602 0.3087 0.1323 0.0284 0.0024
F(x) 0.1681 0.5283 0.8370 0.9693 0.9977 1
(A little rounding error occurred in the p.f.) So, for example, the 0.5 quantile of this 
distribution is 1, which is also the 0.25 quantile and the 0.20 quantile. The entire 
quantile function is in Table 3.1. So, the 90th percentile is 3, which is also the 95th 
percentile, etc. ◭
Certain quantiles have special names.
Deﬁnition
3.3.3Median/Quartiles . The 1/2 quantile or the 50th percentile of a distribution is called its 
median . The 1/4 quantile or 25th percentile is the lowerquartile . The 3/4 quantile or 
75th percentile is called the upperquartile .
Note:TheMedianIsSpecial. The median of a distribution is one of several special 
features that people like to use when sumarizing the distribution of a random vari- 
able. We shall discuss summaries of distributions in more detail in Chapter 4. Because 
the median is such a popular summary, we need to note that there are several dif- 
ferent but similar “deﬁnitions” of median. Recall that the 1/2 quantile is the smallest
numberxsuch thatF(x) ≥1/2. For some distributions, usually discrete distributions, 
there will be an interval of numbers [ x1,x 2)such that for all x∈[x1,x 2),F(x) =1/2. 
In such cases, it is common to refer to all such x(includingx2) as medians of the dis- 
tribution. (See Deﬁnition 4.5.1.) Another popular convention is to call (x 1+x2)/ 2
the median. This last is probably the most common convention. The readers should 
be aware that, whenever they encounter a median, it might be any one of the things 
that we just discussed. Fortunately, they all mean nearly the same thing, namely that 
the number divides the distribution in half as closely as is possible. 
116 Chapter 3 Random Variables and Distributions 
Example 
3.3.10 Uniform Distribution on Integers . Let Xhave the uniform distribution on the integers 
1,2,3,4. (See Deﬁnition 3.1.6.) The c.d.f. of Xis 
F(x) =

0 if x< 1, 
1/4 if 1 ≤x< 2, 
1/2 if 2 ≤x< 3, 
3/4 if 3 ≤x< 4, 
1 if x≥4.
The 1/2 quantile is 2, but every number in the interval [2 ,3] might be called a median. 
The most popular choice would be 2.5. ◭
One advantage to describing a distribution by the quantile function rather than 
by the c.d.f. is that quantile functions are easier to display in tabular form for multiple 
distributions. The reason is that the domain of the quantile function is always the 
interval(0,1)no matter what the possible values of Xare. Quantiles are also useful 
for summarizing distributions in terms of where the probability is. For example, if 
one wishes to say where the middle half of a distribution is, one can say that it lies 
between the 0.25 quantile and the 0.75 quantile. In Sec. 8.5, we shall see how to use 
quantiles to help provide estimates of unknown quantities after observing data. 
In Exercise 19, you can show how to recover the c.d.f. from the quantile function. 
Hence, the quantile function is an alternative way to characterize a distribution. 
Summary
The c.d.f. Fof a random variable Xis F(x) =Pr (X ≤x) for all realx. This function 
is continuous from the right. If we let F(x −)equal the limit of F(y) as yapproaches
xfrom below, then F(x) −F(x −)=Pr (X =x) . A continuous distribution has a 
continuous c.d.f. and F′(x) =f(x) , the p.d.f. of the distribution, for all xat which 
Fis differentiable. A discrete distribution has a c.d.f. that is constant between the 
possible values and jumps by f(x) at each possible value x. The quantile function 
F−1(p) is equal to the smallest xsuch thatF(x) ≥pfor 0<p< 1. 
Exercises 
1. Suppose that a random variable Xhas the Bernoulli
distribution with parameter p=0.7. (See Deﬁnition 
3.1.5.) Sketch the c.d.f. of X.
2. Suppose that a random variable Xcan take only the
values−2,0,1, and 4, and that the probabilities of these 
values are as follows: Pr (X =− 2)=0.4, Pr (X =0)=0.1, 
Pr (X =1)=0.3, and Pr (X =4)=0.2. Sketch the c.d.f. of 
X.
3. Suppose that a coin is tossed repeatedly until a head is 
obtained for the ﬁrst time, and let Xdenote the number
of tosses that are required. Sketch the c.d.f. of X.
4. Suppose that the c.d.f. Fof a random variable Xis as 
sketched in Fig. 3.9. Find each of the following probabili- 
ties:a. Pr (X =− 1)b. Pr (X< 0)
c. Pr (X ≤0)d. Pr (X =1)
e. Pr (0<X ≤3)f. Pr (0<X< 3)
g. Pr (0≤X≤3)h. Pr (1<X ≤2)
i. Pr (1≤X≤2)j. Pr (X> 5)
k. Pr (X ≥5)l. Pr (3≤X≤4)
5. Suppose that the c.d.f. of a random variable Xis as 
follows:
F(x) =

0 for x≤0, 
1
9x2for 0<x ≤3, 
1 for x> 3. 
Find and sketch the p.d.f. of X.
3.3 The Cumulative Distribution Function 117
6. Suppose that the c.d.f. of a random variable Xis as 
follows:
F(x) =/braceleftbiggex−3forx≤3, 
1 for x> 3. 
Find and sketch the p.d.f. of X.
7. Suppose, as in Exercise 7 of Sec. 3.2, that a random 
variableXhas the uniform distribution on the interval 
[−2,8]. Find and sketch the c.d.f. of X.
8. Suppose that a point in the xy -plane is chosen at ran- 
dom from the interior of a circle for which the equation is 
x2+y2=1; and suppose that the probability that the point 
will belong to each region inside the circle is proportional 
to the area of that region. Let Zdenote a random variable 
representing the distance from the center of the circle to 
the point. Find and sketch the c.d.f. of Z.
9. Suppose that Xhas the uniform distribution on the 
interval [0 ,5] and that the random variable Yis deﬁned 
by Y=0 if X≤1, Y=5 if X≥3, and Y=Xotherwise.
Sketch the c.d.f. of Y.
10.For the c.d.f. in Example 3.3.4, ﬁnd the quantile func- 
tion.
11.For the c.d.f. in Exercise 5, ﬁnd the quantile function. 
12.For the c.d.f. in Exercise 6, ﬁnd the quantile function. 
13.Suppose that a broker believes that the change in 
valueXof a particular investment over the next two 
months has the uniform distribution on the interval [ −12 ,
24]. Find the value at risk VaR for two months at proba- 
bility level 0.95.
14.Find the quartiles and the median of the binomial 
distribution with parameters n=10 and p=0.2. 
1
0.8
0.6
0.4
1 /H110021 2 3 4 5 0.2
0 xF(x)
Figure 3.9 The c.d.f. for Exercise 4. 15.Suppose that Xhas the p.d.f. 
f(x) =/braceleftbigg
2xif 0 <x< 1, 
0 otherwise. 
Find and sketch the c.d.f. or X.
16.Find the quantile function for the distribution in Ex- 
ample 3.3.1.
17.Prove that the quantile function F−1of a general ran- 
dom variable Xhas the following three properties that are
analogous to properties of the c.d.f.: 
a. F−1is a nondecreasing function of pfor 0<p< 1. 
b. Letx0=limp→0
p> 0F−1(p) andx1=limp→1
p< 1F−1(p) .
Then x0equals the greatest lower bound on the set 
of numbers csuch that Pr (X ≤c)> 0, and x1equals
the least upper bound on the set of numbers dsuch
that Pr (X ≥d)> 0. 
c. F−1is continuous from the left; that is F−1(p) =
F−1(p −)for all 0<p< 1. 
18.LetXbe a random variable with quantile function 
F−1. Assume the following three conditions: (i) F−1(p) =
cfor allpin the interval (p 0,p 1), (ii) either p0=0 or 
F−1(p 0)<c , and (iii) either p1=1 or F−1(p)>c forp> 
p1. Prove that Pr (X =c) =p1−p0.
19.LetXbe a random variable with c.d.f. Fand quantile
functionF−1. Let x0andx1be as deﬁned in Exercise 17. 
(Note thatx0=−∞ and/orx1=∞ are possible.) Prove 
that for allxin the open interval (x 0,x 1),F(x) is the largest 
psuch thatF−1(p) ≤x.
20.In Exercise 13 of Sec. 3.2, draw a sketch of the c.d.f. F
of Xand ﬁndF( 10 ).
118 Chapter 3 Random Variables and Distributions 
3.4 Bivariate Distributions 
Wegeneralizetheconceptofdistributionofarandomvariabletothejointdistri-
butionoftworandomvariables.Indoingso,weintroducethejointp.f.fortwo 
discreterandomvariables,thejointp.d.f.fortwocontinuousrandomvariables,
andthejointc.d.f.foranytworandomvariables.Wealsointroduceajointhybrid 
ofp.f.andp.d.f.forthecaseofonediscreterandomvariableandonecontinuous 
randomvariable.
Example 
3.4.1Demands for Utilities . In Example 3.1.5, we found the distribution of the random 
variableXthat represented the demand for water. But there is another random 
variable,Y, the demand for electricity, that is also of interest. When discussing 
two random variables at once, it is often convenient to put them together into an 
ordered pair, (X,Y) . As early as Example 1.5.4 on page 19, we actually calculated 
some probabilities associated with the pair (X,Y) . In that example, we deﬁned two 
eventsAandBthat we now can express as A={X≥115}andB={Y≥110}. In 
Example 1.5.4, we computed Pr (A ∩B) and Pr (A ∪B) . We can express A∩Band
A∪Bas events involving the pair (X,Y) . For example, deﬁne the set of ordered 
pairsC={(x,y) :x≥115 andy≥110}so that that the event {(X,Y) ∈C) }= A∩B.
That is, the event that the pair of random variables lies in the set Cis the same 
as the intersection of the two events AandB. In Example 1.5.4, we computed 
Pr (A ∩B) =0.1198. So, we can now assert that Pr ((X,Y) ∈C) =0.1198. ◭
Deﬁnition
3.4.1Joint/Bivariate Distribution . Let XandYbe random variables. The jointdistribution
or bivariatedistribution of XandYis the collection of all probabilities of the form 
Pr[ (X,Y) ∈C]for all sets Cof pairs of real numbers such that {(X,Y) ∈C}is an event. 
It is a straightforward consequence of the deﬁnition of the joint distribution of Xand
Ythat this joint distribution is itself a probability measure on the set of ordered pairs 
of real numbers. The set {(X,Y) ∈C}will be an event for every set Cof pairs of real 
numbers that most readers will be able to imagine. 
In this section and the next two sections, we shall discuss convenient ways to 
characterize and do computations with bivariate distributions. In Sec. 3.7, these 
considerations will be extended to the joint distribution of an arbitrary ﬁnite number 
of random variables. 
Discrete Joint Distributions 
Example 
3.4.2Theater Patrons . Suppose that a sample of 10 people is selected at random from a 
theater with 200 patrons. One random variable of interest might be the number X
of people in the sample who are over 60 years of age, and another random variable 
might be the number Yof people in the sample who live more than 25 miles from 
the theater. For each ordered pair (x,y) withx=0,..., 10 and y=0,..., 10, we 
might wish to compute Pr ((X,Y) =(x,y)) , the probability that there are xpeople in 
the sample who are over 60 years of age and there are ypeople in the sample who 
live more than 25 miles away. ◭
Deﬁnition
3.4.2Discrete Joint Distribution . Let XandYbe random variables, and consider the ordered 
pair(X,Y) . If there are only ﬁnitely or at most countably many different possible 
values(x,y) for the pair (X,Y) , then we say that XandYhave adiscrete joint
distribution .
3.4 Bivariate Distributions 119
The two random variables in Example 3.4.2 have a discrete joint distribution. 
Theorem
3.4.1Suppose that two random variables XandYeach have a discrete distribution. Then 
XandYhave a discrete joint distribution. 
Proof If both XandYhave only ﬁnitely many possible values, then there will be 
only a ﬁnite number of different possible values ( x,y) for the pair ( X,Y). On the 
other hand, if either Xor Yor both can take a countably inﬁnite number of possible 
values, then there will also be a countably inﬁnite number of possible values for the 
pair (X,Y). In all of these cases, the pair (X,Y) has a discrete joint distribution. 
When we deﬁne continuous joint distribution shortly, we shall see that the obvious 
analog of Theorem 3.4.1 is not true. 
Deﬁnition
3.4.3Joint Probability Function, p.f . The jointprobabilityfunction , or the jointp.f. , of Xand
Yis deﬁned as the function fsuch that for every point ( x,y) in the xy -plane,
f(x,y) =Pr (X =xandY=y).
The following result is easy to prove because there are at most countably many 
pairs(x,y) that must account for all of the probability a discrete joint distribution. 
Theorem
3.4.2LetXandYhave a discrete joint distribution. If (x,y) is not one of the possible 
values of the pair (X,Y) , then f(x,y) =0. Also, 
/summationdisplay
All(x,y) f(x,y) =1.
Finally, for each set Cof ordered pairs, 
Pr[ (X,Y) ∈C]=/summationdisplay
(x,y) ∈Cf(x,y).
Example 
3.4.3Specifying a Discrete Joint Distribution by a Table of Proba bilities . In a certain suburban 
area, each household reported the number of cars and the number of television sets 
that they owned. Let Xstand for the number of cars owned by a randomly selected 
household in this area. Let Ystand for the number of television sets owned by that 
same randomly selected household. In this case, Xtakes only the values 1, 2, and 3; 
Ytakes only the values 1, 2, 3, and 4; and the joint p.f. fof XandYis as speciﬁed in 
Table 3.2. 
Table 3.2 Joint p.f. f(x,y) for
Example 3.4.3
y
x 1 2 3 4 
1 0.1 0 0.1 0 
2 0.3 0 0.1 0.2 
3 0 0.2 0 0 
120 Chapter 3 Random Variables and Distributions 
Figure 3.10 The joint p.f. of 
XandYin Example 3.4.3. f(x, y )
xy1 1
2
3234
This joint p.f. is sketched in Fig. 3.10. We shall determine the probability that 
the randomly selected household owns at least two of both cars and televisions. In 
symbols, this is Pr (X ≥2 and Y≥2).
By summing f(x,y) over all values of x≥2 and y≥2, we obtain the value 
Pr (X ≥2 and Y≥2)=f( 2,2)+f( 2,3)+f( 2,4)+f( 3,2)
+f( 3,3)+f( 3,4)
=0.5.
Next, we shall determine the probability that the randomly selected household owns 
exactly one car, namely Pr (X =1). By summing the probabilities in the ﬁrst row of 
the table, we obtain the value 
Pr (X =1)=4/summationdisplay
y=1f( 1,y) =0.2. ◭
Continuous Joint Distributions
Example 
3.4.4Demands for Utilities . Consider again the joint distribution of XandYin Exam- 
ple 3.4.1. When we ﬁrst calculated probabilities for these two random variables back 
in Example 1.5.4 on page 19 (even before we named them or called them random 
variables), we assumed that the probability of each subset of the sample space was 
proportional to the area of the subset. Since the area of the sample space is 29,204, 
the probability that the pair (X,Y) lies in a region Cis the area of Cdivided by 29,204. 
We can also write this relation as 
Pr ((X,Y) ∈C}= /integraldisplay
C/integraldisplay1
29 ,204dxdy, (3.4.1)
assuming that the integral exists. ◭
If one looks carefully at Eq. (3.4.1), one will notice the similarity to Eqs. (3.2.2) 
and (3.2.1). We formalize this connection as follows. 
Deﬁnition
3.4.4Continuous Joint Distribution/Joint p.d.f./Support . Two random variables XandYhave
acontinuousjointdistribution if there exists a nonnegative function fdeﬁned over
the entirexy -plane such that for every subset Cof the plane, 
Pr[ (X,Y) ∈C]=/integraldisplay
C/integraldisplay
f(x,y)dxdy,
3.4 Bivariate Distributions 121
if the integral exists. The function fis called the jointprobabilitydensityfunction
(abbreviated jointp.d.f. ) of XandY. The closure of the set {(x,y) :f(x,y)> 0}is 
called thesupportof(thedistributionof) (X,Y) .
Example 
3.4.5Demands for Utilities . In Example 3.4.4, it is clear from Eq. (3.4.1) that the joint p.d.f. 
of XandYis the function 
f(x,y) =

1
29 ,204for 4≤x≤200 and 1≤y≤150, 
0 otherwise. ◭(3.4.2)
It is clear from Deﬁnition 3.4.4 that the joint p.d.f. of two random variables 
characterizes their joint distribution. The following result is also straightforward. 
Theorem
3.4.3A joint p.d.f. must satisfy the following two conditions: 
f(x,y) ≥0 for −∞<x< ∞and−∞<y< ∞,
and
/integraldisplay∞
−∞/integraldisplay∞
−∞f(x,y)dxdy =1.
Any function that satisﬁes the two displayed formulas in Theorem 3.4.3 is the joint 
p.d.f. for some probability distribution. 
An example of the graph of a joint p.d.f. is presented in Fig. 3.11. 
The total volume beneath the surface z=f(x,y) and above the xy -plane must be 
1. The probability that the pair ( X,Y) will belong to the rectangle Cis equal to the 
volume of the solid ﬁgure with base Ashown in Fig. 3.11. The top of this solid ﬁgure 
is formed by the surface z=f(x,y) .
In Sec. 3.5, we will show that if XandYhave a continuous joint distribution, 
thenXandYeach have a continuous distribution when considered separately. This 
seems reasonable intutively. However, the converse of this statement is not true, and 
the following result helps to show why. 
Figure 3.11 An example of 
a joint p.d.f. 
x yf(x, y )
C
122 Chapter 3 Random Variables and Distributions 
Theorem
3.4.4For every continuous joint distribution on the xy -plane, the following two statements 
hold:
i. Every individual point, and every inﬁnite sequence of points, in the xy -plane
has probability 0. 
ii. Letfbe a continuous function of one real variable deﬁned on a (possibly 
unbounded) interval (a,b) . The sets {(x,y) :y=f(x),a<x<b }and{(x,y) :
x=f(y),a<y<b }have probability 0. 
Proof According to Deﬁnition 3.4.4, the probability that a continuous joint distri- 
bution assigns to a speciﬁed region of the xy -plane can be found by integrating the 
joint p.d.f. f(x,y) over that region, if the integral exists. If the region is a single point, 
the integral will be 0. By Axiom 3 of probability, the probability for any countable 
collection of points must also be 0. The integral of a function of two variables over 
the graph of a continuous function in the xy -plane is also 0. 
Example 
3.4.6Not a Continuous Joint Distribution . It follows from (ii) of Theorem 3.4.4 that the 
probability that (X,Y) will lie on each speciﬁed straight line in the plane is 0. If 
Xhas a continuous distribution and if Y=X, then both XandYhave continuous
distributions, but the probability is 1 that (X,Y) lies on the straight line y=x. Hence, 
XandYcannot have a continuous joint distribution. ◭
Example 
3.4.7Calculating a Normalizing Constant . Suppose that the joint p.d.f. of XandYis speciﬁed 
as follows: 
f(x,y) =/braceleftbigg
cx 2yforx2≤y≤1, 
0 otherwise. 
We shall determine the value of the constant c.
The support Sof (X,Y) is sketched in Fig. 3.12. Since f(x,y) =0 outside S, it 
follows that
/integraldisplay∞
−∞/integraldisplay∞
−∞f(x,y)dxdy =/integraldisplay
S/integraldisplay
f(x,y)dxdy 
=/integraldisplay1
−1/integraldisplay1
x2cx 2ydydx =4
21 c.(3.4.3)
Since the value of this integral must be 1, the value of cmust be 21/4. 
The limits of integration on the last integral in (3.4.3) were determined as follows. 
We have our choice of whether to integrate xor yas the inner integral, and we chose 
y. So, we must ﬁnd, for each x, the interval of yvalues over which to integrate. From 
Fig. 3.12, we see that, for each x,yruns from the curve where y=x2to the line 
wherey=1. The interval of xvalues for the outer integral is from −1 to 1 according 
to Fig. 3.12. If we had chosen to integrate xon the inside, then for each y, we see that 
xruns from−√yto √y, while yruns from 0 to 1. The ﬁnal answer would have been 
the same. ◭
Example 
3.4.8Calculating Probabilities from a Joint p.d.f . For the joint distribution in Example 3.4.7, 
we shall now determine the value of Pr (X ≥Y) .
The subset S0of Swherex≥yis sketched in Fig. 3.13. Hence, 
Pr (X ≥Y) =/integraldisplay
S0/integraldisplay
f(x,y)dxdy =/integraldisplay1
0/integraldisplayx
x221 
4x2ydydx =3
20 . ◭
3.4 Bivariate Distributions 123
Figure 3.12 The support S
of (X,Y) in Example 3.4.8. 
xy
1 /H110021S(1, 1) (/H110021, 1)y/H11005x2
Figure 3.13 The subset S0
of the support Swherex≥y
in Example 3.4.8. 
xy
1 /H110021(1, 1)
y/H11005x2y/H11005x
Example 
3.4.9Determining a Joint p.d.f. by Geometric Methods . Suppose that a point (X,Y) is se- 
lected at random from inside the circle x2+y2≤9. We shall determine the joint 
p.d.f. of XandY.
The support of (X,Y) is the set Sof points on and inside the circle x2+y2≤9. 
The statement that the point (X,Y) is selected at random from inside the circle is 
interpreted to mean that the joint p.d.f. of XandYis constant over Sand is 0 outside S.
Thus, 
f(x,y) =/braceleftbiggcfor(x,y) ∈S,
0 otherwise. 
We must have 
/integraldisplay
S/integraldisplay
f(x,y)dxdy =c×(area of S) =1.
Since the area of the circle Sis 9 π, the value of the constant cmust be 1 /( 9π) .◭
Mixed Bivariate Distributions 
Example 
3.4.10 A Clinical Trial . Consider a clinical trial (such as the one described in Example 2.1.12) 
in which each patient with depression receives a treatment and is followed to see 
whether they have a relapse into depression. Let Xbe the indicator of whether or 
not the ﬁrst patient is a “success” (no relapse). That is X=1 if the patient does not 
relapse and X=0 if the patient relapses. Also, let Pbe the proportion of patients 
who have no replapse among all patients who might receive the treatment. It is clear 
thatXmust have a discrete distribution, but it might be sensible to think of Pas 
a continuous random variable taking its value anywhere in the interval [0 ,1]. Even 
thoughXandPcan have neither a joint discrete distribution nor a joint continuous 
distribution, we can still be interested in the joint distribution of XandP. ◭
124 Chapter 3 Random Variables and Distributions 
Prior to Example 3.4.10, we have discussed bivariate distributions that were 
either discrete or continuous. Occasionally, one must consider a mixed bivariate dis- 
tribution in which one of the random variables is discrete and the other is continuous. 
We shall use a function f(x,y) to characterize such a joint distribution in much the 
same way that we use a joint p.f. to characterize a discrete joint distribution or a joint 
p.d.f. to characterize a continuous joint distribution. 
Deﬁnition
3.4.5Joint p.f./p.d.f . Let XandYbe random variables such that Xis discrete and Yis 
continuous. Suppose that there is a function f(x,y) deﬁned on the xy -plane such
that, for every pair AandBof subsets of the real numbers, 
Pr (X ∈AandY∈B) =/integraldisplay
B/summationdisplay
x∈Af(x,y)dy, (3.4.4)
if the integral exists. Then the function fis called the jointp.f./p.d.f. of XandY.
Clearly, Deﬁnition 3.4.5 can be modiﬁed in an obvious way if Yis discrete and X
is continuous. Every joint p.f./p.d.f. must satisfy two conditions. If Xis the discrete 
random variable with possible values x1,x 2,... andYis the continuous random 
variable, then f(x,y) ≥0 for all x,y and
/integraldisplay∞
−∞∞/summationdisplay
i=1f(x i,y)dy =1. (3.4.5)
Becausefis nonnegative, the sum and integral in Eqs. (3.4.4) and (3.4.5) can be done 
in whichever order is more convenient. 
Note: Probabilities of More General Sets. For a general set Cof pairs of real 
numbers, we can compute Pr ((X,Y) ∈C) using the joint p.f./p.d.f. of XandY. For 
eachx, let Cx={y:(x,y) ∈C}. Then 
Pr ((X,Y) ∈C) =/summationdisplay
Allx/integraldisplay
Cxf(x,y)dy,
if all of the integrals exist. Alternatively, for each y, deﬁne Cy={x:(x,y) ∈C}, and 
then
Pr ((X,Y) ∈C) =/integraldisplay∞
−∞/bracketleftBigg/summationdisplay
x∈Cyf(x,y) /bracketrightBigg
dy,
if the integral exists. 
Example 
3.4.11 A Joint p.f./p.d.f . Suppose that the joint p.f./p.d.f. of XandYis 
f(x,y) =xy x−1
3,forx=1,2,3 and 0 <y< 1. 
We should check to make sure that this function satisﬁes (3.4.5). It is easier to 
integrate over the yvalues ﬁrst, so we compute 
3/summationdisplay
x=1/integraldisplay1
0xy x−1
3dy =3/summationdisplay
x=11
3=1.
Suppose that we wish to compute the probability that Y≥1/2 and X≥2. That is, we 
want Pr (X ∈AandY∈B) withA=[2 ,∞)andB=[1 /2,∞). So, we apply Eq. (3.4.4) 
3.4 Bivariate Distributions 125
to get the probability 
3/summationdisplay
x=2/integraldisplay1
1/2xy x−1
3dy =3/summationdisplay
x=2/parenleftbigg1−(1/2)x
3/parenrightbigg
=0.5417.
For illustration, we shall compute the sum and integral in the other order also. 
For each y∈[1 /2,1),/summationtext3
x=2f(x,y) =2y/ 3+y2. For y≥1/2, the sum is 0. So, the 
probability is 
/integraldisplay1
1/2/bracketleftbigg2
3y+y2/bracketrightbigg
dy =1
3/bracketleftBigg
1−/parenleftbigg1
2/parenrightbigg2/bracketrightBigg
+1
3/bracketleftBigg
1−/parenleftbigg1
2/parenrightbigg3/bracketrightBigg
=0.5417. ◭
Example 
3.4.12 A Clinical Trial . A possible joint p.f./p.d.f. for XandPin Example 3.4.10 is 
f(x,p) =px(1−p) 1−x,forx=0,1 and 0 <p< 1. 
Here,Xis discrete and Pis continuous. The function fis nonnegative, and the 
reader should be able to demonstrate that it satisﬁes (3.4.5). Suppose that we wish 
to compute Pr (X ≤0 and P≤1/2). This can be computed as 
/integraldisplay1/2
0(1−p)dp =− 1
2[(1−1/2)2−(1−0)2]=3
8.
Suppose that we also wish to compute Pr (X =1). This time, we apply Eq. (3.4.4) with 
A={1}andB=(0,1). In this case, 
Pr (X =1)=/integraldisplay1
0pdp =1
2. ◭
A more complicated type of joint distribution can also arise in a practical prob- 
lem.
Example 
3.4.13 A Complicated Joint Distribution . Suppose that XandYare the times at which two 
speciﬁc components in an electronic system fail. There might be a certain probability 
p ( 0<p< 1)that the two components will fail at the same time and a certain 
probability 1−pthat they will fail at different times. Furthermore, if they fail at 
the same time, then their common failure time might be distributed according to a 
certain p.d.f. f(x) ; if they fail at different times, then these times might be distributed 
according to a certain joint p.d.f. g(x,y) .
The joint distribution of XandYin this example is not continuous, because 
there is positive probability pthat(X,Y) will lie on the line x=y. Nor does the joint 
distribution have a joint p.f./p.d.f. or any other simple function to describe it. There 
are ways to deal with such joint distributions, but we shall not discuss them in this 
text. ◭
Bivariate Cumulative Distribution Functions 
The ﬁrst calculation in Example 3.4.12, namely, Pr (X ≤0 and Y≤1/2), is a gener- 
alization of the calculation of a c.d.f. to a bivariate distribution. We formalize the 
generalization as follows. 
Deﬁnition
3.4.6Joint (Cumulative) Distribution Function/c.d.f . The jointdistributionfunction or joint
cumulative distribution function (joint c.d.f. ) of two random variables XandYis 
126 Chapter 3 Random Variables and Distributions 
Figure3.14 The probability 
of a rectangle. 
acd
b xy
A
deﬁned as the function Fsuch that for all values of xandy( −∞<x< ∞and−∞<
y< ∞),
F(x,y) =Pr (X ≤xandY≤y).
It is clear from Deﬁnition 3.4.6 that F(x,y) is monotone increasing in xfor each ﬁxed
yand is monotone increasing in yfor each ﬁxed x.
If the joint c.d.f. of two arbitrary random variables XandYis F, then the 
probability that the pair ( X,Y) will lie in a speciﬁed rectangle in the xy -plane can be 
found fromFas follows: For given numbers a<b andc<d ,
Pr (a<X ≤bandc<Y ≤d) 
=Pr (a<X ≤bandY≤d) −Pr (a<X ≤bandY≤c) 
=[Pr(X ≤bandY≤d) −Pr (X ≤aandY≤d) ] (3.4.6) 
−[Pr(X ≤bandY≤c) −Pr (X ≤aandY≤c) ]
=F(b,d) −F(a,d) −F(b,c) +F(a,c).
Hence, the probability of the rectangle Csketched in Fig. 3.14 is given by the 
combination of values of Fjust derived. It should be noted that two sides of the 
rectangle are included in the set Cand the other two sides are excluded. Thus, if there 
are points or line segments on the boundary of Cthat have positive probability, it is 
important to distinguish between the weak inequalities and the strict inequalities in 
Eq. (3.4.6).
Theorem
3.4.5LetXandYhave a joint c.d.f. F. The c.d.f. F1of just the single random variable X
can be derived from the joint c.d.f. Fas F1(x) =limy→∞F(x,y) . Similarly, the c.d.f. 
F2of YequalsF2(y) =limx→∞F(x,y) , for 0 <y< ∞.
Proof We prove the claim about F1as the claim about F2is similar. Let −∞<x< ∞.
Deﬁne
B0={X≤xandY≤0},
Bn={X≤xandn−1<Y ≤n},forn=1,2,...,
Am=m/uniondisplay
n=0Bn,form=1,2,....
Then {X≤x}= /uniontext∞
n=− 0Bn, and Am={X≤xandY≤m}form=1,2,.... It follows 
that Pr (A m)=F(x,m) for eachm. Also, 
3.4 Bivariate Distributions 127
F1(x) =Pr (X ≤x) =Pr /parenleftBigg∞/uniondisplay
n=1Bn/parenrightBigg
=∞/summationdisplay
n=0Pr (B n)=limm→∞Pr (A m)
=limm→∞F(x,m) =limy→∞F(x,y),
where the third equality follows from countable additivity and the fact that the Bn
events are disjoint, and the last equality follows from the fact that F(x,y) is monotone 
increasing in yfor each ﬁxed x.
Other relationships involving the univariate distribution of X, the univariate distri- 
bution of Y, and their joint bivariate distribution will be presented in the next section. 
Finally, if XandYhave a continuous joint distribution with joint p.d.f. f, then 
the joint c.d.f. at (x,y) is 
F(x,y) =/integraldisplayy
−∞/integraldisplayx
−∞f(r,s)drds.
Here, the symbols randsare used simply as dummy variables of integration. The 
joint p.d.f. can be derived from the joint c.d.f. by using the relations 
f(x,y) =∂2F(x,y) 
∂x∂y =∂2F(x,y) 
∂y∂x 
at every point ( x,y) at which these second-order derivatives exist. 
Example 
3.4.14 Determining a Joint p.d.f. from a Joint c.d.f . Suppose that XandYare random variables
that take values only in the intervals 0 ≤X≤2 and 0 ≤Y≤2. Suppose also that the 
joint c.d.f. of XandY, for 0 ≤x≤2 and 0 ≤y≤2, is as follows: 
F(x,y) =1
16 xy(x +y). (3.4.7)
We shall ﬁrst determine the c.d.f. F1of just the random variable Xand then determine
the joint p.d.f. fof XandY.
The value of F(x,y) at any point ( x,y) in the xy -plane that does not represent
a pair of possible values of XandYcan be calculated from (3.4.7) and the fact that 
F(x,y) =Pr (X ≤xandY≤y) . Thus, if either x< 0 or y< 0, then F(x,y) =0. If both 
x> 2 and y> 2, then F(x,y) =1. If 0 ≤x≤2 and y> 2, then F(x,y) =F(x, 2), and 
it follows from Eq. (3.4.7) that 
F(x,y) =1
8x(x +2).
Similarly, if 0 ≤y≤2 and x> 2, then 
F(x,y) =1
8y(y +2).
The function F(x,y) has now been speciﬁed for every point in the xy -plane.
By letting y→∞ , we ﬁnd that the c.d.f. of just the random variable Xis 
F1(x) =

0 for x< 0, 
1
8x(x +2)for 0≤x≤2, 
1 for x> 2. 
128 Chapter 3 Random Variables and Distributions 
Furthermore, for 0 <x< 2 and 0 <y< 2, 
∂2F(x,y) 
∂x∂y =1
8(x +y).
Also, if x< 0, y< 0, x> 2, or y> 2, then 
∂2F(x,y) 
∂x∂y =0.
Hence, the joint p.d.f. of XandYis 
f(x,y) =/braceleftBigg
1
8(x +y) for 0<x< 2 and 0 <y< 2, 
0 otherwise. ◭
Example 
3.4.15 Demands for Utilities . We can compute the joint c.d.f. for water and electric demand 
in Example 3.4.4 by using the joint p.d.f. that was given in Eq. (3.4.2). If either x≤4 or 
y≤1, then F(x,y) =0 because either X≤xor Y≤ywould be impossible. Similarly, 
if both x≥200 andy≥150, F(x,y) =1 because both X≤xandY≤ywould be sure 
events. For other values of xandy, we compute 
F(x,y) =

/integraldisplayx
4/integraldisplayy
11
29 ,204dydx =xy 
29 ,204for 4≤x≤200, 1 ≤y≤150, 
/integraldisplayx
4/integraldisplay150
11
29 ,204dydx =x
196for 4≤x≤200, y> 150, 
/integraldisplay200
4/integraldisplayy
11
29 ,204dydx =y
149forx> 200, 1 ≤y≤150. 
The reason that we need three cases in the formula for F(x,y) is that the joint p.d.f. 
in Eq. (3.4.2) drops to 0 when xcrosses above 200 or when ycrosses above 150;
hence, we never want to integrate 1 /29 ,204 beyond x=200 or beyond y=150. If 
one takes the limit as y→∞ of F(x,y) (for ﬁxed 4≤x≤200), one gets the second 
case in the formula above, which then is the c.d.f. of X,F1(x) . Similarly, if one takes 
the limx→∞F(x,y) (for ﬁxed 1≤y≤150), one gets the third case in the formula, 
which then is the c.d.f. of Y,F2(y) . ◭
Summary
The joint c.d.f. of two random variables XandYis F(x,y) =Pr (X ≤xandY≤y) .
The joint p.d.f. of two continuous random variables is a nonnegative function fsuch
that the probability of the pair (X,Y) being in a set Cis the integral of f(x,y) over the
setC, if the integral exists. The joint p.d.f. is also the second mixed partial derivative 
of the joint c.d.f. with respect to both variables. The joint p.f. of two discrete random 
variables is a nonnegative function fsuch that the probability of the pair (X,Y) being
in a set Cis the sum of f(x,y) over all points in C. A joint p.f. can be strictly positive at 
countably many pairs (x,y) at most. The joint p.f./p.d.f. of a discrete random variable 
Xand a continuous random variable Yis a nonnegative function fsuch that the
probability of the pair (X,Y) being in a set Cis obtained by summing f(x,y) over
allxsuch that(x,y) ∈Cfor eachyand then integrating the resulting function of y.
3.4 Bivariate Distributions 129
Exercises 
1. Suppose that the joint p.d.f. of a pair of random vari- 
ables(X,Y) is constant on the rectangle where 0 ≤x≤2
and 0≤y≤1, and suppose that the p.d.f. is 0 off of this 
rectangle.
a. Find the constant value of the p.d.f. on the rectangle. 
b. Find Pr (X ≥Y) .
2. Suppose that in an electric display sign there are three 
light bulbs in the ﬁrst row and four light bulbs in the second 
row. Let Xdenote the number of bulbs in the ﬁrst row that 
will be burned out at a speciﬁed time t, and let Ydenote
the number of bulbs in the second row that will be burned 
out at the same time t. Suppose that the joint p.f. of Xand
Yis as speciﬁed in the following table: 
Y
X 0 1 2 3 4 
0 0.08 0.07 0.06 0.01 0.01 
1 0.06 0.10 0.12 0.05 0.02 
2 0.05 0.06 0.09 0.04 0.03 
3 0.02 0.03 0.03 0.03 0.04 
Determine each of the following probabilities: 
a. Pr (X =2) b. Pr (Y ≥2)
c. Pr (X ≤2 and Y≤2)d. Pr (X =Y) 
e. Pr (X>Y) 
3. Suppose that XandYhave a discrete joint distribution 
for which the joint p.f. is deﬁned as follows: 
f(x,y) =

c|x+y|forx=− 2,−1,0,1,2 and 
y=− 2,−1,0,1,2, 
0 otherwise. 
Determine (a)the value of the constant c;(b)Pr (X =
0 and Y=− 2);(c)Pr (X =1);(d)Pr (|X−Y|≤ 1).
4. Suppose that XandYhave a continuous joint distribu- 
tion for which the joint p.d.f. is deﬁned as follows: 
f(x,y) =/braceleftbigg
cy 2for 0≤x≤2 and 0 ≤y≤1, 
0 otherwise. 
Determine (a)the value of the constant c;(b)Pr (X +Y > 
2);(c)Pr (Y < 1/2);(d)Pr (X ≤1);(e)Pr (X =3Y) .
5. Suppose that the joint p.d.f. of two random variables X
andYis as follows: 
f(x,y) =/braceleftbigg
c(x 2+y) for 0≤y≤1−x2,
0 otherwise. Determine (a)the value of the constant c;
(b)Pr (0≤X≤1/2);(c)Pr (Y ≤X+1);
(d)Pr (Y =X2).
6. Suppose that a point ( X,Y) is chosen at random from 
the regionSin the xy -plane containing all points ( x,y)
such thatx≥0, y≥0, and 4 y+x≤4. 
a. Determine the joint p.d.f. of XandY.
b. Suppose that S0is a subset of the region Shaving area
αand determine Pr[ (X,Y) ∈S0]. 
7. Suppose that a point ( X,Y) is to be chosen from the 
squareSin the xy -plane containing all points ( x,y) such 
that 0≤x≤1 and 0 ≤y≤1. Suppose that the probabil- 
ity that the chosen point will be the corner (0,0)is 0.1, 
the probability that it will be the corner (1,0)is 0.2, the 
probability that it will be the corner (0,1)is 0.4, and the 
probability that it will be the corner (1,1)is 0.1. Suppose 
also that if the chosen point is not one of the four cor- 
ners of the square, then it will be an interior point of the 
square and will be chosen according to a constant p.d.f. 
over the interior of the square. Determine (a)Pr (X ≤1/4)
and(b)Pr (X +Y≤1).
8. Suppose that XandYare random variables such that
(X,Y) must belong to the rectangle in the xy -plane con-
taining all points (x,y) for which 0≤x≤3 and 0 ≤y≤4. 
Suppose also that the joint c.d.f. of XandYat every point 
(x,y) in this rectangle is speciﬁed as follows: 
F(x,y) =1
156xy(x 2+y).
Determine (a)Pr (1≤X≤2 and 1 ≤Y≤2);
(b)Pr (2≤X≤4 and 2 ≤Y≤4);(c)the c.d.f. of Y;
(d)the joint p.d.f. of XandY;(e)Pr (Y ≤X) .
9. In Example 3.4.5, compute the probability that water 
demandXis greater than electric demand Y.
10.LetYbe the rate (calls per hour) at which calls arrive 
at a switchboard. Let Xbe the number of calls during a 
two-hour period. A popular choice of joint p.f./p.d.f. for 
(X,Y) in this example would be one like 
f(x,y) =/braceleftBigg
(2y) x
x!e−3yif y> 0 and x=0,1,...,
0 otherwise. 
a. Verify that fis a joint p.f./p.d.f. Hint: First, sum over 
thexvalues using the well-known formula for the
power series expansion of e2y.
b. Find Pr (X =0).
11.Consider the clinical trial of depression drugs in Ex- 
ample 2.1.4. Suppose that a patient is selected at random 
from the 150 patients in that study and we record Y, an 
130 Chapter 3 Random Variables and Distributions 
Table 3.3 Proportions in clinical depression study for Exercise 11 
Treatmentgroup( Y)
Response(X) Imipramine (1) Lithium (2) Combination (3) Placebo (4)
Relapse (0) 0.120 0.087 0.146 0.160
No relapse (1) 0.147 0.166 0.107 0.067 
indicator of the treatment group for that patient, and X, an 
indicator of whether or not the patient relapsed. Table 3.3 
contains the joint p.f. of XandY.
a. Calculate the probability that a patient selected at 
random from this study used Lithium (either aloneor in combination with Imipramine) and did not re- 
lapse.
b. Calculate the probability that the patient had a re- 
lapse (without regard to the treatment group). 
3.5 Marginal Distributions 
Earlier in this chapter, we introduced distributions for random variables, and 
in Sec. 3.4 we discussed a generalization to joint distributions of two random 
variablessimultaneously.Often,westartwithajointdistributionoftworandom 
variables and we then want to ﬁnd the distribution of just one of them. The 
distributionofonerandomvariable Xcomputedfromajointdistributionisalso 
calledthemarginaldistributionof X.Eachrandomvariablewillhaveamarginal
c.d.f.aswellasamarginalp.d.f.orp.f.Wealsointroducetheconceptofindependent
randomvariables,whichisanaturalgeneralizationofindependentevents.
Deriving a Marginal p.f. or a Marginal p.d.f. 
We have seen in Theorem 3.4.5 that if the joint c.d.f. Fof two random variables X
andYis known, then the c.d.f. F1of the random variable Xcan be derived from 
F. We saw an example of this derivation in Example 3.4.15. If Xhas a continuous 
distribution, we can also derive the p.d.f. of Xfrom the joint distribution.
Example 
3.5.1Demands for Utilities . Look carefully at the formula for F(x,y) in Example 3.4.15, 
speciﬁcally the last two branches that we identiﬁed as F1(x) andF2(y) , the c.d.f.’s of 
the two individual random variables XandY. It is apparent from those two formulas 
and Theorem 3.3.5 that the p.d.f. of Xalone is 
f1(x) =

1
196for 4≤x≤200, 
0 otherwise, 
which matches what we already found in Example 3.2.1. Similarly, the p.d.f. of Y
alone is 
f2(y) =

1
149for 1≤y≤150, 
0 otherwise. ◭
The ideas employed in Example 3.5.1 lead to the following deﬁnition. 
3.5 Marginal Distributions 131
Figure 3.15 Computing
f1(x) from the joint p.f. 
xy
x3 ····· ····· x2x1x4
Deﬁnition
3.5.1Marginal c.d.f./p.f./p.d.f . Suppose that XandYhave a joint distribution. The c.d.f. of 
Xderived by Theorem 3.4.5 is called the marginalc.d.f. of X. Similarly, the p.f. or p.d.f. 
of Xassociated with the marginal c.d.f. of Xis called the marginalp.f. or marginal
p.d.f. of X.
To obtain a speciﬁc formula for the marginal p.f. or marginal p.d.f., we start with 
a discrete joint distribution. 
Theorem
3.5.1If XandYhave a discrete joint distribution for which the joint p.f. is f, then the 
marginal p.f. f1of Xis 
f1(x) =/summationdisplay
Allyf(x,y). (3.5.1)
Similarly, the marginal p.f. f2of Yis f2(y) =/summationtext
Allxf(x,y) .
Proof We prove the result for f1, as the proof for f2is similar. We illustrate the 
proof in Fig. 3.15. In that ﬁgure, the set of points in the dashed box is the set of 
pairs with ﬁrst coordinate x. The event {X=x}can be expressed as the union of the 
events represented by the pairs in the dashed box, namely, By={X=xandY=y}
for all possible y. The Byevents are disjoint and Pr (B y)=f(x,y) . Since Pr (X =x) =/summationtext
AllyPr (B y), Eq. (3.5.1) holds. 
Example 
3.5.2Deriving a Marginal p.f. from a Table of Probabilities . Suppose that XandYare the
random variables in Example 3.4.3 on page 119. These are respectively the numbers 
of cars and televisions owned by a radomly selected household in a certain suburban 
area. Table 3.2 on page 119 gives their joint p.f., and we repeat that table in Table 3.4 
together with row and column totals added to the margins. 
The marginal p.f. f1of Xcan be read from the row totals of Table 3.4. The 
numbers were obtained by summing the values in each row of this table from the four 
columns in the central part of the table (those labeled y=1,2,3,4). In this way, it is 
found thatf1(1)=0.2, f1(2)=0.6, f1(3)=0.2, and f1(x) =0 for all other values of x.
This marginal p.f. gives the probabilities that a randomly selected household owns 
1, 2, or 3 cars. Similarly, the marginal p.f. f2of Y, the probabilities that a household 
owns 1, 2, 3, or 4 televisions, can be read from the column totals. These numbers were 
obtained by adding the numbers in each of the columns from the three rows in the 
central part of the table (those labeled x=1,2,3.) ◭
The name marginaldistribution derives from the fact that the marginal distribu-
tions are the totals that appear in the margins of tables like Table 3.4. 
If XandYhave a continuous joint distribution for which the joint p.d.f. is f, then 
the marginal p.d.f. f1of Xis again determined in the manner shown in Eq. (3.5.1), but 
132 Chapter 3 Random Variables and Distributions 
Table 3.4 Joint p.f. f(x,y) with marginal
p.f.’s for Example 3.5.2 
y
x 1 2 3 4 Total
1 0.1 0 0.1 0 0.2 
2 0.3 0 0.1 0.2 0.6 
3 0 0.2 0 0 0.2 
Total 0.4 0.2 0.2 0.2 1.0 
the sum over all possible values of Yis now replaced by the integral over all possible 
values of Y.
Theorem
3.5.2If XandYhave a continuous joint distribution with joint p.d.f. f, then the marginal 
p.d.f. f1of Xis 
f1(x) =/integraldisplay∞
−∞f(x,y)dy for−∞<x< ∞. (3.5.2)
Similarly, the marginal p.d.f. f2of Yis 
f2(y) =/integraldisplay∞
−∞f(x,y)dx for−∞<y< ∞. (3.5.3)
Proof We prove (3.5.2) as the proof of (3.5.3) is similar. For each x, Pr (X ≤x) can be 
written as Pr ((X,Y) ∈C) , where C={(r,s)) :r≤x}. We can compute this probability 
directly from the joint p.d.f. of XandYas 
Pr ((X,Y) ∈C) =/integraldisplayx
−∞/integraldisplay∞
−∞f(r,s)dsdr 
=/integraldisplayx
−∞/bracketleftbigg/integraldisplay∞
−∞f(r,s)ds /bracketrightbigg
dr (3.5.4)
The inner integral in the last expression of Eq. (3.5.4) is a function of rand it 
can easily be recognized as f1(r) , where f1is deﬁned in Eq. (3.5.2). It follows that 
Pr (X ≤x) =/integraltextx
−∞f1(r)dr , so f1is the marginal p.d.f. of X.
Example 
3.5.3Deriving a Marginal p.d.f . Suppose that the joint p.d.f. of XandYis as speciﬁed in 
Example 3.4.8, namely,
f(x,y) =/braceleftBigg
21 
4x2yforx2≤y≤1, 
0 otherwise. 
The set Sof points (x,y) for whichf(x,y)> 0 is sketched in Fig. 3.16. We shall 
determine ﬁrst the marginal p.d.f. f1of Xand then the marginal p.d.f. f2of Y.
It can be seen from Fig. 3.16 that Xcannot take any value outside the interval
[−1,1]. Therefore, f1(x) =0 for x< −1 or x> 1. Furthermore, for −1≤x≤1, it is 
seen from Fig. 3.16 that f(x,y) =0 unless x2≤y≤1. Therefore, for −1≤x≤1, 
f1(x) =/integraldisplay∞
−∞f(x,y)dy =/integraldisplay1
x2/parenleftbigg21 
4/parenrightbigg
x2ydy =/parenleftbigg21 
8/parenrightbigg
x2(1−x4).
3.5 Marginal Distributions 133
Figure3.16 The set Swhere
f(x,y)> 0 in Example 3.5.3. 
xy
1 /H110021S(1, 1) (/H110021, 1)y/H11005x2
Figure 3.17 The marginal 
p.d.f. of Xin Example 3.5.3. 
x11
/H110021f1(x)
Figure 3.18 The marginal 
p.d.f. of Yin Example 3.5.3. 
x10f2(y)
This marginal p.d.f. of Xis sketched in Fig. 3.17. 
Next, it can be seen from Fig. 3.16 that Ycannot take any value outside the
interval [0 ,1]. Therefore, f2(y) =0 for y< 0 or y> 1. Furthermore, for 0 ≤y≤1, it 
is seen from Fig. 3.12 that f(x,y) =0 unless −√y≤x≤√y. Therefore, for 0 ≤y≤1, 
f2(y) =/integraldisplay∞
−∞f(x,y)dx =/integraldisplay√y
−√y/parenleftbigg21 
4/parenrightbigg
x2ydx =/parenleftbigg7
2/parenrightbigg
y5/2.
This marginal p.d.f. of Yis sketched in Fig. 3.18. ◭
If Xhas a discrete distribution and Yhas a continuous distribution, we can derive 
the marginal p.f. of Xand the marginal p.d.f. of Yfrom the joint p.f./p.d.f. in the same 
ways that we derived a marginal p.f. or a marginal p.d.f. from a joint p.f. or a joint 
p.d.f. The following result can be proven by combining the techniques used in the 
proofs of Theorems 3.5.1 and 3.5.2. 
Theorem
3.5.3Letfbe the joint p.f./p.d.f. of XandY, with Xdiscrete and Ycontinuous. Then the 
marginal p.f. of Xis 
f1(x) =Pr (X =x) =/integraldisplay∞
−∞f(x,y)dy, for allx,
134 Chapter 3 Random Variables and Distributions 
and the marginal p.d.f. of Yis 
f2(y) =/summationdisplay
xf(x,y), for−∞<y< ∞.
Example 
3.5.4Determining a Marginal p.f. and Marginal p.d.f. from a Joint p.f./p.d.f . Suppose that the 
joint p.f./p.d.f. of XandYis as in Example 3.4.11 on page 124. The marginal p.f. of X
is obtained by integrating 
f1(x) =/integraldisplay1
0xy x−1
3dy =1
3,
forx=1,2,3. The marginal p.d.f. of Yis obtained by summing 
f2(y) =1
3+2y
3+y2,for 0<y< 1. ◭
Although the marginal distributions of XandYcan be derived from their 
joint distribution, it is not possible to reconstruct the joint distribution of Xand
Yfrom their marginal distributions without additional information. For instance, 
the marginal p.d.f.’s sketched in Figs. 3.17 and 3.18 reveal no information about the 
relationship between XandY. In fact, by deﬁnition, the marginal distribution of 
Xspeciﬁes probabilities for Xwithout regard for the values of any other random 
variables. This property of a marginal p.d.f. can be further illustrated by another 
example.
Example 
3.5.5Marginal and Joint Distributions . Suppose that a penny and a nickel are each tossed n
times so that every pair of sequences of tosses ( ntosses in each sequence) is equally 
likely to occur. Consider the following two deﬁnitions of XandY: (i) Xis the number 
of heads obtained with the penny, and Yis the number of heads obtained with the 
nickel. (ii) Both XandYare the number of heads obtained with the penny, so the 
random variables XandYare actually identical.
In case (i), the marginal distribution of Xand the marginal distribution of Ywill
be identical binomial distributions. The same pair of marginal distributions of Xand
Ywill also be obtained in case (ii). However, the joint distribution of XandYwill
not be the same in the two cases. In case (i), XandYcan take different values. Their 
joint p.f. is 
f(x,y) =/braceleftBigg/parenleftbign
x/parenrightbig/parenleftbig n
y/parenrightbig/parenleftBig
1
2/parenrightBigx+y
forx=0,1...,n ,y=0,1,...,n ,
0 otherwise. 
In case (ii), XandYmust take the same value, and their joint p.f. is 
f(x,y) =/braceleftBigg/parenleftbign
x/parenrightbig/parenleftBig
1
2/parenrightBigx
forx=y=0,1...,n ,
0 otherwise. ◭
Independent Random Variables 
Example 
3.5.6Demands for Utilities . In Examples 3.4.15 and 3.5.1, we found the marginal c.d.f.’s of 
water and electric demand were, respectively, 
F1(x) =

0 for x< 4, 
x
196for 4≤x≤200, 
1 for x> 200, F2(y) =

0 for y< 1, 
y
149for 1≤y≤150, 
1 for y> 150. 
3.5 Marginal Distributions 135
The product of these two functions is precisely the same as the joint c.d.f. of Xand
Ygiven in Example 3.5.1. One consequence of this fact is that, for every xand
y, Pr (X ≤x,andY≤y) =Pr (X ≤x) Pr (Y ≤y) . This equation makes XandYan 
example of the next deﬁnition. ◭
Deﬁnition
3.5.2Independent Random Variables . It is said that two random variables XandYare
independent if, for every two sets AandBof real numbers such that {X∈A}and
{Y∈B}are events,
Pr (X ∈AandY∈B) =Pr (X ∈A) Pr (Y ∈B). (3.5.5)
In other words, let Ebe any event the occurrence or nonoccurrence of which depends 
only on the value of X(such as E={X∈A}), and let Dbe any event the occurrence or 
nonoccurrence of which depends only on the value of Y(such as D={Y∈B}). Then 
XandYare independent random variables if and only if EandDare independent
events for all such events EandD.
If XandYare independent, then for all real numbers xandy, it must be true 
that
Pr (X ≤xandY≤y) =Pr (X ≤x) Pr (Y ≤y). (3.5.6)
Moreover, since all probabilities for XandYof the type appearing in Eq. (3.5.5) can 
be derived from probabilities of the type appearing in Eq. (3.5.6), it can be shown that 
if Eq. (3.5.6) is satisﬁed for all values of xandy, then XandYmust be independent. 
The proof of this statement is beyond the scope of this book and is omitted, but we 
summarize it as the following theorem. 
Theorem
3.5.4Let the joint c.d.f. of XandYbe F, let the marginal c.d.f. of Xbe F1, and let the 
marginal c.d.f. of Ybe F2. Then XandYare independent if and only if, for all real 
numbersxandy,F(x,y) =F1(x)F 2(y) .
For example, the demands for water and electricity in Example 3.5.6 are independent. 
If one returns to Example 3.5.1, one also sees that the product of the marginal p.d.f.’s 
of water and electric demand equals their joint p.d.f. given in Eq. (3.4.2). This relation 
is characteristic of independent random variables whether discrete or continuous. 
Theorem
3.5.5Suppose that XandYare random variables that have a joint p.f., p.d.f., or p.f./p.d.f. f.
Then XandYwill be independent if and only if fcan be represented in the following 
form for−∞<x< ∞and−∞<y< ∞:
f(x,y) =h1(x)h 2(y), (3.5.7)
whereh1is a nonnegative function of xalone andh2is a nonnegative function of y
alone.
Proof We shall give the proof only for the case in which Xis discrete and Yis 
continuous. The other cases are similar. For the “if” part, assume that Eq. (3.5.7) 
holds. Write 
f1(x) =/integraldisplay∞
−∞h1(x)h 2(y)dy =c1h1(x),
wherec1=/integraltext∞
−∞h2(y)dy must be ﬁnite and strictly positive, otherwise f1wouldn’t be 
a p.f. So, h1(x) =f1(x)/c 1. Similarly, 
f2(y) =/summationdisplay
xh1(x)h 2(y) =h2(y) /summationdisplay
x1
c1f1(x) =1
c1h2(y).
136 Chapter 3 Random Variables and Distributions 
So, h2(y) =c1f2(y) . Since f(x,y) =h1(x)h 2(y) , it follows that 
f(x,y) =f1(x) 
c1c1f2(y) =f1(x)f 2(y). (3.5.8)
Now letAandBbe sets of real numbers. Assuming the integrals exist, we can write 
Pr (X ∈AandY∈B) =/summationdisplay
x∈A/integraldisplay
Bf(x,y)dy 
=/integraldisplay
B/summationdisplay
x∈Af1(x)f 2(y)dy,
=/summationdisplay
x∈Af1(x) /integraldisplay
Bf2(y)dy,
where the ﬁrst equality is from Deﬁnition 3.4.5, the second is from Eq. (3.5.8), and the 
third is straightforward rearrangement. We now see that XandYare independent
according to Deﬁnition 3.5.2. 
For the “only if” part, assume that XandYare independent. Let AandBbe sets 
of real numbers. Let f1be the marginal p.d.f. of X, and let f2be the marginal p.f. of 
Y. Then 
Pr (X ∈AandY∈B) =/summationdisplay
x∈Af1(x) /integraldisplay
Bf2(y)dy 
=/integraldisplay
B/summationdisplay
x∈Af1(x)f 2(y)dy,
(if the integral exists) where the ﬁrst equality follows from Deﬁnition 3.5.2 and the
second is a straightforward rearrangement. We now see that f1(x)f 2(y) satisﬁes the
conditions needed to be f(x,y) as stated in Deﬁnition 3.4.5. 
A simple corollary follows from Theorem 3.5.5. 
Corollary 
3.5.1Two random variables XandYare independent if and only if the following factor- 
ization is satisﬁed for all real numbers xandy:
f(x,y) =f1(x)f 2(y). (3.5.9)
As stated in Sec. 3.2 (see page 102), in a continuous distribution the values of a 
p.d.f. can be changed arbitrarily at any countable set of points. Therefore, for such a 
distribution it would be more precise to state that the random variables XandYare
independent if and only if it is possible to choose versions of f,f1, and f2such that
Eq. (3.5.9) is satisﬁed for −∞<x< ∞and−∞<y< ∞.
The Meaning of Independence We have given a mathematical deﬁnition of in- 
dependent random variables in Deﬁnition 3.5.2, but we have not yet given any in- 
terpretation of the concept of independent random variables. Because of the close 
connection between independent events and independent random variables, the in- 
terpretation of independent random variables should be closely related to the inter- 
pretation of independent events. We model two events as independent if learning 
that one of them occurs does not change the probability that the other one occurs. 
It is easiest to extend this idea to discrete random variables. Suppose that XandY
3.5 Marginal Distributions 137
Table 3.5 Joint p.f. f(x,y) for Example 3.5.7
y
x 1 2 3 4 5 6 Total
0 1/24 1/24 1/24 1/24 1/24 1/24 1/4 
1 1/12 1/12 1/12 1/12 1/12 1/12 1/2 
2 1/24 1/24 1/24 1/24 1/24 1/24 1/4 
Total 1/6 1/6 1/6 1/6 1/6 1/6 1.000 
have a discrete joint distribution. If, for each y, learning that Y=ydoes not change
any of the probabilities of the events {X=x}, we would like to say that XandYare
independent. From Corollary 3.5.1 and the deﬁnition of marginal p.f., we see that in- 
deedXandYare independent if and only if, for each yandxsuch that Pr (Y =y)> 0, 
Pr (X =x|Y=y) =Pr (X =x) , that is, learning the value of Ydoesn’t change any of 
the probabilities associated with X. When we formally deﬁne conditional distribu- 
tions in Sec. 3.6, we shall see that this interpretation of independent discrete random 
variables extends to all bivariate distributions. In summary, if we are trying to decide 
whether or not to model two random variables XandYas independent, we should 
think about whether we would change the distribution of Xafter we learned the value 
of Yor vice versa. 
Example 
3.5.7Games of Chance . A carnival game consists of rolling a fair die, tossing a fair coin 
two times, and recording both outcomes. Let Ystand for the number on the die, 
and letXstand for the number of heads in the two tosses. It seems reasonable to 
believe that all of the events determined by the roll of the die are independent of all 
of the events determined by the ﬂips of the coin. Hence, we can assume that XandY
are independent random variables. The marginal distribution of Yis the uniform 
distribution on the integers 1 ,..., 6, while the distribution of Xis the binomial 
distribution with parameters 2 and 1 /2. The marginal p.f.’s and the joint p.f. of X
andYare given in Table 3.5, where the joint p.f. was constructed using Eq. (3.5.9). 
The Total column gives the marginal p.f. f1of X, and the Total row gives the marginal 
p.f. f2of Y. ◭
Example 
3.5.8Determining Whether Random Variables Are Independent in a Cl inical Trial . Return to 
the clinical trial of depression drugs in Exercise 11 of Sec. 3.4 (on page 129). In that 
trial, a patient is selected at random from the 150 patients in the study and we record 
Y, an indicator of the treatment group for that patient, and X, an indicator of whether 
or not the patient relapsed. Table 3.6 repeats the joint p.f. of XandYalong with the
marginal distributions in the margins. We shall determine whether or not XandY
are independent.
In Eq. (3.5.9), f(x,y) is the probability in the xth row and the yth column of the 
table,f1(x) is the number in the Total column in the xth row, and f2(y) is the number 
in the Total row in the yth column. It is seen in the table that f( 1,2)=0.087, while 
f1(1)=0.513, and f2(1)=0.253. Hence, f( 1,2)′negationslash=f1(1)f 2(1)=0.129. It follows that 
XandYare not independent. ◭
It should be noted from Examples 3.5.7 and 3.5.8 that XandYwill be indepen- 
dent if and only if the rows of the table specifying their joint p.f. are proportional to 
138 Chapter 3 Random Variables and Distributions 
Table 3.6 Proportions marginals in Example 3.5.8 
Treatmentgroup ( Y)
Response (X) Imipramine (1) Lithium (2) Combination (3) Placebo (4) Total 
Relapse (0) 0.120 0.087 0.146 0.160 0.513
No relapse (1) 0.147 0.166 0.107 0.067 0.487 
Total 0.267 0.253 0.253 0.227 1.0 
one another, or equivalently, if and only if the columns of the table are proportional 
to one another. 
Example 
3.5.9Calculating a Probability Involving Independent Random Vari ables . Suppose that two 
measurements XandYare made of the rainfall at a certain location on May 1 in two 
consecutive years. It might be reasonable, given knowledge of the history of rainfall 
on May 1, to treat the random variables XandYas independent. Suppose that the 
p.d.f. gof each measurement is as follows: 
g(x) =/braceleftbigg2xfor 0≤x≤1, 
0 otherwise. 
We shall determine the value of Pr (X +Y≤1).
SinceXandYare independent and each has the p.d.f. g, it follows from Eq. (3.5.9) 
that for all values of xandythe joint p.d.f. f(x,y) of XandYwill be speciﬁed by 
the relation f(x,y) =g(x)g(y) . Hence, 
f(x,y) =/braceleftbigg4xy for 0≤x≤1 and 0 ≤y≤1, 
0 otherwise. 
The set Sin the xy -plane, where f(x,y)> 0, and the subset S0, where x+y≤1, are 
sketched in Fig. 3.19. Thus, 
Pr (X +Y≤1)=/integraldisplay
S0/integraldisplay
f(x,y)dxdy =/integraldisplay1
0/integraldisplay1−x
04xydydx =1
6.
As a ﬁnal note, if the two measurements XandYhad been made on the same day at 
nearby locations, then it might not make as much sense to treat them as independent, 
since we would expect them to be more similar to each other than to historical 
rainfalls. For example, if we ﬁrst learn that Xis small compared to historical rainfall 
on the date in question, we might then expect Yto be smaller than the historical 
distribution would suggest. ◭
Figure 3.19 The subset S0
wherex+y≤1
in Example 3.5.9. 
x11
0S
S0y
3.5 Marginal Distributions 139
Theorem 3.5.5 says that XandYare independent if and only if, for all values of 
xandy,fcan be factored into the product of an arbitrary nonnegative function of x
and an arbitrary nonnegative function of y. However, it should be emphasized that, 
just as in Eq. (3.5.9), the factorization in Eq. (3.5.7) must be satisﬁed for all values of 
xandy(−∞<x< ∞and−∞<y< ∞). 
Example 
3.5.10 Dependent Random Variables . Suppose that the joint p.d.f. of XandYhas the follow-
ing form:
f(x,y) =/braceleftbigg
kx 2y2forx2+y2≤1, 
0 otherwise. 
We shall show that XandYare not independent.
It is evident that at each point inside the circle x2+y2≤1, f(x,y) can be factored 
as in Eq. (3.5.7). However, this same factorization cannot also be satisﬁed at every 
point outside this circle. For example, f( 0.9,0.9)=0, but neither f1(0.9)=0 nor 
f2(0.9)=0. (In Exercise 13, you can verify this feature of f1andf2.) 
The important feature of this example is that the values of XandYare con-
strained to lie inside a circle. The joint p.d.f. of XandYis positive inside the circle 
and zero outside the circle. Under these conditions, XandYcannot be independent, 
because for every given value yof Y, the possible values of Xwill depend on y. For 
example, if Y=0, then Xcan have any value such that X2≤1; if Y=1/2, then X
must have a value such that X2≤3/4. ◭
Example 3.5.10 shows that one must be careful when trying to apply Theo- 
rem 3.5.5. The situation that arose in that example will occur whenever {(x,y) :
f(x,y)> 0}has boundaries that are curved or not parallel to the coordinate axes. 
There is one important special case in which it is easy to check the conditions of 
Theorem 3.5.5. The proof is left as an exercise. 
Theorem
3.5.6LetXandYhave a continuous joint distribution. Suppose that {(x,y) :f(x,y)> 0}
is a rectangular region R(possibly unbounded) with sides (if any) parallel to the 
coordinate axes. Then XandYare independent if and only if Eq. (3.5.7) holds for 
all(x,y) ∈R.
Example 
3.5.11 Verifying the Factorization of a Joint p.d.f . Suppose that the joint p.d.f. fof XandYis 
as follows: 
f(x,y) =/braceleftbiggke −(x +2y) forx≥0 and y≥0, 
0 otherwise, 
wherekis some constant. We shall ﬁrst determine whether XandYare independent
and then determine their marginal p.d.f.’s. 
In this example, f(x,y) =0 outside of an unbounded rectangular region Rwhose
sides are the lines x=0 and y=0. Furthermore, at each point inside R,f(x,y) can
be factored as in Eq. (3.5.7) by letting h1(x) =ke −xandh2(y) =e−2y. Therefore, X
andYare independent.
It follows that in this case, except for constant factors, h1(x) forx≥0 and h2(y) 
fory≥0 must be the marginal p.d.f.’s of XandY. By choosing constants that make 
h1(x) andh2(y) integrate to unity, we can conclude that the marginal p.d.f.’s f1and
f2of XandYmust be as follows: 
f1(x) =/braceleftbigge−xforx≥0, 
0 otherwise, 
140 Chapter 3 Random Variables and Distributions 
and
f2(y) =/braceleftbigg
2e−2yfory≥0, 
0 otherwise. 
If we multiply f1(x) timesf2(y) and compare the product to f(x,y) , we see that 
k=2. ◭
Note:SeparateFunctionsofIndependentRandomVariablesAr eIndependent. If 
XandYare independent, then h(X) andg(Y) are independent no matter what the 
functionshandgare. This is true because for every t, the event {h(X)≤t}can always
be written as {X∈A}, where A={x:h(x)≤t}. Similarly, {g(Y) ≤u}can be written 
as {Y∈B}, so Eq. (3.5.6) for h(X) andg(Y) follows from Eq. (3.5.5) for XandY.
Summary
Letf(x,y) be a joint p.f., joint p.d.f., or joint p.f./p.d.f. of two random variables X
andY. The marginal p.f. or p.d.f. of Xis denoted by f1(x) , and the marginal p.f. or 
p.d.f. of Yis denoted by f2(y) . To obtain f1(x) , compute /summationtext
yf(x,y) if Yis discrete 
or /integraltext∞
−∞f(x,y)dy if Yis continuous. Similarly, to obtain f2(y) , compute /summationtext
xf(x,y) 
if Xis discrete or /integraltext∞
−∞f(x,y)dx if Xis continuous. The random variables Xand
Yare independent if and only if f(x,y) =f1(x)f 2(y) forallxandy. This is true 
regardless of whether Xand/orYis continuous or discrete. A sufﬁcient condition for 
two continuous random variables to be independent is that R={(x,y) :f(x,y)> 0}
be rectangular with sides parallel to the coordinate axes and that f(x,y) factors into
separate functions of xof yin R.
Exercises 
1. Suppose that XandYhave a continuous joint distribu- 
tion for which the joint p.d.f. is 
f(x,y) =/braceleftbiggkfora≤x≤bandc≤y≤d,
0 otherwise, 
wherea<b ,c<d , and k> 0. Find the marginal distribu- 
tions of XandY.
2. Suppose that XandYhave a discrete joint distribution 
for which the joint p.f. is deﬁned as follows: 
f(x,y) =/braceleftBigg
1
30 (x +y) forx=0,1,2 and y=0,1,2,3, 
0 otherwise. 
a. Determine the marginal p.f.’s of XandY.
b. AreXandYindependent?
3. Suppose that XandYhave a continuous joint distribu- 
tion for which the joint p.d.f. is deﬁned as follows: 
f(x,y) =/braceleftbigg3
2y2for 0≤x≤2 and 0 ≤y≤1, 
0 otherwise. a. Determine the marginal p.d.f.’s of XandY.
b. AreXandYindependent?
c. Are the event {X< 1}and the event {Y≥1/2}inde-
pendent?
4. Suppose that the joint p.d.f. of XandYis as follows: 
f(x,y) =/braceleftbigg15 
4x2for 0≤y≤1−x2,
0 otherwise. 
a. Determine the marginal p.d.f.’s of XandY.
b. AreXandYindependent?
5. A certain drugstore has three public telephone booths. 
For i=0, 1, 2, 3, let pidenote the probability that ex-
actlyitelephone booths will be occupied on any Monday 
evening at 8:00 p.m. ; and suppose that p0=0.1, p1=0.2, 
p2=0.4, and p3=0.3. Let XandYdenote the number of 
booths that will be occupied at 8:00 p.m. on two indepen- 
dent Monday evenings. Determine: (a)the joint p.f. of X
andY;(b)Pr (X =Y) ;(c)Pr (X>Y) .
3.6 Conditional Distributions 141
6. Suppose that in a certain drug the concentration of a 
particular chemical is a random variable with a continuous 
distribution for which the p.d.f. gis as follows: 
g(x) =/braceleftbigg3
8x2for 0≤x≤2, 
0 otherwise. 
Suppose that the concentrations XandYof the chemical 
in two separate batches of the drug are independent ran- 
dom variables for each of which the p.d.f. is g. Determine 
(a)the joint p.d.f. of XandY;(b)Pr (X =Y) ;(c)Pr (X>Y) ;
(d)Pr (X +Y≤1).
7. Suppose that the joint p.d.f. of XandYis as follows: 
f(x,y) =/braceleftbigg
2xe −yfor 0≤x≤1 and 0 <y< ∞,
0 otherwise. 
AreXandYindependent?
8. Suppose that the joint p.d.f. of XandYis as follows: 
f(x,y) =/braceleftbigg
24 xy forx≥0, y≥0, and x+y≤1, 
0 otherwise. 
AreXandYindependent?
9. Suppose that a point ( X,Y) is chosen at random from 
the rectangle Sdeﬁned as follows: 
S={(x,y) : 0 ≤x≤2 and 1 ≤y≤4}.
a. Determine the joint p.d.f. of XandY, the marginal 
p.d.f. of X, and the marginal p.d.f. of Y.
b. AreXandYindependent?
10.Suppose that a point ( X,Y) is chosen at random from 
the circleSdeﬁned as follows: 
S={(x,y) :x2+y2≤1}.
a. Determine the joint p.d.f. of XandY, the marginal 
p.d.f. of X, and the marginal p.d.f. of Y.
b. AreXandYindependent?11.Suppose that two persons make an appointment to 
meet between 5 p.m. and 6 p.m. at a certain location, and 
they agree that neither person will wait more than 10 
minutes for the other person. If they arrive independently 
at random times between 5 p.m. and 6 p.m. , what is the 
probability that they will meet?
12.Prove Theorem 3.5.6. 
13.In Example 3.5.10, verify that XandYhave the same
marginal p.d.f.’s and that 
f1(x) =/braceleftbigg
2kx 2(1−x2)3/2/3 if −1≤x≤1, 
0 otherwise. 
14.For the joint p.d.f. in Example 3.4.7, determine 
whether or not XandYare independent.
15.A painting process consists of two stages. In the ﬁrst 
stage, the paint is applied, and in the second stage, a pro- 
tective coat is added. Let Xbe the time spent on the ﬁrst 
stage, and let Ybe the time spent on the second stage. The 
ﬁrst stage involves an inspection. If the paint fails the in- 
spection, one must wait three minutes and apply the paint
again. After a second application, there is no further in- 
spection. The joint p.d.f. of XandYis 
f(x,y) =

1
3if 1 <x< 3 and 0 <y< 1, 
1
6if 6 <x< 8 and 0 <y< 1, 
0 otherwise. 
a. Sketch the region where f(x,y)> 0. Note that it is 
not exactly a rectangle. 
b. Find the marginal p.d.f.’s of XandY.
c. Show thatXandYare independent.
This problem does not contradict Theorem 3.5.6. In that 
theorem the conditions, including that the set where 
f(x,y)> 0 be rectangular, are sufﬁcient but not neces- 
sary.
3.6 Conditional Distributions 
Wegeneralizetheconceptofconditionalprobabilitytoconditionaldistributions.
Recallthatdistributionsarejustcollectionsofprobabilitiesofeventsdetermined 
byrandomvariables.Conditionaldistributionswillbetheprobabilitiesofevents 
determinedbysomerandomvariablesconditionaloneventsdeterminedbyother 
randomvariables.Theideaisthattherewilltypicallybemanyrandomvariablesof
interestinanappliedproblem.Afterweobservesomeofthoserandomvariables,
wewanttobeabletoadjusttheprobabilitiesassociatedwiththeonesthathavenot
yetbeenobserved.Theconditionaldistributionofonerandomvariable Xgiven
anotherYwillbethedistributionthatwewouldusefor Xafterwelearnthevalue 
ofY.
142 Chapter 3 Random Variables and Distributions 
Table 3.7 Joint p.f. for Example 3.6.1 
BrandY
StolenX 1 2 3 4 5 Total
0 0.129 0.298 0.161 0.280 0.108 0.976 
1 0.010 0.010 0.001 0.002 0.001 0.024 
Total 0.139 0.308 0.162 0.282 0.109 1.000 
Discrete Conditional Distributions 
Example 
3.6.1Auto Insurance . Insurance companies keep track of how likely various cars are to be 
stolen. Suppose that a company in a particular area computes the joint distribution 
of car brands and the indicator of whether the car will be stolen during a particular 
year that appears in Table 3.7. 
We let X=1 mean that a car is stolen, and we let X=0 mean that the car is not 
stolen. We let Ytake one of the values from 1 to 5 to indicate the brand of car as 
indicated in Table 3.7. If a customer applies for insurance for a particular brand of 
car, the company needs to compute the distribution of the random variable Xas part 
of its premium determination. The insurance company might adjust their premium 
according to a risk factor such as likelihood of being stolen. Although, overall, the 
probability that a car will be stolen is 0.024, if we assume that we know the brand 
of car, the probability might change quite a bit. This section introduces the formal 
concepts for addressing this type of problem. ◭
Suppose that XandYare two random variables having a discrete joint distribu- 
tion for which the joint p.f. is f. As before, we shall let f1andf2denote the marginal
p.f.’s of XandY, respectively. After we observe that Y=y, the probability that the 
random variable Xwill take a particular value xis speciﬁed by the following condi- 
tional probability:
Pr (X =x|Y=y) =Pr (X =xandY=y) 
Pr (Y =y) 
=f(x,y) 
f2(y) . (3.6.1)
In other words, if it is known that Y=y, then the probability that X=xwill be 
updated to the value in Eq. (3.6.1). Next, we consider the entire distribution of X
after learning that Y=y.
Deﬁnition
3.6.1Conditional Distribution/p.f . Let XandYhave a discrete joint distribution with joint 
p.f. f. Let f2denote the marginal p.f. of Y. For each ysuch thatf2(y)> 0, deﬁne 
g1(x |y) =f(x,y) 
f2(y) . (3.6.2)
Then g1is called the conditionalp.f.of XgivenY. The discrete distribution whose p.f. 
is g1(.|y) is called the conditionaldistributionof XgiventhatY=y.
3.6 Conditional Distributions 143
Table 3.8 Conditional p.f. of YgivenXfor Exam-
ple 3.6.3
BrandY
StolenX 1 2 3 4 5 
0 0.928 0.968 0.994 0.993 0.991 
1 0.072 0.032 0.006 0.007 0.009 
We should verify that g1(x |y) is actually a p.f. as a function of xfor eachy. Let ybe 
such thatf2(y)> 0. Then g1(x |y) ≥0 for all xand
/summationdisplay
xg1(x |y) =1
f2(y) /summationdisplay
xf(x,y) =1
f2(y) f2(y) =1.
Notice that we do not bother to deﬁne g1(x |y) for thoseysuch thatf2(y) =0. 
Similarly, if xis a given value of Xsuch thatf1(x) =Pr (X =x)> 0, and if g2(y |x) 
is the conditionalp.f.of YgiventhatX=x, then 
g2(y |x) =f(x,y) 
f1(x) . (3.6.3)
For each xsuch thatf1(x)> 0, the function g2(y |x) will be a p.f. as a function of y.
Example 
3.6.2Calculating a Conditional p.f. from a Joint p.f . Suppose that the joint p.f. of XandYis 
as speciﬁed in Table 3.4 in Example 3.5.2. We shall determine the conditional p.f. of 
Ygiven thatX=2. 
The marginal p.f. of Xappears in the Total column of Table 3.4, so f1(2)=Pr (X =
2)=0.6. Therefore, the conditional probability g2(y |2)thatYwill take a particular 
valueyis 
g2(y |2)=f( 2,y) 
0.6.
It should be noted that for all possible values of y, the conditional probabilities 
g2(y |2)must be proportional to the joint probabilities f( 2,y) . In this example, each 
value of f( 2,y) is simply divided by the constant f1(2)=0.6 in order that the sum of 
the results will be equal to 1. Thus, 
g2(1|2)=1/2, g 2(2|2)=0, g 2(3|2)=1/6, g 2(4|2)=1/3. ◭
Example 
3.6.3Auto Insurance . Consider again the probabilities of car brands and cars being stolen 
in Example 3.6.1. The conditional distribution of X(being stolen) given Y(brand)
is given in Table 3.8. It appears that Brand 1 is much more likely to be stolen than 
other cars in this area, with Brand 1 also having a signiﬁcant chance of being stolen. 
◭
Continuous Conditional Distributions 
Example 
3.6.4Processing Times . A manufacturing process consists of two stages. The ﬁrst stage 
takesYminutes, and the whole process takes Xminutes (which includes the ﬁrst
144 Chapter 3 Random Variables and Distributions 
Yminutes). Suppose that XandYhave a joint continuous distribution with joint 
p.d.f. 
f(x,y) =/braceleftbigg
e−xfor 0≤y≤x< ∞,
0 otherwise. 
After we learn how much time Ythat the ﬁrst stage takes, we want to update our 
distribution for the total time X. In other words, we would like to be able to compute 
a conditional distribution for XgivenY=y. We cannot argue the same way as we 
did with discrete joint distributions, because {Y=y}is an event with probability 0 
for ally. ◭
To facilitate the solutions of problems such as the one posed in Example 3.6.4, 
the concept of conditional probability will be extended by considering the deﬁnition 
of the conditional p.f. of Xgiven in Eq. (3.6.2) and the analogy between a p.f. and a 
p.d.f. 
Deﬁnition
3.6.2Conditional p.d.f . Let XandYhave a continuous joint distribution with joint p.d.f. 
fand respective marginals f1andf2. Let ybe a value such that f2(y)> 0. Then the 
conditionalp.d.f. g1ofXgiventhatY=yis deﬁned as follows: 
g1(x |y) =f(x,y) 
f2(y) for−∞<x< ∞. (3.6.4)
For values of ysuch thatf2(y) =0, we are free to deﬁne g1(x |y) however we wish, 
so long as g1(x |y) is a p.d.f. as a function of x.
It should be noted that Eq. (3.6.2) and Eq. (3.6.4) are identical. However, 
Eq. (3.6.2) was derived as the conditional probability that X=xgiven thatY=y,
whereas Eq. (3.6.4) was deﬁned to be the value of the conditional p.d.f. of Xgiven
thatY=y. In fact, we should verify that g1(x |y) as deﬁned above really is a p.d.f. 
Theorem
3.6.1For each y,g1(x |y) deﬁned in Deﬁnition 3.6.2 is a p.d.f. as a function of x.
Proof If f2(y) =0, then g1is deﬁned to be any p.d.f. we wish, and hence it is a p.d.f. 
If f2(y)> 0, g1is deﬁned by Eq. (3.6.4). For each such y, it is clear that g1(x |y) ≥0
for allx. Also, if f2(y)> 0, then 
/integraldisplay∞
−∞g1(x |y)dx =/integraltext∞
−∞f(x,y)dx 
f2(y) =f2(y) 
f2(y) =1,
by using the formula for f2(y) in Eq. (3.5.3). 
Example 
3.6.5Processing Times . In Example 3.6.4, Yis the time that the ﬁrst stage of a process takes, 
whileXis the total time of the two stages. We want to calculate the conditional p.d.f. 
of XgivenY. We can calculate the marginal p.d.f. of Yas follows: For each y, the 
possible values of Xare allx≥y, so for each y> 0, 
f2(y) =/integraldisplay∞
ye−xdx =e−y,
andf2(y) =0 for y< 0. For each y≥0, the conditional p.d.f. of XgivenY=yis then 
g1(x |y) =f(x,y) 
f2(y) =e−x
e−y=ey−x,forx≥y,
3.6 Conditional Distributions 145
Figure 3.20 The condi- 
tional p.d.f. g1(x |y0)is pro- 
portional to f(x,y 0).
x yf(x, y )
f(x, y 0)
y0
andg1(x |y) =0 for x<y . So, for example, if we observe Y=4 and we want the 
conditional probability that X≥9, we compute 
Pr (X ≥9|Y=4)=/integraldisplay∞
9e4−xdx =e−5=0.0067. ◭
Deﬁnition 3.6.2 has an interpretation that can be understood by considering 
Fig. 3.20. The joint p.d.f. fdeﬁnes a surface over the xy -plane for which the height
f(x,y) at each point (x,y) represents the relative likelihood of that point. For 
instance, if it is known that Y=y0, then the point ( x,y) must lie on the line y=y0in 
thexy -plane, and the relative likelihood of any point ( x,y0) on this line is f(x,y 0).
Hence, the conditional p.d.f. g1(x |y0)of Xshould be proportional to f(x,y 0). In other 
words,g1(x |y0)is essentially the same as f(x,y 0), but it includes a constant factor 
1/[f2(y 0)], which is required to make the conditional p.d.f. integrate to unity over all 
values of x.
Similarly, for each value of xsuch thatf1(x)> 0, the conditionalp.d.f.of Ygiven
thatX=xis deﬁned as follows: 
g2(y |x) =f(x,y) 
f1(x) for−∞<y< ∞. (3.6.5)
This equation is identical to Eq. (3.6.3), which was derived for discrete distributions. 
If f1(x) =0, then g2(y |x) is arbitrary so long as it is a p.d.f. as a function of y.
Example 
3.6.6Calculating a Conditional p.d.f. from a Joint p.d.f . Suppose that the joint p.d.f. of Xand
Yis as speciﬁed in Example 3.4.8 on page 122. We shall ﬁrst determine the conditional 
p.d.f. of Ygiven thatX=xand then determine some probabilities for Ygiven the
speciﬁc value X=1/2. 
The set Sfor whichf(x,y)> 0 was sketched in Fig. 3.12 on page 123. Further- 
more, the marginal p.d.f. f1was derived in Example 3.5.3 on page 132 and sketched 
in Fig. 3.17 on page 133. It can be seen from Fig. 3.17 that f1(x)> 0 for −1<x< 1 but 
not forx=0. Therefore, for each given value of xsuch that−1<x< 0 or 0 <x< 1, 
the conditional p.d.f. g2(y |x) of Ywill be as follows: 
g2(y |x) =

2y
1−x4forx2≤y≤1, 
0 otherwise. 
146 Chapter 3 Random Variables and Distributions 
In particular, if it is known that X=1/2, then Pr /parenleftBig
Y≥1
4/vextendsingle/vextendsingleX=1
2/parenrightBig
=1 and 
Pr /parenleftBigg
Y≥3
4/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleX=1
2/parenrightBigg
=/integraldisplay1
3/4g2/parenleftBigg
y/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
2/parenrightBigg
dy =7
15 . ◭
Note:AConditionalp.d.f.IsNottheResultofConditioning onaSetofProbability 
Zero. The conditional p.d.f. g1(x |y) of XgivenY=yis the p.d.f. we would use for 
Xif we were to learn that Y=y. This sounds as if we were conditioning on the event 
{Y=y}, which has zero probability if Yhas a continuous distribution. Actually, for 
the cases we shall see in this text, the value of g1(x |y) is a limit: 
g1(x |y) =lim
ǫ→0∂
∂x Pr (X ≤x|y−ǫ<Y ≤y+ǫ). (3.6.6)
The conditioning event {y−ǫ≤Y≤y+ǫ}in Eq. (3.6.6) has positive probability if 
the marginal p.d.f. of Yis positive at y. The mathematics required to make this rigor- 
ous is beyond the scope of this text. (See Exercise 11 in this section and Exercises 25 
and 26 in Sec. 3.11 for results that we can prove.) Another way to think about condi- 
tioning on a continuous random variable is to notice that the conditional p.d.f.’s that 
we compute are typically continuous as a function of the conditioning variable. This 
means that conditioning on Y=yor on Y=y+ǫfor smallǫwill produce nearly
the same conditional distribution for X. So it does not matter much if we use Y=y
as a surogate for Yclose to y. Nevertheless, it is important to keep in mind that the 
conditional p.d.f. of XgivenY=yis better thought of as the conditional p.d.f. of X
given thatYis very close to y. This wording is awkward, so we shall not use it, but 
we must remember the distinction between the conditional p.d.f. and conditioning 
on an event with probability 0. Despite this distinction, it is still legitimate to treat Y
as the constant ywhen dealing with the conditional distribution of XgivenY=y.
For mixed joint distributions, we continue to use Eqs. (3.6.2) and (3.6.3) to deﬁne 
conditional p.f.’s and p.d.f.’s. 
Deﬁnition
3.6.3Conditional p.f. or p.d.f. from Mixed Distribution . Let Xbe discrete and let Ybe 
continuous with joint p.f./p.d.f. f. Then the conditionalp.f.of XgivenY=yis deﬁned 
by Eq. (3.6.2), and the conditionalp.d.f.of YgivenX=xis deﬁned by Eq. (3.6.3). 
Construction of the Joint Distribution 
Example 
3.6.7Defective Parts . Suppose that a certain machine produces defective and nondefective 
parts, but we do not know what proportion of defectives we would ﬁnd among 
all parts that could be produced by this machine. Let Pstand for the unknown
proportion of defective parts among all possible parts produced by the machine. If we 
were to learn that P=p, we might be willing to say that the parts were independent 
of each other and each had probability pof being defective. In other words, if we 
condition on P=p, then we have the situation described in Example 3.1.9. As in 
that example, suppose that we examine nparts and let Xstand for the number of 
defectives among the nexamined parts. The distribution of X, assuming that we know 
P=p, is the binomial distribution with parameters nandp. That is, we can let the 
binomial p.f. (3.1.4) be the conditional p.f. of XgivenP=p, namely, 
g1(x |p) =/parenleftbiggn
x/parenrightbigg
px(1−p) n−x,forx=0,...,n .
3.6 Conditional Distributions 147
We might also believe that Phas a continuous distribution with p.d.f. such as f2(p) =1
for 0≤p≤1. (This means that Phas the uniform distribution on the interval [0 ,1].) 
We know that the conditional p.f. g1of XgivenP=psatisﬁes
g1(x |p) =f(x,p) 
f2(p) ,
wherefis the joint p.f./p.d.f. of XandP. If we multiply both sides of this equation 
by f2(p) , it follows that the joint p.f./p.d.f. of XandPis 
f(x,p) =g1(x |p)f 2(p) =/parenleftbiggn
x/parenrightbigg
px(1−p) n−x,forx=0,...,n, and 0≤p≤1.
◭
The construction in Example 3.6.7 is available in general, as we explain next. 
GeneralizingtheMultiplicationRuleforConditionalProbabilities A special case 
of Theorem 2.1.2, the multiplication rule for conditional probabilities, says that if 
AandBare two events, then Pr (A ∩B) =Pr (A)Pr (B |A) . The following theorem, 
whose proof is immediate from Eqs. (3.6.4) and (3.6.5), generalizes Theorem 2.1.2 to 
the case of two random variables. 
Theorem
3.6.2Multiplication Rule for Distributions . Let XandYbe random variables such that X
has p.f. or p.d.f. f1(x) andYhas p.f. or p.d.f. f2(y) . Also, assume that the conditional 
p.f. or p.d.f. of XgivenY=yis g1(x |y) while the conditional p.f. or p.d.f. of Ygiven
X=xis g2(y |x) . Then for each ysuch thatf2(y)> 0 and each x,
f(x,y) =g1(x |y)f 2(y), (3.6.7)
wherefis the joint p.f., p.d.f., or p.f./p.d.f. of XandY. Similarly, for each xsuch that
f1(x)> 0 and each y,
f(x,y) =f1(x)g 2(y |x). (3.6.8)
In Theorem 3.6.2, if f2(y 0)=0 for some value y0, then it can be assumed without 
loss of generality that f(x,y 0)=0 for all values of x. In this case, both sides of 
Eq. (3.6.7) will be 0, and the fact that g1(x |y0)is not uniquely deﬁned becomes 
irrelevant. Hence, Eq. (3.6.7) will be satisﬁed for allvalues of xandy. A similar 
statement applies to Eq. (3.6.8). 
Example 
3.6.8Waiting in a Queue . Let Xbe the amount of time that a person has to wait for service 
in a queue. The faster the server works in the queue, the shorter should be the 
waiting time. Let Ystand for the rate at which the server works, which we will take 
to be unknown. A common choice of conditional distribution for XgivenY=yhas
conditional p.d.f. for each y> 0: 
g1(x |y) =/braceleftbigg
ye −xy forx≥0, 
0 otherwise. 
We shall assume that Yhas a continuous distribution with p.d.f. f2(y) =e−yfory> 0. 
Now we can construct the joint p.d.f. of XandYusing Theorem 3.6.2: 
f(x,y) =g1(x |y)f 2(y) =/braceleftbigg
ye −y(x +1)forx≥0, y> 0, 
0 otherwise. ◭
148 Chapter 3 Random Variables and Distributions 
Example 
3.6.9Defective Parts . Let Xbe the number of defective parts in a sample of size n, and 
letPbe the proportion of defectives among all parts, as in Example 3.6.7. The joint 
p.f./p.d.f of XandP=pwas calculated there as 
f(x,p) =g1(x |p)f 2(p) =/parenleftbiggn
x/parenrightbigg
px(1−p) n−x,forx=0,...,n and 0≤p≤1.
We could now compute the conditional p.d.f. of PgivenX=xby ﬁrst ﬁnding the 
marginal p.f. of X:
f1(x) =/integraldisplay1
0/parenleftbiggn
x/parenrightbigg
px(1−p) n−xdp, (3.6.9)
The conditional p.d.f. of PgivenX=xis then 
g2(p |x) =f(x,p) 
f1(x) =px(1−p) n−x
/integraltext1
0qx(1−q) n−xdq ,for 0<p< 1. (3.6.10) 
The integral in the denominator of Eq. (3.6.10) can be tedious to calculate, but it can 
be found. For example, if n=2 and x=1, we get 
/integraldisplay1
0q( 1−q)dq =1
2−1
3=1
6.
In this case, g2(p |1)=6p( 1−p) for 0≤p≤1. ◭
Bayes’ Theorem and the Law of Total Probability for Random Varia bles The 
calculation done in Eq. (3.6.9) is an example of the generalization of the law of total 
probability to random variables. Also, the calculation in Eq. (3.6.10) is an example of 
the generalization of Bayes’ theorem to random variables. The proofs of these results 
are straightforward and not given here.
Theorem
3.6.3Law of Total Probability for Random Variables . If f2(y) is the marginal p.f. or p.d.f. of a 
random variable Yandg1(x |y) is the conditional p.f. or p.d.f. of XgivenY=y, then 
the marginal p.f. or p.d.f. of Xis 
f1(x) =/summationdisplay
yg1(x |y)f 2(y), (3.6.11)
if Yis discrete. If Yis continuous, the marginal p.f. or p.d.f. of Xis 
f1(x) =/integraldisplay∞
−∞g1(x |y)f 2(y)dy. (3.6.12)
There are versions of Eqs. (3.6.11) and (3.6.12) with xandyswitched and the
subscripts 1 and 2 switched. These versions would be used if the joint distribution 
of XandYwere constructed from the conditional distribution of YgivenXand the
marginal distribution of X.
Theorem
3.6.4Bayes’ Theorem for Random Variables . If f2(y) is the marginal p.f. or p.d.f. of a random 
variableYandg1(x |y) is the conditional p.f. or p.d.f. of XgivenY=y, then the 
conditional p.f. or p.d.f. of YgivenX=xis 
g2(y |x) =g1(x |y)f 2(y) 
f1(x) , (3.6.13)
3.6 Conditional Distributions 149
wheref1(x) is obtained from Eq. (3.6.11) or (3.6.12). Similarly, the conditional p.f. 
or p.d.f. of XgivenY=yis 
g1(x |y) =g2(y |x)f 1(x) 
f2(y) , (3.6.14)
wheref2(y) is obtained from Eq. (3.6.11) or (3.6.12) with xandyswitched and with
the subscripts 1 and 2 switched. 
Example 
3.6.10 Choosing Points from Uniform Distributions . Suppose that a point Xis chosen from 
the uniform distribution on the interval [0 ,1], and that after the value X=xhas been
observed(0<x< 1), a point Yis then chosen from the uniform distribution on the 
interval [x,1]. We shall derive the marginal p.d.f. of Y.
SinceXhas a uniform distribution, the marginal p.d.f. of Xis as follows: 
f1(x) =/braceleftbigg1 for 0 <x< 1, 
0 otherwise. 
Similarly, for each value X=x(0 <x< 1), the conditional distribution of Yis the 
uniform distribution on the interval [ x,1]. Since the length of this interval is 1 −x,
the conditional p.d.f. of Ygiven thatX=xwill be 
g2(y |x) =

1
1−xforx<y< 1, 
0 otherwise. 
It follows from Eq. (3.6.8) that the joint p.d.f. of XandYwill be 
f(x,y) =

1
1−xfor 0<x<y< 1, 
0 otherwise. (3.6.15)
Thus, for 0 <y< 1, the value of the marginal p.d.f. f2(y) of Ywill be 
f2(y) =/integraldisplay∞
−∞f(x,y)dx =/integraldisplayy
01
1−xdx =− log(1−y). (3.6.16)
Furthermore, since Ycannot be outside the interval 0 <y< 1, then f2(y) =0 for 
y≤0 or y≥1. This marginal p.d.f. f2is sketched in Fig. 3.21. It is interesting to note 
that in this example the function f2is unbounded. 
We can also ﬁnd the conditional p.d.f. of XgivenY=yby applying Bayes’ theo- 
rem (3.6.14). The product of g2(y |x) andf1(x) was already calculated in Eq. (3.6.15). 
Figure 3.21 The marginal 
p.d.f. of Yin Example 3.6.10. 
yf2(y)
10
150 Chapter 3 Random Variables and Distributions 
The ratio of this product to f2(y) from Eq. (3.6.16) is 
g1(x |y) =

−1
(1−x) log(1−y) for 0<x<y ,
0 otherwise. ◭
Theorem
3.6.5Independent Random Variables . Suppose that XandYare two random variables
having a joint p.f., p.d.f., or p.f./p.d.f. f. Then XandYare independent if and only if 
for every value of ysuch thatf2(y)> 0 and every value of x,
g1(x |y) =f1(x). (3.6.17)
Proof Theorem 3.5.4 says that XandYare independent if and only if f(x,y) can be 
factored in the following form for −∞<x< ∞and−∞<y< ∞:
f(x,y) =f1(x)f 2(y),
which holds if and only if, for all xand allysuch thatf2(y)> 0, 
f1(x) =f(x,y) 
f2(y) . (3.6.18)
But the right side of Eq. (3.6.18) is the formula for g1(x |y) . Hence, XandYare
independent if and only if Eq. (3.6.17) holds for all xand allysuch thatf2(y)> 0. 
Theorem 3.6.5 says that XandYare independent if and only if the conditional p.f. or 
p.d.f. of XgivenY=yis the same as the marginal p.f. or p.d.f. of Xfor allysuch that
f2(y)> 0. Because g1(x |y) is arbitrary when f2(y) =0, we cannot expect Eq. (3.6.17) 
to hold in that case. 
Similarly, it follows from Eq. (3.6.8) that XandYare independent if and only if 
g2(y |x) =f2(y), (3.6.19)
for every value of xsuch thatf1(x)> 0. Theorem 3.6.5 and Eq. (3.6.19) give the 
mathematical justiﬁcation for the meaning of independence that we presented on 
page 136.
Note: Conditional Distributions Behave Just Like Distribu tions. As we noted on 
page 59, conditional probabilities behave just like probabilities. Since distributions 
are just collections of probabilities, it follows that conditional distributions behave 
just like distributions. For example, to compute the conditional probability that a 
discrete random variable Xis in some interval [ a,b ] given Y=y, we must add g1(x |y) 
for all values of xin the interval. Also, theorems that we have proven or shall prove 
about distributions will have versions conditional on additional random variables. 
We shall postpone examples of such theorems until Sec. 3.7 because they rely on 
joint distributions of more than two random variables. 
Summary
The conditional distribution of one random variable Xgiven an observed value y
of another random variable Yis the distribution we would use for Xif we were to 
learn thatY=y. When dealing with the conditional distribution of XgivenY=y,
it is safe to behave as if Ywere the constant y. If XandYhave joint p.f., p.d.f., 
or p.f./p.d.f. f(x,y) , then the conditional p.f. or p.d.f. of XgivenY=yis g1(x |y) =
3.6 Conditional Distributions 151
f(x,y)/f 2(y) , where f2is the marginal p.f. or p.d.f. of Y. When it is convenient to 
specify a conditional distribution directly, the joint distribution can be constructed 
from the conditional together with the other marginal. For example, 
f(x,y) =g1(x |y)f 2(y) =f1(x)g 2(y |x).
In this case, we have versions of the law of total probability and Bayes’ theorem for 
random variables that allow us to calculate the other marginal and conditional. 
Two random variables XandYare independent if and only if the conditional p.f. 
or p.d.f. of XgivenY=yis the same as the marginal p.f. or p.d.f. of Xfor allysuch
thatf2(y)> 0. Equivalently, XandYare independent if and only if the conditional 
p.f. of p.d.f. of YgivenX=xis the same as the marginal p.f. or p.d.f. of Yfor allx
such thatf1(x)> 0. 
Exercises 
1. Suppose that two random variables XandYhave the
joint p.d.f. in Example 3.5.10 on page 139. Compute the 
conditional p.d.f. of XgivenY=yfor eachy.
2. Each student in a certain high school was classiﬁed ac- 
cording to her year in school (freshman, sophomore, ju- 
nior, or senior) and according to the number of times that 
she had visited a certain museum (never, once, or more 
than once). The proportions of students in the various clas- 
siﬁcations are given in the following table: 
More
Never Once than once
Freshmen 0.08 0.10 0.04 
Sophomores 0.04 0.10 0.04
Juniors 0.04 0.20 0.09 
Seniors 0.02 0.15 0.10
a. If a student selected at random from the high school 
is a junior, what is the probability that she has never 
visited the museum?
b. If a student selected at random from the high school 
has visited the museum three times, what is the prob- 
ability that she is a senior? 
3. Suppose that a point ( X,Y) is chosen at random from 
the diskSdeﬁned as follows: 
S={(x,y) :(x −1)2+(y +2)2≤9}.
Determine (a)the conditional p.d.f. of Yfor every given
value of X, and (b)Pr (Y > 0|X=2).
4. Suppose that the joint p.d.f. of two random variables X
andYis as follows: f(x,y) =/braceleftbigg
c(x +y2)for 0≤x≤1 and 0 ≤y≤1, 
0 otherwise. 
Determine (a)the conditional p.d.f. of Xfor every given
value of Y, and (b)Pr (X< 1
2|Y=1
2).
5. Suppose that the joint p.d.f. of two points XandY
chosen by the process described in Example 3.6.10 is as 
given by Eq. (3.6.15). Determine (a)the conditional p.d.f. 
of Xfor every given value of Y, and (b)Pr /parenleftBig
X> 1
2/vextendsingle/vextendsingle/vextendsingleY=3
4/parenrightBig
.
6. Suppose that the joint p.d.f. of two random variables X
andYis as follows: 
f(x,y) =/braceleftbigg
csinxfor 0≤x≤π/ 2 and 0 ≤y≤3, 
0 otherwise. 
Determine (a)the conditional p.d.f. of Yfor every given
value of X, and (b)Pr (1<Y < 2|X=0.73 ).
7. Suppose that the joint p.d.f. of two random variables X
andYis as follows: 
f(x,y) =

3
16 (4−2x−y) forx> 0,y> 0, 
and 2x+y< 4, 
0 otherwise. 
Determine (a)the conditional p.d.f. of Yfor every given
value of X, and (b)Pr (Y ≥2|X=0.5).
8. Suppose that a person’s score Xon a mathematics ap- 
titude test is a number between 0 and 1, and that his score 
Yon a music aptitude test is also a number between 0 
and 1. Suppose further that in the population of all col- 
lege students in the United States, the scores XandYare
distributed according to the following joint p.d.f.: 
f(x,y) =/braceleftBigg
2
5(2x+3y) for 0≤x≤1 and 0 ≤y≤1, 
0 otherwise. 
152 Chapter 3 Random Variables and Distributions 
a. What proportion of college students obtain a score 
greater than 0.8 on the mathematics test? 
b. If a student’s score on the music test is 0.3, what is the 
probability that his score on the mathematics test will 
be greater than 0.8? 
c. If a student’s score on the mathematics test is 0.3, 
what is the probability that his score on the music 
test will be greater than 0.8? 
9. Suppose that either of two instruments might be used 
for making a certain measurement. Instrument 1 yields a 
measurement whose p.d.f. h1is 
h1(x) =/braceleftbigg
2xfor 0<x< 1, 
0 otherwise. 
Instrument 2 yields a measurement whose p.d.f. h2is 
h2(x) =/braceleftbigg
3x2for 0<x< 1, 
0 otherwise. 
Suppose that one of the two instruments is chosen at ran- 
dom and a measurement Xis made with it. 
a. Determine the marginal p.d.f. of X.
b. If the value of the measurement is X=1/4, what is 
the probability that instrument 1 was used? 
10.In a large collection of coins, the probability Xthat a
head will be obtained when a coin is tossed varies from one 
coin to another, and the distribution of Xin the collection 
is speciﬁed by the following p.d.f.: 
f1(x) =/braceleftbigg
6x( 1−x) for 0<x< 1, 
0 otherwise. 
Suppose that a coin is selected at random from the collec- 
tion and tossed once, and that a head is obtained. Deter- 
mine the conditional p.d.f. of Xfor this coin.11.The deﬁnition of the conditional p.d.f. of XgivenY=
yis arbitrary if f2(y) =0. The reason that this causes no 
serious problem is that it is highly unlikely that we will 
observeYclose to a value y0such thatf2(y 0)=0. To be 
more precise, let f2(y 0)=0, and let A0=[y0−ǫ,y 0+ǫ]. 
Also, let y1be such that f2(y 1)> 0, and let A1=[y1−
ǫ,y 1+ǫ]. Assume that f2is continuous at both y0andy1.
Show that
lim
ǫ→0Pr (Y ∈A0)
Pr (Y ∈A1)=0.
That is, the probability that Yis close to y0is much smaller 
than the probability that Yis close to y1.
12.LetYbe the rate (calls per hour) at which calls arrive 
at a switchboard. Let Xbe the number of calls during a 
two-hour period. Suppose that the marginal p.d.f. of Yis 
f2(y) =/braceleftbigge−yif y> 0, 
0 otherwise, 
and that the conditional p.f. of XgivenY=yis 
g1(x |y) =

(2y) x
x!e−2yif x=0,1,...,
0 otherwise. 
a. Find the marginal p.f. of X. (You may use the formula /integraltext∞
0yke−ydy =k!.) 
b. Find the conditional p.d.f. g2(y |0)of YgivenX=0. 
c. Find the conditional p.d.f. g2(y |1)of YgivenX=1. 
d. For what values of yis g2(y |1)>g 2(y |0)? Does this 
agree with the intuition that the more calls you see,
the higher you should think the rate is?
13.Start with the joint distribution of treatment group 
and response in Table 3.6 on page 138. For each treatment 
group, compute the conditional distribution of response 
given the treatment group. Do they appear to be very 
similar or quite different? 
3.7 Multivariate Distributions 
In this section, we shall extend the results that were developed in Sections 3.4,
3.5, and 3.6 for two random variables XandYto an arbitrary ﬁnite number 
nof random variables X1,...,X n. In general, the joint distribution of more 
than two random variables is called a multivariate distribution . The theory of
statisticalinference(thesubjectofthepartofthisbookbeginningwithChapter7) 
reliesonmathematicalmodelsforobservabledatainwhicheachobservationis 
arandomvariable.Forthisreason, multivariatedistributionsarisenaturallyin 
themathematicalmodelsfordata.Themostcommonlyusedmodelwillbeonein 
whichtheindividualdatarandomvariablesareconditionallyindependentgiven 
oneortwootherrandomvariables.
3.7 Multivariate Distributions 153
Joint Distributions
Example 
3.7.1 A Clinical Trial . Suppose that mpatients with a certain medical condition are given a 
treatment, and each patient either recovers from the condition or fails to recover. For 
eachi=1,...,m , we can let Xi=1 if patient irecovers and Xi=0 if not. We might 
also believe that there is a random variable Phaving a continuous distribution taking 
values between 0 and 1 such that, if we knew that P=p, we would say that the m
patients recover or fail to recover independently of each other each with probability 
pof recovery. We now have named n=m+1 random variables in which we are 
interested. ◭
The situation described in Example 3.7.1 requires us to construct a joint distri- 
bution fornrandom variables. We shall now provide deﬁnitions and examples of the 
important concepts needed to discuss multivariate distributions. 
Deﬁnition
3.7.1 Joint Distribution Function/c.d.f . The jointc.d.f. of nrandom variables X1,...,X nis 
the function Fwhose value at every point (x 1,...,x n)in n-dimensional space Rnis 
speciﬁed by the relation 
F(x 1,...,x n)=Pr (X 1≤x1,X 2≤x2,...,X n≤xn). (3.7.1)
Every multivariate c.d.f. satisﬁes properties similar to those given earlier for univari- 
ate and bivariate c.d.f.’s. 
Example 
3.7.2 Failure Times . Suppose that a machine has three parts, and part iwill fail at time Xi
fori=1,2,3. The following function might be the joint c.d.f. of X1,X2, and X3:
F(x 1,x 2,x 3)=/braceleftbigg
(1−e−x1)( 1−e−2x2)( 1−e−3x3)forx1,x 2,x 3≥0, 
0 otherwise. ◭
Vector Notation In the study of the joint distribution of nrandom variables
X1,...,X n, it is often convenient to use the vector notation X=(X 1, . . . , X n)and
to refer to Xas a random vector . Instead of speaking of the joint distribution of 
the random variables X1, . . . , X nwith a joint c.d.f. F(x 1, . . . , x n), we can simply 
speak of the distribution of the random vector Xwith c.d.f. F( x). When this vector 
notation is used, it must be kept in mind that if Xis an n-dimensional random vec-
tor, then its c.d.f. is deﬁned as a function on n-dimensional space Rn. At each point 
x=(x 1, . . . , x n)∈Rn, the value of F( x)is speciﬁed by Eq. (3.7.1). 
Deﬁnition
3.7.2 Joint Discrete Distribution/p.f . It is said that nrandom variables X1, . . . , X nhave a
discretejoint distribution if the random vector (X 1, . . . , X n)can have only a ﬁnite 
number or an inﬁnite sequence of different possible values (x 1, . . . , x n)in Rn. The 
jointp.f.of X1, . . . , X nis then deﬁned as the function fsuch that for every point
(x 1, . . . , x n)∈Rn,
f(x 1, . . . , x n)=Pr (X 1=x1, . . . , X n=xn). 
In vector notation, Deﬁnition 3.7.2 says that the random vector Xhas a discrete 
distribution and that its p.f. is speciﬁed at every point x∈Rnby the relation 
f( x)=Pr (X=x). 
154 Chapter 3 Random Variables and Distributions 
The following result is a simple generalization of Theorem 3.4.2. 
Theorem
3.7.1 If Xhas a joint discrete distribution with joint p.f. f, then for every subset C⊂Rn,
Pr (X∈C) =/summationdisplay
x∈Cf( x). 
It is easy to show that, if each of X1, . . . , X nhas a discrete distribution, then 
X=(X 1, . . . , X n)has a discrete joint distribution. 
Example 
3.7.3 A Clinical Trial . Consider the mpatients in Example 3.7.1. Suppose for now that 
P=pis known so that we don’t treat it as a random variable. The joint p.f. of 
X=(X 1, . . . , X m)is 
f( x)=px1+...+xm(1−p) m−x1−...−xm,
for all xi∈{0,1}and 0 otherwise. ◭
Deﬁnition
3.7.3 Continuous Distribution/p.d.f . It is said that nrandom variables X1, . . . , X nhave a
continuous joint distribution if there is a nonnegative function fdeﬁned on Rnsuch
that for every subset C⊂Rn,
Pr[ (X 1, . . . , X n)∈C]=/integraldisplay
. . . 
C/integraldisplay
f(x 1, . . . x n) dx 1. . . dx n, (3.7.2)
if the integral exists. The function fis called the jointp.d.f. of X1, . . . , X n.
In vector notation, f( x)denotes the p.d.f. of the random vector Xand Eq. (3.7.2)
could be rewritten more simply in the form 
Pr (X∈C) =/integraldisplay
. . . 
C/integraldisplay
f( x) d x.
Theorem
3.7.2 If the joint distribution of X1, . . . , X nis continuous, then the joint p.d.f. fcan be 
derived from the joint c.d.f. Fby using the relation 
f(x 1, . . . , x n)=∂nF(x 1, . . . , x n)
∂x 1. . . ∂x n
at all points (x 1, . . . , x n)at which the derivative in this relation exists. 
Example 
3.7.4 Failure Times . We can ﬁnd the joint p.d.f. for the three random variables in Exam- 
ple 3.7.2 by applying Theorem 3.7.2. The third-order mixed partial is easily calculated 
to be 
f(x 1, x 2, x 3)=/braceleftbigg
6e−x1−2x2−3x3forx1, x 2, x 3>0, 
0 otherwise. ◭
It is important to note that, even if each of X1, . . . , X nhas a continuous distri- 
bution, the vector X=(X 1, . . . , X n)might not have a continuous joint distribution. 
See Exercise 9 in this section. 
Example 
3.7.5 Service Times in a Queue . A queue is a system in which customers line up for service 
and receive their service according to some algorithm. A simple model is the single- 
server queue, in which all customers wait for a single server to serve everyone ahead 
of them in the line and then they get served. Suppose that ncustomers arrive at a 
3.7 Multivariate Distributions 155
single-server queue for service. Let Xibe the time that the server spends serving 
customer ifori=1, . . . , n . We might use a joint distribution for X=(X 1, . . . , X n)
with joint p.d.f. of the form 
f( x)=

c
/parenleftbig
2+/summationtextn
i=1xi/parenrightbign+1for all xi>0, 
0 otherwise. (3.7.3)
We shall now ﬁnd the value of csuch that the function in Eq. (3.7.3) is a joint p.d.f. 
We can do this by integrating over each variable x1, . . . , x nin succession (starting 
withxn). The ﬁrst integral is 
/integraldisplay∞
0c
(2+x1+. . . +xn)n+1dx n=c/n 
(2+x1+. . . +xn−1)n. (3.7.4)
The right-hand side of Eq. (3.7.4) is in the same form as the original p.d.f. except 
thatnhas been reduced to n−1 and chas been divided by n. It follows that when 
we integrate over the variable xi(fori=n−1, n −2, . . . , 1), the result will be in 
the same form with nreduced to i−1 and cdivided by n(n−1). . . i. The result of 
integrating all coordinates except x1is then 
c/n !
(2+x1)2,forx1>0.
Integrating x1out of this yields c/ [2 (n !)], which must equal 1, so c=2(n !). ◭
Mixed Distributions 
Example 
3.7.6 Arrivals at a Queue . In Example 3.7.5, we introduced the single-server queue and 
discussed service times. Some features that inﬂuence the performance of a queue are 
the rate at which customers arrive and the rate at which customers are served. Let Z
stand for the rate at which customers are served, and let Ystand for the rate at which 
customers arrive at the queue. Finally, let Wstand for the number of customers that 
arrive during one day. Then Wis discrete while YandZcould be continuous random 
variables. A possible joint p.f./p.d.f. for these three random variables is 
f(y, z, w) =/braceleftbigg
6e−3z−10 y(8y) w/w ! for z, y > 0 and w=0,1, . . . , 
0 otherwise. 
We can verify this claim shortly. ◭
Deﬁnition
3.7.4 Joint p.f./p.d.f . Let X1, . . . , X nbe random variables, some of which have a continuous 
joint distribution and some of which have discrete distributions; their joint distribu- 
tion would then be represented by a function fthat we call the joint p.f./p.d.f . The 
function has the property that the probability that Xlies in a subset C⊂Rnis calcu- 
lated by summing f( x)over the values of the coordinates of xthat correspond to the 
discrete random variables and integrating over those coordinates that correspond to 
the continuous random variables for all points x∈C.
Example 
3.7.7 Arrivals at a Queue . We shall now verify that the proposed p.f./p.d.f. in Example 3.7.6 
actually sums and integrates to 1 over all values of (y, z, w) . We must sum over w
and integrate over yandz. We have our choice of in what order to do them. It is not 
156 Chapter 3 Random Variables and Distributions 
difﬁcult to see that we can factor fas f(y, z, w) =h2(z)h 13 (y, w) , where 
h2(z) =/braceleftbigg6e−3zforz > 0, 
0 otherwise, 
h13 (y, w) =/braceleftbigg
e−10 y(8y) w/w ! for y > 0 and w=0,1, . . . , 
0 otherwise. 
So we can integrate zout ﬁrst to get 
/integraldisplay∞
−∞f(y, z, w)dz =h13 (y, w) /integraldisplay∞
06e−3zdz =2h13 (y, w). 
Integrating yout of h13 (y, w) is possible, but not pleasant. Instead, notice that 
(8y) w/w ! is the wth term in the Taylor expansion of e8y. Hence, 
∞/summationdisplay
w=02h13 (y, w) =2e−10 y∞/summationdisplay
w=0(8y) w
w!=2e−10 ye8y=2e−2y,
fory > 0 and 0 otherwise. Finally, integrating over yyields 1. ◭
Example 
3.7.8 A Clinical Trial . In Example 3.7.1, one of the random variables Phas a continuous 
distribution, and the others X1, . . . , X mhave discrete distributions. A possible joint 
p.f./p.d.f. for (X 1, . . . , X m, P) is 
f( x, p) =/braceleftbigg
px1+...+xm(1−p) m−x1−...−xmfor all xi∈{0,1}and 0≤p≤1, 
0 otherwise. 
We can ﬁnd probabilities based on this function. Suppose, for example, that we want 
the probability that there is exactly one success among the ﬁrst two patients, that is, 
Pr (X 1+X2=1). We must integrate f( x, p) overpand sum over all values of xthat
havex1+x2=1. For purposes of illustration, suppose that m=4. First, factor out 
px1+x2(1−p) 2−x1−x2=p( 1−p) , which yields 
f( x, p) =[p( 1−p) ]px3+x4(1−p) 2−x3−x4,
forx3, x 4∈{0,1}, 0 < p < 1, and x1+x2=1. Summing over x3yields
[p( 1−p) ]/parenleftBig
px4(1−p) 1−x4(1−p) +pp x4(1−p) 1−x4/parenrightBig
=[p( 1−p) ]px4(1−p) 1−x4.
Summing this over x4givesp( 1−p) . Next, integrate over pto get /integraltext1
0p( 1−p)dp =
1/6. Finally, note that there are two (x 1, x 2)vectors, (1,0)and(0,1), that have 
x1+x2=1, so Pr (X 1+X2=1)=(1/6)+(1/6)=1/3. ◭
Marginal Distributions 
DerivingaMarginalp.d.f. If the joint distribution of nrandom variables X1, . . . , 
Xnis known, then the marginal distribution of each single random variable Xican
be derived from this joint distribution. For example, if the joint p.d.f. of X1, . . . , X n
is f, then the marginal p.d.f. f1of X1is speciﬁed at every value x1by the relation 
f1(x 1)=/integraltext∞
−∞. . . /integraltext∞
−∞ /bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
n−1f(x 1, . . . , x n) dx 2. . . dx n.
More generally, the marginal joint p.d.f. of any kof the nrandom variables
X1, . . . , X ncan be found by integrating the joint p.d.f. over all possible values of 
3.7 Multivariate Distributions 157
the other n−kvariables. For example, if fis the joint p.d.f. of four random variables 
X1,X2,X3, and X4, then the marginal bivariate p.d.f. f24 of X2andX4is speciﬁed at 
each point (x 2, x 4)by the relation 
f24 (x 2, x 4)=/integraldisplay∞
−∞/integraldisplay∞
−∞f(x 1, x 2, x 3, x 4) dx 1dx 3.
Example 
3.7.9 Service Times in a Queue . Suppose that n=5 in Example 3.7.5 and that we want the 
marginal bivariate p.d.f. of (X 1, X 4). We must integrate Eq. (3.7.3) over x2,x3, and x5.
Since the joint p.d.f. is symmetric with respect to permutations of the coordinates of 
x, we shall just integrate over the last three variables and then change the names of 
the remaining variables to x1andx4. We already saw how to do this in Example 3.7.5. 
The result is 
f12 (x 1, x 2)=

4
(2+x1+x2)3forx1, x 2>0, 
0 otherwise. (3.7.5)
Then f14 is just like (3.7.5) with all the 2 subscripts changed to 4. The univariate 
marginal p.d.f. of each Xiis 
fi(x i)=

2
(2+xi)2forxi>0, 
0 otherwise. (3.7.6)
So, for example, if we want to know how likely it is that a customer will have to wait 
longer than three time units, we can calculate Pr (X i>3)by integrating the function 
in Eq. (3.7.6) from 3 to ∞. The result is 0.4. ◭
If nrandom variables X1, . . . , X nhave a discrete joint distribution, then the 
marginal joint p.f. of each subset of the nvariables can be obtained from relations 
similar to those for continuous distributions. In the new relations, the integrals are 
replaced by sums. 
DerivingaMarginalc.d.f. Consider now a joint distribution for which the joint 
c.d.f. of X1, . . . , X nis F. The marginal c.d.f. F1of X1can be obtained from the 
following relation:
F1(x 1)=Pr (X 1≤x1)=Pr (X 1≤x1, X 2<∞, . . . , X n<∞)
= limx2,...,x n→∞F(x 1, x 2, . . . , x n). 
Example 
3.7.10 Failure Times . We can ﬁnd the marginal c.d.f. of X1from the joint c.d.f. in Exam- 
ple 3.7.2 by letting x2andx3go to ∞. The limit is F1(x 1)=1−e−x1forx1≥0 and 0 
otherwise. ◭
More generally, the marginal joint c.d.f. of any kof the nrandom variables
X1, . . . , X ncan be found by computing the limiting value of the n-dimensional c.d.f. 
Fas xj→∞ for each of the other n−kvariables xj. For example, if Fis the joint 
c.d.f. of four random variables X1,X2,X3, and X4, then the marginal bivariate c.d.f. 
F24 of X2andX4is speciﬁed at every point ( x2,x4) by the relation 
F24 (x 2, x 4)= limx1,x 3→∞F(x 1, x 2, x 3, x 4). 
158 Chapter 3 Random Variables and Distributions 
Example 
3.7.11 Failure Times . We can ﬁnd the marginal bivariate c.d.f. of X1andX3from the joint
c.d.f. in Example 3.7.2 by letting x2go to ∞. The limit is 
F13 (x 1, x 3)=/braceleftbigg
(1−e−x1)( 1−e−3x3)forx1, x 3≥0, 
0 otherwise. ◭
Independent Random Variables 
Deﬁnition
3.7.5 Independent Random Variables . It is said that nrandom variables X1, . . . , X nare
independent if, for every nsetsA1, A 2, . . . , A nof real numbers, 
Pr (X 1∈A1, X 2∈A2, . . . , X n∈An)
=Pr (X 1∈A1)Pr (X 2∈A2). . . Pr (X n∈An). 
If X1, . . . , X nare independent, it follows easily that the random variables in every 
nonempty subset of X1, . . . , X nare also independent. (See Exercise 11.)
There is a generalization of Theorem 3.5.4. 
Theorem
3.7.3 LetFdenote the joint c.d.f. of X1, . . . , X n, and let Fidenote the marginal univariate
c.d.f. of Xifori=1, . . . , n . The variables X1, . . . , X nare independent if and only if, 
for all points (x 1, x 2, . . . , x n)∈Rn,
F(x 1, x 2, . . . , x n)=F1(x 1)F 2(x 2). . . Fn(x n). 
Theorem 3.7.3 says that X1, . . . , X nare independent if and only if their joint c.d.f. 
is the product of their nindividual marginal c.d.f.’s. It is easy to check that the three 
random variables in Example 3.7.2 are independent using Theorem 3.7.3. 
There is also a generalization of Corollary 3.5.1. 
Theorem
3.7.4 If X1, . . . , X nhave a continuous, discrete, or mixed joint distribution for which the 
joint p.d.f., joint p.f., or joint p.f./p.d.f. is f, and if fiis the marginal univariate p.d.f. or 
p.f. of Xi(i =1, . . . , n) , then X1, . . . , X nare independent if and only if the following 
relation is satisﬁed at all points (x 1, x 2, . . . , x n)∈Rn:
f(x 1, x 2, . . . , x n)=f1(x 1)f 2(x 2). . . fn(x n). (3.7.7)
Example 
3.7.12 Service Times in a Queue . In Example 3.7.9, we can multiply together the two uni- 
variate marginal p.d.f.’s of X1andX2calculated using Eq. (3.7.6) and see that the
product does notequal the bivariate marginal p.d.f. of (X 1, X 2)in Eq. (3.7.5). So X1
andX2are not independent. ◭
Deﬁnition
3.7.6 Random Samples/i.i.d./Sample Size . Consider a given probability distribution on the 
real line that can be represented by either a p.f. or a p.d.f. f. It is said that n
random variables X1, . . . , X nform arandom sample from this distribution if these 
random variables are independent and the marginal p.f. or p.d.f. of each of them is 
f. Such random variables are also said to be independentandidenticallydistributed ,
abbreviated i.i.d. We refer to the number nof random variables as the samplesize .
Deﬁnition 3.7.6 says that X1, . . . , X nform a random sample from the distribution 
represented by fif their joint p.f. or p.d.f. gis speciﬁed as follows at all points 
(x 1, x 2, . . . , x n)∈Rn:
g(x 1, . . . , x n)=f(x 1)f(x 2). . . f(x n). 
Clearly, an i.i.d. sample cannot have a mixed joint distribution. 
3.7 Multivariate Distributions 159
Example 
3.7.13 Lifetimes of Light Bulbs . Suppose that the lifetime of each light bulb produced in a 
certain factory is distributed according to the following p.d.f.: 
f(x) =/braceleftbigg
xe −xforx > 0, 
0 otherwise. 
We shall determine the joint p.d.f. of the lifetimes of a random sample of nlight bulbs
drawn from the factory’s production. 
The lifetimes X1, . . . , X nof the selected bulbs will form a random sample from 
the p.d.f. f. For typographical simplicity, we shall use the notation exp (v) to denote 
the exponential evwhen the expression for vis complicated. Then the joint p.d.f. g
of X1, . . . , X nwill be as follows: If xi>0 for i=1, . . . , n ,
g(x 1, . . . , x n)=n/productdisplay
i=1f(x i)
=/parenleftBiggn/productdisplay
i=1xi/parenrightBigg
exp/parenleftBigg
−n/summationdisplay
i=1xi/parenrightBigg
.
Otherwise, g(x 1, . . . , x n)=0. 
Every probability involving the nlifetimes X1, . . . , X ncan in principle be deter- 
mined by integrating this joint p.d.f. over the appropriate subset of Rn. For example, if 
Cis the subset of points (x 1, . . . , x n)such that xi>0 for i=1, . . . , n and/summationtextn
i=1xi< a ,
where ais a given positive number, then 
Pr /parenleftBiggn/summationdisplay
i=1Xi< a /parenrightBigg
=/integraldisplay
. . . 
C/integraldisplay/parenleftBiggn/productdisplay
i=1xi/parenrightBigg
exp/parenleftBigg
−n/summationdisplay
i=1xi/parenrightBigg
dx 1. . . dx n. ◭
The evaluation of the integral given at the end of Example 3.7.13 may require 
a considerable amount of time without the aid of tables or a computer. Certain 
other probabilities, however, can be evaluated easily from the basic properties of 
continuous distributions and random samples. For example, suppose that for the 
conditions of Example 3.7.13 it is desired to ﬁnd Pr (X 1< X 2<. . . < X n). Since the 
random variables X1, . . . , X nhave a continuous joint distribution, the probability 
that at least two of these random variables will have the same value is 0. In fact, 
the probability is 0 that the vector (X 1, . . . , X n)will belong to each speciﬁc subset 
of Rnfor which the n-dimensional volume is 0. Furthermore, since X1, . . . , X nare
independent and identically distributed, each of these variables is equally likely to 
be the smallest of the nlifetimes, and each is equally likely to be the largest. More 
generally, if the lifetimes X1, . . . , X nare arranged in order from the smallest to the 
largest, each particular ordering of X1, . . . , X nis as likely to be obtained as any 
other ordering. Since there are n! different possible orderings, the probability that 
the particular ordering X1< X 2<. . . < X nwill be obtained is 1 /n !. Hence, 
Pr (X 1< X 2<. . . < X n)=1
n!.
Conditional Distributions
Suppose that nrandom variables X1, . . . , X nhave a continuous joint distribution for 
which the joint p.d.f. is fand that f0denotes the marginal joint p.d.f. of the k < n ran-
dom variables X1, . . . , X k. Then for all values of x1, . . . , x ksuch that f0(x 1, . . . , x k) > 
0, the conditional p.d.f. of (X k+1, . . . , X n)given that X1=x1, . . . , X k=xkis deﬁned 
160 Chapter 3 Random Variables and Distributions 
as follows: 
gk+1...n(x k+1, . . . , x n|x1, . . . , x k)=f(x 1, x 2, . . . , x n)
f0(x 1, . . . , x k).
The deﬁnition above generalizes to arbitrary joint distributions as follows. 
Deﬁnition
3.7.7 Conditional p.f., p.d.f., or p.f./p.d.f . Suppose that the random vector X=(X 1, . . . , X n)
is divided into two subvectors YandZ, where Yis a k-dimensional random vector
comprising kof the nrandom variables in X, and Zis an (n −k) -dimensional random
vector comprising the other n−krandom variables in X. Suppose also that the 
n-dimensional joint p.f., p.d.f., or p.f./p.d.f. of (Y,Z)is fand that the marginal (n −k) -
dimensional p.f., p.d.f., or p.f./p.d.f. of Zis f2. Then for every given point z∈Rn−ksuch
thatf2(z) > 0, the conditional k-dimensional p.f., p.d.f., or p.f./p.d.f. g1of Ygiven
Z=zis deﬁned as follows: 
g1(y|z)=f( y,z)
f2(z)fory∈Rk. (3.7.8)
Eq. (3.7.8) can be rewritten as 
f( y,z)=g1(y|z)f 2(z), (3.7.9)
which allows construction of the joint distribution from a conditional distribution and 
a marginal distribution. As in the bivariate case, it is safe to assume that f( y,z)=0
whenever f2(z)=0. Then Eq. (3.7.9) holds for all yandzeven though g1(y|z)is not 
uniquely deﬁned.
Example 
3.7.14 Service Times in a Queue . In Example 3.7.9, we calculated the marginal bivariate 
distribution of two service times Z=(X 1, X 2). We can now ﬁnd the conditional three- 
dimensional p.d.f. of Y=(X 3, X 4, X 5)givenZ=(x 1, x 2)for every pair (x 1, x 2)such
thatx1, x 2>0: 
g1(x 3, x 4, x 5|x1, x 2)=f(x 1, . . . , x 5)
f12 (x 1, x 2)
=/parenleftbigg240
(2+x1+. . . +x5)6/parenrightbigg/parenleftbigg 4
(2+x1+x2)3/parenrightbigg−1
=60 (2+x1+x2)3
(2+x1+. . . +x5)6, (3.7.10)
forx3, x 4, x 5>0, and 0 otherwise. The joint p.d.f. in (3.7.10) looks like a bu nch of 
symbols, but it can be quite useful. Suppose that we observe X1=4 and X2=6. Then 
g1(x 3, x 4, x 5|4.6)=

103,680
(12 +x3+x4+x5)6forx3, x 4, x 5>0, 
0 otherwise. 
We can now calculate the conditional probability that X3>3 given X1=4, X 2=6: 
3.7 Multivariate Distributions 161
Pr (X 3>3|X1=4, X 2=6)=/integraldisplay∞
3/integraldisplay∞
0/integraldisplay∞
010 ,360
(12 +x3+x4+x5)6dx 5dx 4dx 3
=/integraldisplay∞
3/integraldisplay∞
020 ,736
(12 +x3+x4)5dx 4dx 3
=/integraldisplay∞
35184
(12 +x3)4dx 3
=1728
15 3=0.512.
Compare this to the calculation of Pr (X 3>3)=0.4 at the end of Example 3.7.9. 
After learning that the ﬁrst two service times are a bit longer than three time units, we 
revise the probability that X3>3 upward to reﬂect what we learned from the ﬁrst two 
observations. If the ﬁrst two service times had been small, the conditional probability 
thatX3>3 would have been smaller than 0.4. For example, Pr (X 3>3|X1=1, X 2=
1.5)=0.216. ◭
Example 
3.7.15 Determining a Marginal Bivariate p.d.f . Suppose that Zis a random variable for which 
the p.d.f. f0is as follows: 
f0(z) =/braceleftbigg
2e−2zforz > 0, 
0 otherwise. (3.7.11)
Suppose, furthermore, that for every given value Z=z > 0 two other random vari- 
ablesX1andX2are independent and identically distributed and the conditional p.d.f. 
of each of these variables is as follows: 
g(x |z) =/braceleftbigg
ze −zx forx > 0, 
0 otherwise. (3.7.12)
We shall determine the marginal joint p.d.f. of (X 1, X 2).
SinceX1andX2are i.i.d. for each given value of Z, their conditional joint p.d.f. 
whenZ=z > 0 is 
g12 (x 1, x 2|z) =/braceleftbigg
z2e−z(x 1+x2)forx1, x 2>0, 
0 otherwise. 
The joint p.d.f. fof (Z, X 1, X 2)will be positive only at those points (z, x 1, x 2)
such that x1, x 2, z > 0. It now follows that, at every such point, 
f(z, x 1, x 2)=f0(z)g 12 (x 1, x 2|z) =2z2e−z( 2+x1+x2).
For x1>0 and x2>0, the marginal joint p.d.f. f12 (x 1, x 2)of X1andX2can be 
determined either using integration by parts or some special results that will arise 
in Sec. 5.7: 
f12 (x 1, x 2)=/integraldisplay∞
0f(z, x 1, x 2) dz =4
(2+x1+x2)3,
forx1, x 2>0. The reader will note that this p.d.f. is the same as the margin al bivariate 
p.d.f. of (X 1, X 2)found in Eq. (3.7.5). 
From this marginal bivariate p.d.f., we can evaluate probabilities involving X1
andX2, such as Pr (X 1+X2<4). We have 
Pr (X 1+X2<4)=/integraldisplay4
0/integraldisplay4−x2
04
(2+x1+x2)3dx 1dx 2=4
9. ◭
162 Chapter 3 Random Variables and Distributions 
Example 
3.7.16 Service Times in a Queue . We can think of the random variable Zin Example 3.7.15 
as the rate at which customers are served in the queue of Example 3.7.5. With this 
interpretation, it is useful to ﬁnd the conditional distribution of the rate Zafter we 
observe some of the service times such as X1andX2.
For every value of z, the conditional p.d.f. of ZgivenX1=x1andX2=x2is 
g0(z |x1, x 2)=f(z, x 1, x 2)
f12 (x 1, x 2)
=/braceleftBigg
1
2(2+x1+x2)3z2e−z( 2+x1+x2)forz > 0, 
0 otherwise. (3.7.13)
Finally, we shall evaluate Pr (Z ≤1|X1=1, X 2=4). We have 
Pr (Z ≤1|X1=1, X 2=4)=/integraldisplay1
0g0(z |1,4) dz 
=/integraldisplay1
0171.5z2e−7zdz =0.9704. ◭
LawofTotalProbabilityandBayes’Theorem Example 3.7.15 contains an example 
of the multivariate version of the law of total probability, while Example 3.7.16 
contains an example of the multivariate version of Bayes’ theorem. The proofs of 
the general versions are straightforward consequences of Deﬁnition 3.7.7. 
Theorem
3.7.5 Multivariate Law of Total Probability and Bayes’ Theorem . Assume the conditions and 
notation given in Deﬁnition 3.7.7. If Zhas a continuous joint distribution, the mar- 
ginal p.d.f. of Yis 
f1(y)=/integraldisplay∞
−∞. . . /integraldisplay∞
−∞/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
n−kg1(y|z)f 2(z) d z, (3.7.14)
and the conditional p.d.f. of ZgivenY=yis 
g2(z|y)=g1(y|z)f 2(z)
f1(y). (3.7.15)
If Zhas a discrete joint distribution, then the multiple integral in (3.7.14) must be 
replaced by a multiple summation. If Zhas a mixed joint distribution, the multiple 
integral must be replaced by integration over those coordinates with continuous 
distributions and summation over those coordinates with discrete distributions.
ConditionallyIndependentRandomVariables In Examples 3.7.15 and 3.7.16, Zis 
the single random variable ZandY=(X 1, X 2). These examples also illustrate the use 
of conditionally independent random variables. That is, X1andX2are conditionally
independent given Z=zfor all z > 0. In Example 3.7.16, we said that Zwas the
rate at which customers were served. When this rate is unknown, it is a major source 
of uncertainty. Partitioning the sample space by the values of the rate Zand then
conditioning on each value of Zremoves a major source of uncertainty for part of 
the calculation.
In general, conditional independence for random variables is similar to condi- 
tional independence for events.
3.7 Multivariate Distributions 163
Deﬁnition
3.7.8 Conditionally Independent Random Variables . Let Zbe a random vector with joint 
p.f., p.d.f., or p.f./p.d.f. f0(z). Several random variables X1, . . . , X nareconditionally
independentgiven Zif, for all zsuch that f0(z) > 0, we have 
g( x|z)=n/productdisplay
i=1gi(x i|z), 
where g( x|z)stands for the conditional multivariate p.f., p.d.f., or p.f./p.d.f. of Xgiven
Z=zandgi(x i|z)stands for the conditional univariate p.f. or p.d.f. of XigivenZ=z.
In Example 3.7.15, gi(x i|z) =ze −zx iforxi>0 and i=1,2. 
Example 
3.7.17 A Clinical Trial . In Example 3.7.8, the joint p.f./p.d.f. given there was constructed by 
assuming that X1, . . . , X mwere conditionally independent given P=peach with
the same conditional p.f., gi(x i|p) =pxi(1−p) 1−xiforxi∈{0,1}and that Phad
the uniform distribution on the interval [0 ,1]. These assumptions produce, in the 
notation of Deﬁnition 3.7.8, 
g( x|p) =/braceleftbiggpx1+...+xm(1−p) 40 −x1−...−xmfor all xi∈{0,1}and 0≤p≤1, 
0 otherwise, 
for 0≤p≤1. Combining this with the marginal p.d.f. of P,f2(p) =1 for 0 ≤p≤1
and 0 otherwise, we get the joint p.f./p.d.f. given in Example 3.7.8. ◭
Conditional Versions of Past and Future Theorems We mentioned earlier that 
conditional distributions behave just like distributions. Hence, all theorems that we 
have proven and will prove in the future have conditional versions. For example, 
the law of total probability in Eq. (3.7.14) has the following version conditional on 
another random vector W=w:
f1(y|w)=/integraldisplay∞
−∞. . . /integraldisplay∞
−∞/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
n−kg1(y|z,w)f 2(z|w) d z, (3.7.16)
where f1(y|w)stands for the conditional p.d.f., p.f., or p.f./p.d.f. of YgivenW=w,
g1(y|z,w)stands for the conditional p.d.f., p.f., or p.f./p.d.f. of Ygiven(Z,W)=(z,w),
andf2(z|w)stands for the conditional p.d.f. of ZgivenW=w. Using the same 
notation, the conditional version of Bayes’ theorem is 
g2(z|y,w)=g1(y|z,w)f 2(z|w)
f1(y|w). (3.7.17)
Example 
3.7.18 Conditioning on Random Variables in Sequence . In Example 3.7.15, we found the 
conditional p.d.f. of Zgiven(X 1, X 2)=(x 1, x 2). Suppose now that there are three 
more observations available, X3,X4, and X5, and suppose that all of X1, . . . , X 5
are conditionally i.i.d. given Z=zwith p.d.f. g(x |z) . We shall use the conditional 
version of Bayes’ theorem to compute the conditional p.d.f. of Zgiven(X 1, . . . , X 5)=
(x 1, . . . , x 5). First, we shall ﬁnd the conditional p.d.f. g345(x 3, x 4, x 5|x1, x 2, z) of Y=
(X 3, X 4, X 5)givenZ=zandW=(X 1, X 2)=(x 1, x 2). We shall use the notation for 
p.d.f.’s in the discussion immediately preceding this example. Since X1, . . . , X 5are
conditionally i.i.d. given Z, we have that g1(y|z, w)does not depend on w. In fact, 
g1(y|z, w)=g(x 3|z)g(x 4|z)g(x 5|z) =z3e−z(x 3+x4+x5),
164 Chapter 3 Random Variables and Distributions 
forx3, x 4, x 5>0. We also need the conditional p.d.f. of ZgivenW=w, which was 
calculated in Eq. (3.7.13), and we now denote it 
f2(z |w)=1
2(2+x1+x2)3z2e−z( 2+x1+x2).
Finally, we need the conditional p.d.f. of the last three observations given the ﬁrst 
two. This was calculated in Example 3.7.14, and we now denote it 
f1(y|w)=60 (2+x1+x2)3
(2+x1+. . . +x5)6.
Now combine these using Bayes’ theorem (3.7.17) to obtain 
g2(z|y,w)=z3e−z(x 3+x4+x5)1
2(2+x1+x2)3z2e−z( 2+x1+x2)
60 (2+x1+x2)3
(2+x1+. . . +x5)6
=1
120(2+x1+. . . +x5)6z5e−z( 2+x1+...+x5),
forz > 0. ◭
Note:SimpleRuleforCreatingConditionalVersionsofResul ts.If you ever wish to 
determine the conditional version given W=wof a result that you have proven, here 
is a simple method. Just add “conditional on W=w” to every probabilistic statement 
in the result. This includes all probabilities, c.d.f.’s, quantiles, names of distributions, 
p.d.f.’s, p.f.’s, and so on. It also includes all future probabilistic concepts that we 
introduce in later chapters (such as expected values and variances in Chapter 4). 
Note:IndependenceisaSpecialCaseofConditionalIndepen dence. LetX1, . . . , 
Xnbe independent random variables, and let Wbe a constant random variable. 
That is, there is a constant csuch that Pr (W =c) =1. Then X1, . . . , X nare also
conditionally independent given W=c. The proof is straightforward and is left to 
the reader (Exercise 15). This result is not particularly interesting in its own right. 
Its value is the following: If we prove a result for conditionally independent random 
variables or conditionally i.i.d. random variables, then the same result will hold for 
independent random variables or i.i.d. random variables as the case may be. 
Histograms 
Example 
3.7.19 Rate of Service . In Examples 3.7.5 and 3.7.6, we considered customers arriving at a 
queue and being served. Let Zstand for the rate at which customers were served, 
and we let X1, X 2, . . . stand for the times that the successive customers requrired for
service. Assume that X1, X 2, . . . are conditionally i.i.d. given Z=zwith p.d.f. 
g(x |z) =/braceleftbigg
ze −zx forx > 0, 
0 otherwise. (3.7.18)
This is the same as (3.7.12) from Example 3.7.15. In that example, we modeled Zas 
a random variable with p.d.f. f0(z) =2 exp (−2z) forz > 0. In this example, we shall 
assume that X1, . . . , X nwill be observed for some large value n, and we want to 
think about what these observations tell us about Z. To be speciﬁc, suppose that we 
observe n=100 service times. The ﬁrst 10 times are listed here: 
1.39 ,0.61 ,2.47 ,3.35 ,2.56 ,3.60 ,0.32 ,1.43 ,0.51 ,0.94 .
3.7 Multivariate Distributions 165
The smallest and largest observed service times from the entire sample are 0.004 and 
9.60, respectively. It would be nice to have a graphical display of the entire sample 
of n=100 service times without having to list them separately. ◭
The histogram, deﬁned below, is a graphical display of a collection of numbers. 
It is particularly useful for displaying the observed values of a collection of random 
variables that have been modeled as conditionally i.i.d. 
Deﬁnition
3.7.9 Histogram . Let x1, . . . , x nbe a collection of numbers that all lie between two values 
a < b . That is, a≤xi≤bfor all i=1, . . . , n . Choose some integer k≥1 and divide 
the interval [ a, b ] into kequal-length subintervals of length (b −a)/k . For each 
subinterval, count how many of the numbers x1, . . . , x nare in the subinterval. Let 
cibe the count for subinterval ifori=1, . . . , k . Choose a number r > 0. (Typically, 
r=1 or r=nor r=n(b−a)/k .) Draw a two-dimensional graph with the horizonal 
axis running from ato b. For each subinterval i=1, . . . , k draw a rectangular bar of 
width(b −a)/k and height equal to ci/r over the midpoint of the ith interval. Such 
a graph is called a histogram .
The choice of the number rin the deﬁnition of histogram depends on what one 
wishes to be displayed on the vertical axis. The shape of the histogram is identical 
regardless of what value one chooses for r. With r=1, the height of each bar is the raw 
count for each subinterval, and counts are displayed on the vertical axis. With r=n,
the height of each bar is the proportion of the set of numbers in each subinterval, 
and the vertical axis displays proportions. With r=n(b−a)/k , the area of each bar 
is the proportion of the set of numbers in each subinterval. 
Example 
3.7.20Rate of Service . The n=100 observed service times in Example 3.7.19 all lie between 
0 and 10. It is convenient, in this example, to draw a histogram with horizontal axis 
running from 0 to 10 and divided into 10 subintervals of length 1 each. Other choices 
are possible, but this one will do for illustration. Figure 3.22 contains the histogram of 
the 100 observed service times with r=100. One sees that the numbers of observed 
service times in the subintervals decrease as the center of the subinterval increses. 
This matches the behavior of the conditional p.d.f. g(x |z) of the service times as a 
function of xfor ﬁxed z. ◭
Histograms are useful as more than just graphical displays of large sets of num- 
bers. After we see the law of large numbers (Theorem 6.2.4), we can show that the 
Figure 3.22 Histogram
of service times for Exam- 
ple 3.7.20 with a=0, b=10, 
k=10, and r=100. 
0.05
00.100.150.200.250.30
2 4 6 8 10 
Time Proportion 
166 Chapter 3 Random Variables and Distributions 
histogram of a large (conditionally) i.i.d. sample of continuous random variables is 
an approximation to the (conditional) p.d.f. of the random variables in the sample, 
so long as one uses the third choice of r, namely, r=n(b−a)/k .
Note:MoreGeneralHistograms. Sometimes it is convenient to divide the range of 
the numbers to be plotted in a histogram into unequal-length subintervals. In such a 
case, one would typically let the height of each bar be ci/r i, where ciis the raw count 
andriis proportional to the length of the ith subinterval. In this way, the area of each 
bar is still proportional to the count or proportion in each subinterval. 
Summary
A ﬁnite collection of random variables is called a random vector. We have deﬁned 
joint distributions for arbitrary random vectors. Every random vector has a joint c.d.f. 
Continuous random vectors have a joint p.d.f. Discrete random vectors have a joint 
p.f. Mixed distribution random vectors have a joint p.f./p.d.f. The coordinates of an 
n-dimensional random vector Xare independent if the joint p.f., p.d.f., or p.f./p.d.f. 
f( x)factors into/producttextn
i=1fi(x i).
We can compute marginal distributions of subvectors of a random vector, and 
we can compute the conditional distribution of one subvector given the rest of the 
vector. We can construct a joint distribution for a random vector by piecing together 
a marginal distribution for part of the vector and a conditional distribution for the 
rest given the ﬁrst part. There are versions of Bayes’ theorem and the law of total 
probability for random vectors.
An n-dimensional random vector Xhas coordinates that are conditionally inde-
pendent given Zif the conditional p.f., p.d.f., or p.f./p.d.f. g( x|z)of XgivenZ=z
factors into/producttextn
i=1gi(x i|z). There are versions of Bayes’ theorem, the law of total 
probability, and all future theorems about random variables and random vectors 
conditional on an arbitrary additional random vector. 
Exercises 
1. Suppose that three random variables X1,X2, and X3
have a continuous joint distribution with the following 
joint p.d.f.: f(x 1, x 2, x 3)=
/braceleftbiggc(x 1+2x2+3x3)for 0≤xi≤1(i =1,2,3),
0 otherwise. 
Determine (a)the value of the constant c;
(b)the marginal joint p.d.f. of X1andX3; and 
(c)Pr /parenleftBig
X3<1
2/vextendsingle/vextendsingle/vextendsingleX1=1
4, X 2=3
4/parenrightBig
.
2. Suppose that three random variables X1,X2, and X3
have a mixed joint distribution with p.f./p.d.f.: 
f(x 1, x 2, x 3)
=

cx 1+x2+x3
1(1−x1)3−x2−x3if 0 < x 1<1
andx2, x 3∈{0,1},
0 otherwise. (Notice that X1has a continuous distribution and X2and
X3have discrete distributions.) Determine (a)the value of 
the constant c;(b)the marginal joint p.f. of X2andX3; and 
(c)the conditional p.d.f. of X1givenX2=1 and X3=1. 
3. Suppose that three random variables X1,X2, and X3
have a continuous joint distribution with the following 
joint p.d.f.: f(x 1, x 2, x 3)=
/braceleftbiggce −(x 1+2x2+3x3)forxi>0(i =1,2,3),
0 otherwise. 
Determine (a)the value of the constant c;(b)the marginal
joint p.d.f. of X1andX3; and (c)Pr (X 1<1|X2=2, X 3=1).
4. Suppose that a point ( X1,X2,X3) is chosen at random, 
that is, in accordance with the uniform p.d.f., from the 
following set S:
S={(x 1, x 2, x 3): 0 ≤xi≤1 for i=1,2,3}.
3.8 Functions of a Random Variable 167
Determine:
a. Pr /bracketleftbigg/parenleftBig
X1−1
2/parenrightBig2
+/parenleftBig
X2−1
2/parenrightBig2
+/parenleftBig
X3−1
2/parenrightBig2
≤1
4/bracketrightbigg
b. Pr (X 2
1+X2
2+X2
3≤1)
5. Suppose that an electronic system contains ncompo-
nents that function independently of each other and that 
the probability that component iwill function properly is 
pi(i =1, . . . , n) . It is said that the components are con- 
nectedinseries if a necessary and sufﬁcient condition for 
the system to function properly is that all ncomponents
function properly. It is said that the components are con- 
nectedinparallel if a necessary and sufﬁcient condition for 
the system to function properly is that at least one of the 
ncomponents functions properly. The probability that the 
system will function properly is called the reliability of the 
system. Determine the reliability of the system, (a)assum-
ing that the components are connected in series, and (b)
assuming that the components are connected in parallel. 
6. Suppose that the nrandom variables X1. . . , X nform a
random sample from a discrete distribution for which the 
p.f. is f. Determine the value of Pr (X 1=X2=. . . =Xn).
7. Suppose that the nrandom variables X1, . . . , X nform a
random sample from a continuous distribution for which 
the p.d.f. is f. Determine the probability that at least k
of these nrandom variables will lie in a speciﬁed interval 
a≤x≤b.
8. Suppose that the p.d.f. of a random variable Xis as 
follows:
f(x) =/braceleftBigg
1
n!xne−xforx > 0
0 otherwise. 
Suppose also that for any given value X=x (x > 0), the n
random variables Y1, . . . , Y nare i.i.d. and the conditional
p.d.f. gof each of them is as follows: 
g(y |x) =/braceleftBigg
1
xfor 0< y < x ,
0 otherwise. 
Determine (a)the marginal joint p.d.f. of Y1, . . . , Y nand
(b)the conditional p.d.f. of Xfor any given values of 
Y1, . . . , Y n.9. LetXbe a random variable with a continuous distribu- 
tion. Let X1=X2=X.
a. Prove that both X1andX2have a continuous distri- 
bution.
b. Prove thatX=(X 1, X 2)does not have a continuous 
joint distribution.
10.Return to the situation described in Example 3.7.18. 
LetX=(X 1, . . . , X 5)and compute the conditional p.d.f. 
of ZgivenX=xdirectly in one step, as if all of Xwere
observed at the same time. 
11.Suppose that X1, . . . , X nare independent. Let k < n 
and let i1, . . . , i kbe distinct integers between 1 and n.
Prove that Xi1, . . . , X ikare independent.
12.LetXbe a random vector that is split into three parts, 
X=(Y,Z,W). Suppose that Xhas a continuous joint 
distribution with p.d.f. f( y,z,w). Let g1(y,z|w)be the 
conditional p.d.f. of (Y,Z)givenW=w, and let g2(y|w)
be the conditional p.d.f. of YgivenW=w. Prove that 
g2(y|w)=/integraltext
g1(y,z|w) d z.
13.LetX1, X 2, X 3be conditionally independent given 
Z=zfor all zwith the conditional p.d.f. g(x |z) in Eq. 
(3.7.12). Also, let the marginal p.d.f. of Zbe f0in 
Eq. (3.7.11). Prove that the conditional p.d.f. of X3given
(X 1, X 2)=(x 1, x 2)is /integraltext∞
0g(x 3|z)g 0(z |x1, x 2) dz , where g0is 
deﬁned in Eq. (3.7.13). (You can prove this even if you 
cannot compute the integral in closed form.) 
14.Consider the situation described in Example 3.7.14. 
Suppose that X1=5 and X2=7 are observed. 
a. Compute the conditional p.d.f. of X3given(X 1, X 2)=
(5,7). (You may use the result stated in Exercise 12.) 
b. Find the conditional probability that X3>3 given 
(X 1, X 2)=(5,7)and compare it to the value of 
Pr (X 3>3)found in Example 3.7.9. Can you suggest 
a reason why the conditional probability should be 
higher than the marginal probability?
15.LetX1, . . . , X nbe independent random variables, and 
letWbe a random variable such that Pr (W =c) =1 for 
some constant c. Prove that X1, . . . , X nare conditionally
independent given W=c.
3.8 Functions of a Random Variable 
Oftenweﬁndthatafterwecomputethedistributionofarandomvariable X, we 
reallywantthedistributionofsomefunctionof X.Forexample,if Xistherateat 
whichcustomersareservedinaqueue,then 1/X istheaveragewaitingtime.Ifwe 
havethedistributionof X,weshouldbeabletodeterminethedistributionof 1/X 
orofanyotherfunctionof X.Howtodothatisthesubjectofthissection.
168 Chapter 3 Random Variables and Distributions 
Random Variable with a Discrete Distribution 
Example 
3.8.1Distance from the Middle . Let Xhave the uniform distribution on the integers 
1,2, . . . , 9. Suppose that we are interested in how far Xis from the middle of the 
distribution, namely, 5. We could deﬁne Y=|X−5|and compute probabilities such
as Pr (Y =1)=Pr (X ∈{4,6})=2/9. ◭
Example 3.8.1 illustrates the general procedure for ﬁnding the distribution of a 
function of a discrete random variable. The general result is straightforward. 
Theorem
3.8.1Function of a Discrete Random Variable . Let Xhave a discrete distribution with p.f. f,
and let Y=r(X) for some function of rdeﬁned on the set of possible values of X.
For each possible value yof Y, the p.f. gof Yis 
g(y) =Pr (Y =y) =Pr[ r(X) =y]=/summationdisplay
x:r(x) =yf(x). 
Example 
3.8.2Distance from the Middle . The possible values of Yin Example 3.8.1 are 0, 1, 2, 3, 
and 4. We see that Y=0 if and only if X=5, so g( 0)=f( 5)=1/9. For all other 
values of Y, there are two values of Xthat give that value of Y. For example, 
{Y=4}={X=1}∪{X=9}. So, g(y) =2/9 for y=1,2,3,4. ◭
Random Variable with a Continuous Distribution 
If a random variable Xhas a continuous distribution, then the procedure for deriving 
the probability distribution of a function of Xdiffers from that given for a discrete 
distribution. One way to proceed is by direct calculation as in Example 3.8.3. 
Example 
3.8.3Average Waiting Time . Let Zbe the rate at which customers are served in a queue, 
and suppose that Zhas a continuous c.d.f. F. The average waiting time is Y=1/Z .
If we want to ﬁnd the c.d.f. Gof Y, we can write 
G(y)=Pr (Y ≤y) =Pr /parenleftbigg1
Z≤y/parenrightbigg
=Pr /parenleftbigg
Z≥1
y/parenrightbigg
=Pr /parenleftbigg
Z > 1
y/parenrightbigg
=1−F/parenleftbigg1
y/parenrightbigg
,
where the fourth equality follows from the fact that Zhas a continuous distribution 
so that Pr (Z =1/y) =0. ◭
In general, suppose that the p.d.f. of Xis fand that another random variable is 
deﬁned as Y=r(X) . For each real number y, the c.d.f. G(y) of Ycan be derived as 
follows:
G(y)=Pr (Y ≤y) =Pr[ r(X) ≤y]
=/integraldisplay
{x:r(x) ≤y}f(x) dx. 
If the random variable Yalso has a continuous distribution, its p.d.f. gcan be obtained 
from the relation
g(y) =dG(y) 
dy .
This relation is satisﬁed at every point yat which Gis differentiable. 
3.8 Functions of a Random Variable 169
Figure 3.23 The p.d.f. of 
Y=X2in Example 3.8.4. 
y10g(y)
Example 
3.8.4Deriving the p.d.f. of X2whenXHas a Uniform Distribution . Suppose that Xhas the
uniform distribution on the interval [ −1,1], so 
f(x) =/braceleftbigg1/2 for −1≤x≤1, 
0 otherwise. 
We shall determine the p.d.f. of the random variable Y=X2.
SinceY=X2, then Ymust belong to the interval 0 ≤Y≤1. Thus, for each value 
of Ysuch that 0≤y≤1, the c.d.f. G(y) of Yis 
G(y)=Pr (Y ≤y) =Pr (X 2≤y) 
=Pr (−y1/2≤X≤y1/2)
=/integraldisplayy1/2
−y1/2f(x) dx =y1/2.
For 0 < y < 1, it follows that the p.d.f. g(y) of Yis 
g(y) =dG(y) 
dy =1
2y1/2.
This p.d.f. of Yis sketched in Fig. 3.23. It should be noted that although Yis 
simply the square of a random variable with a uniform distribution, the p.d.f. of Yis 
unbounded in the neighborhood of y=0. ◭
Linear functions are very useful transformations, and the p.d.f. of a linear func- 
tion of a continuous random variable is easy to derive. The proof of the following 
result is left to the reader in Exercise 5. 
Theorem
3.8.2Linear Function . Suppose that Xis a random variable for which the p.d.f. is fand that
Y=aX +b (a ×negationslash=0). Then the p.d.f. of Yis 
g(y) =1
|a|f/parenleftbiggy−b
a/parenrightbigg
for−∞< y < ∞, (3.8.1)
and 0 otherwise. 
The Probability Integral Transformation 
Example 
3.8.5LetXbe a continuous random variable with p.d.f. f(x) =exp(−x) forx > 0 and 0 
otherwise. The c.d.f. of Xis F(x) =1−exp(−x) forx > 0 and 0 otherwise. If we let 
170 Chapter 3 Random Variables and Distributions 
Fbe the function rin the earlier results of this section, we can ﬁnd the distribution 
of Y=F(X) . The c.d.f. or Yis, for 0 < y < 1, 
G(y)=Pr (Y ≤y) =Pr (1−exp(−X) ≤y) =Pr (X ≤− log(1−y)) 
=F( −log(1−y)) =1−exp(−[−log(1−y) ])=y, 
which is the c.d.f. of the uniform distribution on the interval [0 ,1]. It follows that Y
has the uniform distribution on the interval [0 ,1]. ◭
The result in Example 3.8.5 is quite general. 
Theorem
3.8.3Probability Integral Transformation . Let Xhave a continuous c.d.f. F, and let Y=F(X) .
(This transformation from Xto Yis called the probability integral transformation .) 
The distribution of Yis the uniform distribution on the interval [0 ,1]. 
Proof First, because Fis the c.d.f. of a random variable, then 0 ≤F(x) ≤1 for 
−∞< x < ∞. Therefore, Pr (Y < 0)=Pr (Y > 1)=0. Since Fis continuous, the set 
of xsuch that F(x) =yis a nonempty closed and bounded interval [ x0, x 1] for each y
in the interval (0,1). Let F−1(y) denote the lower endpoint x0of this interval, which 
was called the yquantile of Fin Deﬁnition 3.3.2. In this way, Y≤yif and only if 
X≤x1. Let Gdenote the c.d.f. of Y. Then 
G(y)=Pr (Y ≤y) =Pr (X ≤x1)=F(x 1)=y. 
Hence, G(y)=yfor 0< y < 1. Because this function is the c.d.f. of the uniform 
distribution on the interval [0 ,1], this uniform distribution is the distribution of Y.
Because Pr (X =F−1(Y)) =1 in the proof of Theorem 3.8.3, we have the following 
corollary.
Corollary 
3.8.1LetYhave the uniform distribution on the interval [0 ,1], and let Fbe a continuous 
c.d.f. with quantile function F−1. Then X=F−1(Y) has c.d.f. F.
Theorem 3.8.3 and its corollary give us a method for transforming an arbitrary 
continuous random variable Xinto another random variable Zwith any desired
continuous distribution. To be speciﬁc, let Xhave a continuous c.d.f. F, and let G
be another continuous c.d.f. Then Y=F(X) has the uniform distribution on the 
interval [0 ,1] according to Theorem 3.8.3, and Z=G−1(Y) has the c.d.f. Gaccording
to Corollary 3.8.1. Combining these, we see that Z=G−1[F(X) ] has c.d.f. G.
Simulation
Pseudo-Random Numbers Most computer packages that do statistical analyses 
also produce what are called pseudo-random numbers . These numbers appear to 
have some of the properties that a random sample would have, even though they 
are generated by deterministic algorithms. The most fundamental of these programs 
are the ones that generate pseudo-random numbers that appear to have the uniform 
distribution on the interval [0 ,1]. We shall refer to such functions as uniformpseudo-
random number generators . The important features that a uniform pseudo-random 
number generator must have are the following.
The numbers that it produces need to be spread somewhat uniformly over the 
interval [0 ,1], and they need to appear to be observed values of independent random 
3.8 Functions of a Random Variable 171
variables. This last feature is very complicated to word precisely. An example of a 
sequence that does notappear to be observations of independent random variables 
would be one that was perfectly evenly spaced. Another example would be one with 
the following behavior: Suppose that we look at the sequence X1, X 2, . . . one at a 
time, and every time we ﬁnd an Xi>0.5, we write down the next number Xi+1. If the 
subsequence of numbers that we write down is not spread approximately uniformly 
over the interval [0 ,1], then the original sequence does not look like observations 
of independent random variables with the uniform distribution on the interval [0 ,1]. 
The reason is that the conditional distribution of Xi+1given that Xi>0.5 is supposed 
to be uniform over the interval [0 ,1], according to independence. 
GeneratingPseudo-RandomNumbersHavingaSpeciﬁedDistri bution A uniform 
pseudo-random number generator can be used to generate values of a random 
variable Yhaving any speciﬁed continuous c.d.f. G. If a random variable Xhas the
uniform distribution on the interval [0 ,1] and if the quantile function G−1is deﬁned 
as before, then it follows from Corollary 3.8.1 that the c.d.f. of the random variable 
Y=G−1(X) will be G. Hence, if a value of Xis produced by a uniform pseudo- 
random number generator, then the corresponding value of Ywill have the desired
property. If nindependent values X1, . . . , X nare produced by the generator, then 
the corresponding values Y1, . . . , Y nwill appear to form a random sample of size n
from the distribution with the c.d.f. G.
Example 
3.8.6Generating Independent Values from a Speciﬁed p.d.f . Suppose that a uniform pseudo- 
random number generator is to be used to generate three independent values from 
the distribution for which the p.d.f. gis as follows: 
g(y) =/braceleftBigg
1
2(2−y) for 0< y < 2, 
0 otherwise. 
For 0 < y < 2, the c.d.f. Gof the given distribution is 
G(y)=y−y2
4.
Also, for 0 < x < 1, the inverse function y=G−1(x) can be found by solving the 
equation x=G(y) fory. The result is 
y=G−1(x) =2[1−(1−x) 1/2]. (3.8.2)
The next step is to generate three uniform pseudo-random numbers x1,x2, and x3
using the generator. Suppose that the three generated values are 
x1=0.4125, x 2=0.0894, x 3=0.8302.
When these values of x1,x2, and x3are substituted successively into Eq. (3.8.2),
the values of ythat are obtained are y1=0.47, y2=0.09, and y3=1.18. These are 
then treated as the observed values of three independent random variables with the 
distribution for which the p.d.f. is g. ◭
If Gis a general c.d.f., there is a method similar to Corollary 3.8.1 that can be 
used to transform a uniform random variable into a random variable with c.d.f. G.
See Exercise 12 in this section. There are other computer methods for generating 
values from certain speciﬁed distributions that are faster and more accurate than
using the quantile function. These topics are discussed in the books by Kennedy and 
172 Chapter 3 Random Variables and Distributions 
Gentle (1980) and Rubinstein (1981). Chapter 12 of this text contains techniques and 
examples that show how simulation can be used to solve statistical problems. 
GeneralFunction In general, if Xhas a continuous distribution and if Y=r(X) ,
then it is not necessarily true that Ywill also have a continuous distribution. For ex- 
ample, suppose that r(x) =c, where cis a constant, for all values of xin some interval 
a≤x≤b, and that Pr (a ≤X≤b) > 0. Then Pr (Y =c) > 0. Since the distribution of Y
assigns positive probability to the value c, this distribution cannot be continuous. In 
order to derive the distribution of Yin a case like this, the c.d.f. of Ymust be derived 
by applying methods like those described above. For certain functions r, however, 
the distribution of Ywill be continuous; and it will then be possible to derive the 
p.d.f. of Ydirectly without ﬁrst deriving its c.d.f. We shall develop this case in detail 
at the end of this section. 
Direct Derivation of the p.d.f. When ris One-to-One and Differentiable 
Example 
3.8.7Average Waiting Time . Consider Example 3.8.3 again. The p.d.f. gof Ycan be com- 
puted from G(y)=1−F( 1/y) because Fand 1/y both have derivatives at enough 
places. We apply the chain rule for differentiation to obtain 
g(y) =dG(y) 
dy =− dF(x) 
dx /vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=1/y /parenleftbigg
−1
y2/parenrightbigg
=f/parenleftbigg1
y/parenrightbigg1
y2,
except at y=0 and at those values of ysuch that F(x) is not differentiable at x=1/y .
◭
DifferentiableOne-To-OneFunctions The method used in Example 3.8.7 general- 
izes to very arbitrary differentiable one-to-one functions. Before stating the general 
result, we should recall some properties of differentiable one-to-one functions from 
calculus. Let rbe a differentiable one-to-one function on the open interval (a, b) .
Then ris either strictly increasing or strictly decreasing. Because ris also continu- 
ous, it will map the interval (a, b) to another open interval (α, β) , called the imageof 
(a, b) underr. That is, for each x∈(a, b) ,r(x) ∈(α, β) , and for each y∈(α, β) there is 
x∈(a, b) such that y=r(x) and this yis unique because ris one-to-one. So the inverse 
sof rwill exist on the interval (α, β) , meaning that for x∈(a, b) andy∈(α, β) we 
haver(x) =yif and only if s(y) =x. The derivative of swill exist (possibly inﬁnite),
and it is related to the derivative of rby 
ds(y) 
dy =/parenleftBigg
dr(x) 
dx /vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=s(y) /parenrightBigg−1
.
Theorem
3.8.4LetXbe a random variable for which the p.d.f. is fand for which Pr (a < X < b) =1. 
(Here, aand/or bcan be either ﬁnite or inﬁnite.) Let Y=r(X) , and suppose that r(x) 
is differentiable and one-to-one for a < x < b . Let (α, β) be the image of the interval 
(a, b) under the function r. Let s(y) be the inverse function of r(x) forα < y < β .
Then the p.d.f. gof Yis 
g(y) =

f[s(y) ]/vextendsingle/vextendsingle/vextendsingle/vextendsingleds(y) 
dy /vextendsingle/vextendsingle/vextendsingle/vextendsingleforα < y < β ,
0 otherwise. (3.8.3)
3.8 Functions of a Random Variable 173
Proof If ris increasing, then sis increasing, and for each y∈(α, β) ,
G(y)=Pr (Y ≤y) =Pr[ r(X) ≤y]=Pr[ X≤s(y) ]=F[s(y) ].
It follows that Gis differentiable at all ywhere both sis differentiable and where 
F(x) is differentiable at x=s(y) . Using the chain rule for differentiation, it follows 
that the p.d.f. g(y) forα < y < β will be 
g(y) =dG(y) 
dy =dF [s(y) ]
dy =f[s(y) ]ds(y) 
dy . (3.8.4)
Because sis increasing, ds(y)/dy is positive; hence, it equals |ds(y)/dy |and Eq.
(3.8.4) implies Eq. (3.8.3). Similarly, if ris decreasing, then sis decreasing, and for 
eachy∈(α, β) ,
G(y)=Pr[ r(X) ≤y]=Pr[ X≥s(y) ]=1−F[s(y) ].
Using the chain rule again, we differentiate Gto get the p.d.f. of Y
g(y) =dG(y) 
dy =− f[s(y) ]ds(y) 
dy . (3.8.5)
Sincesis strictly decreasing, ds(y)/dy is negative so that −ds(y)/dy equals|ds(y)/ 
dy |. It follows that Eq. (3.8.5) implies Eq. (3.8.3). 
Example 
3.8.8Microbial Growth . A popular model for populations of microscopic organisms in 
large environments is exponential growth. At time 0, suppose that vorganisms are
introduced into a large tank of water, and let Xbe the rate of growth. After time 
t, we would predict a population size of ve Xt . Assume that Xis unknown but has a 
continuous distribution with p.d.f. 
f(x) =/braceleftbigg3(1−x) 2for 0< x < 1, 
0 otherwise. 
We are interested in the distribution of Y=ve Xt for known values of vandt. For 
concreteness, let v=10 and t=5, so that r(x) =10 e5x.
In this example, Pr (0< X < 1)=1 and ris a continuous and strictly increasing 
function of xfor 0< x < 1. As xvaries over the interval (0,1), it is found that 
y=r(x) varies over the interval (10 ,10 e5). Furthermore, for 10 < y < 10 e5, the 
inverse function is s(y) =log(y/ 10 )/ 5. Hence, for 10 < y < 10 e5,
ds(y) 
dy =1
5y.
It follows from Eq. (3.8.3) that g(y) will be 
g(y) =

3(1−log(y/ 10 )/ 5)2
5yfor 10 < y < 10 e5,
0 otherwise. ◭
Summary
We learned several methods for determining the distribution of a function of a 
random variable. For a random variable Xwith a continuous distribution having 
p.d.f. f, if ris strictly increasing or strictly decreasing with differentiable inverse 
s(i.e., s(r(x)) =xandsis differentiable), then the p.d.f. of Y=r(X) is g(y) =
174 Chapter 3 Random Variables and Distributions 
f(s(y)) |ds(y)/dy |. A special transformation allows us to transform a random variable 
Xwith the uniform distribution on the interval [0 ,1]into a random variable Ywith an 
arbitrary continuous c.d.f. Gby Y=G−1(X) . This method can be used in conjunction 
with a uniform pseudo-random number generator to generate random variables with 
arbitrary continuous distributions.
Exercises 
1. Suppose that the p.d.f. of a random variable Xis as 
follows:
f(x) =/braceleftbigg3x2for 0< x < 1, 
0 otherwise. 
Also, suppose that Y=1−X2. Determine the p.d.f. of Y.
2. Suppose that a random variable Xcan have each of the 
seven values−3,−2,−1,0,1,2,3 with equal probability. 
Determine the p.f. of Y=X2−X.
3. Suppose that the p.d.f. of a random variable Xis as 
follows:
f(x) =/braceleftBigg
1
2xfor 0< x < 2, 
0 otherwise. 
Also, suppose that Y=X( 2−X) . Determine the c.d.f. and 
the p.d.f. of Y.
4. Suppose that the p.d.f. of Xis as given in Exercise 3. 
Determine the p.d.f. of Y=4−X3.
5. Prove Theorem 3.8.2. ( Hint: Either apply Theorem 
3.8.4 or ﬁrst compute the c.d.f. seperately for a > 0 and 
a < 0.) 
6. Suppose that the p.d.f. of Xis as given in Exercise 3. 
Determine the p.d.f. of Y=3X+2. 
7. Suppose that a random variable Xhas the uniform
distribution on the interval [0 ,1]. Determine the p.d.f. of 
(a)X2,(b)−X3, and (c)X1/2.
8. Suppose that the p.d.f. of Xis as follows: 
f(x) =/braceleftbigge−xforx > 0, 
0 for x≤0. 
Determine the p.d.f. of Y=X1/2.
9. Suppose that Xhas the uniform distribution on the 
interval [0 ,1]. Construct a random variable Y=r(X) for
which the p.d.f. will be 
g(y) =/braceleftBigg
3
8y2for 0< y < 2, 
0 otherwise. 10.LetXbe a random variable for which the p.d.f fis as 
given in Exercise 3. Construct a random variable Y=r(X) 
for which the p.d.f. gis as given in Exercise 9. 
11.Explain how to use a uniform pseudo-random number 
generator to generate four independent values from a 
distribution for which the p.d.f. is 
g(y) =/braceleftBigg
1
2(2y+1)for 0< y < 1, 
0 otherwise. 
12.LetFbe an arbitrary c.d.f. (not necessarily discrete, 
not necessarily continuous, not necessarily either). Let 
F−1be the quantile function from Deﬁnition 3.3.2. Let X
have the uniform distribution on the interval [0 ,1]. Deﬁne 
Y=F−1(X) . Prove that the c.d.f. of Yis F.Hint: Compute
Pr (Y ≤y) in two cases. First, do the case in which yis the 
unique value of xsuch that F(x) =F(y) . Second, do the 
case in which there is an entire interval of xvalues such
thatF(x) =F(y) .
13.LetZbe the rate at which customers are served in a 
queue. Assume that Zhas the p.d.f. 
f(z) =/braceleftbigg2e−2zforz > 0, 
0 otherwise. 
Find the p.d.f. of the average waiting time T=1/Z .
14.LetXhave the uniform distribution on the interval 
[a, b ], and let c > 0. Prove that cX +dhas the uniform
distribution on the interval [ ca +d, cb +d]. 
15.Most of the calculation in Example 3.8.4 is quite gen- 
eral. Suppose that Xhas a continuous distribution with 
p.d.f. f. Let Y=X2, and show that the p.d.f. of Yis 
g(y) =1
2y1/2[f(y 1/2)+f( −y1/2)].
16.In Example 3.8.4, the p.d.f. of Y=X2is much larger 
for values of ynear 0 than for values of ynear 1 despite 
the fact that the p.d.f. of Xis ﬂat. Give an intuitive reason 
why this occurs in this example. 
17.An insurance agent sells a policy which has a $100 de- 
ductible and a $5000 cap. This means that when the policy 
holder ﬁles a claim, the policy holder must pay the ﬁrst 
3.9 Functions of Two or More Random Variables 175
$100. After the ﬁrst $100, the insurance company pays the
rest of the claim up to a maximum payment of $5000. Any 
excess must be paid by the policy holder. Suppose that the 
dollar amount Xof a claim has a continuous distribution 
with p.d.f. f(x) =1/( 1+x) 2forx > 0 and 0 otherwise. Let 
Ybe the amount that the insurance company has to pay 
on the claim. a. WriteYas a function of X, i.e., Y=r(X) .
b. Find the c.d.f. of Y.
c. Explain why Yhas neither a continuous nor a dis- 
crete distribution.
3.9 Functions of Two or More Random Variables 
When we observe data consisting of the values of several random variables, we 
need to summarize the observed values in order to be able to focus on the infor- 
mation in the data. Summarizing consists of constructing one or a few functions 
of the random variables that capture the bulk of the information. In this section,
we describe the techniques needed to determine the distribution of a function of 
twoormorerandomvariables.
Random Variables with a Discrete Joint Distribution 
Example 
3.9.1Bull Market . Three different investment ﬁrms are trying to advertise their mutual 
funds by showing how many perform better than a recognized standard. Each com- 
pany has 10 funds, so there are 30 in total. Suppose that the ﬁrst 10 funds belong to the 
ﬁrst ﬁrm, the next 10 to the second ﬁrm, and the last 10 to the third ﬁrm. Let Xi=1
if fund iperforms better than the standard and Xi=0 otherwise, for i=1, . . . , 30. 
Then, we are interested in the three functions 
Y1=X1+. . . +X10 ,
Y2=X11 +. . . +X20 ,
Y3=X21 +. . . +X30 .
We would like to be able to determine the joint distribution of Y1,Y2, and Y3from
the joint distribution of X1, . . . , X 30 . ◭
The general method for solving problems like those of Example 3.9.1 is a straight- 
forward extension of Theorem 3.8.1. 
Theorem
3.9.1Functions of Discrete Random Variables . Suppose that nrandom variables X1, . . . , X n
have a discrete joint distribution for which the joint p.f. is f, and that mfunctions
Y1, . . . , Y mof these nrandom variables are deﬁned as follows: 
Y1=r1(X 1, . . . , X n), 
Y2=r2(X 1, . . . , X n), 
...
Ym=rm(X 1, . . . , X n). 
176 Chapter 3 Random Variables and Distributions 
For given values y1, . . . , y mof the mrandom variables Y1, . . . , Y m, let Adenote the
set of all points (x 1, . . . , x n)such that
r1(x 1, . . . , x n)=y1,
r2(x 1, . . . , x n)=y2,
...
rm(x 1, . . . , x n)=ym.
Then the value of the joint p.f. gof Y1, . . . , Y mis speciﬁed at the point (y 1, . . . , y m)
by the relation 
g(y 1, . . . , y m)=/summationdisplay
(x 1,...,x n)∈Af(x 1, . . . , x n). 
Example 
3.9.2Bull Market . Recall the situation in Example 3.9.1. Suppose that we want the joint 
p.f. gof (Y 1, Y 2, Y 3)at the point (3,5,8). That is, we want g( 3,5,8)=Pr (Y 1=3, Y 2=
5, Y 3=8). The set Aas deﬁned in Theorem 3.9.1 is 
A={(x 1, . . . , x 30 ):x1+. . . +x10 =3, x 11 +. . . +x20 =5, x 21 +. . . +x30 =8}.
Two of the points in the set Aare
(1,1,1,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0), 
(1,0,0,0,1,0,0,1,0,0,0,1,1,0,0,1,0,1,0,1,1,0,1,1,1,0,1,1,1,1). 
A counting argument like those developed in Sec. 1.8 can be used to discover that 
there are/parenleftbigg10 
3/parenrightbigg/parenleftbigg 10 
5/parenrightbigg/parenleftbigg 10 
8/parenrightbigg
=1,360,800
points in A. Unless the joint distribution of X1, . . . , X 30 has some simple structure,
it will be extremely tedious to compute g( 3,5,8)as well as most other values of g.
For example, if all of the 2 30 possible values of the vector (X 1, . . . , X 30 )are equally
likely, then 
g( 3,5,8)=1,360,800
230 =1.27 ×10 −3. ◭
The next result gives an important example of a function of discrete random variables. 
Theorem
3.9.2Binomial and Bernoulli Distributions . Assume that X1, . . . , X nare i.i.d. random vari-
ables having the Bernoulli distribution with parameter p. Let Y=X1+. . . +Xn.
Then Yhas the binomial distribution with parameters nandp.
Proof It is clear that Y=yif and only if exactly yof X1, . . . , X nequal 1 and the 
othern−yequal 0. There are /parenleftbign
y/parenrightbig
distinct possible values for the vector (X 1, . . . , X n)
that have yones and n−yzeros. Each such vector has probability py(1−p) n−yof 
being observed; hence the probability that Y=yis the sum of the probabilities of 
those vectors, namely, /parenleftbign
y/parenrightbig
py(1−p) n−yfory=0, . . . , n . From Deﬁnition 3.1.7, we 
see that Yhas the binomial distribution with parameters nandp.
Example 
3.9.3Sampling Parts . Suppose that two machines are producing parts. For i=1,2, the 
probability is pithat machine iwill produce a defective part, and we shall assume 
that all parts from both machines are independent. Assume that the ﬁrst n1parts
are produced by machine 1 and that the last n2parts are produced by machine 2, 
3.9 Functions of Two or More Random Variables 177
withn=n1+n2being the total number of parts sampled. Let Xi=1 if the ith part 
is defective and Xi=0 otherwise for i=1, . . . , n . Deﬁne Y1=X1+. . . +Xn1and
Y2=Xn1+1+. . . +Xn. These are the total numbers of defective parts produced by 
each machine. The assumptions stated in the problem allow us to conclude that Y1and
Y2are independent according to the note about separate functions of independent 
random variables on page 140. Furthermore, Theorem 3.9.2 says that Yjhas the
binomial distribution with parameters njandpjforj=1,2. These two marginal 
distributions, together with the fact that Y1andY2are independent, give the entire
joint distribution. So, for example, if gis the joint p.f. of Y1andY2, we can compute 
g(y 1, y 2)=/parenleftbiggn1
y1/parenrightbigg
py1
1(1−p1)n1−y1/parenleftbiggn2
y2/parenrightbigg
py2
2(1−p) n2−y2,
fory1=0, . . . , n 1andy2=0, . . . , n 2, while g(y 1, y 2)=0 otherwise. There is no need 
to ﬁnd a set Aas in Example 3.9.2, because of the simplifying structure of the joint 
distribution of X1, . . . , X n. ◭
Random Variables with a Continuous Joint Distribution 
Example 
3.9.4Total Service Time . Suppose that the ﬁrst two customers in a queue plan to leave 
together. Let Xibe the time it takes to serve customer ifori=1,2. Suppose also that 
X1andX2are independent random variables with common distribution having p.d.f. 
f(x) =2e−2xforx > 0 and 0 otherwise. Since the customers will leave together, they 
are interested in the total time it takes to serve both of them, namely, Y=X1+X2.
We can now ﬁnd the p.d.f. of Y.
For each y, let 
Ay={(x 1, x 2):x1+x2≤y}.
Then Y≤yif and only if (X 1, X 2)∈Ay. The set Ayis pictured in Fig. 3.24. If we let 
G(y) denote the c.d.f. of Y, then, for y > 0, 
G(y)=Pr ((X1, X 2)∈Ay)=/integraldisplayy
0/integraldisplayy−x2
04e−2x1−2x2dx 1dx 2
=/integraldisplayy
02e−2x2/bracketleftBig
1−e−2(y −x2)/bracketrightBig
dx 2=/integraldisplayy
0/bracketleftBig
2e−2x2−2e−2y/bracketrightBig
dx 2
=1−e−2y−2ye −2y.
Figure3.24 The set Ayin 
Example 3.9.4 and in the 
proof of Theorem 3.9.4. 
Ayy
y
178 Chapter 3 Random Variables and Distributions 
Taking the derivative of G(y) with respect to y, we get the p.d.f. 
g(y) =d
dy /bracketleftBig
1−e−2y−ye −2y/bracketrightBig
=4ye −2y,
fory > 0 and 0 otherwise. ◭
The transformation in Example 3.9.4 is an example of a brute-force method that is 
always available for ﬁnding the distribution of a function of several random variables, 
however, it might be difﬁcult to apply in individual cases. 
Theorem
3.9.3Brute-Force Distribution of a Function . Suppose that the joint p.d.f. of X=(X 1, . . . , X n)
is f( x)and that Y=r( X). For each real number y, deﬁne Ay={x:r( x)≤y}. Then 
the c.d.f. G(y) of Yis 
G(y)=/integraldisplay
. . . 
Ay/integraldisplay
f( x) d x. (3.9.1)
Proof From the deﬁnition of c.d.f., 
G(y)=Pr (Y ≤y) =Pr[ r( X)≤y]=Pr (X∈Ay), 
which equals the right side of Eq. (3.9.1) by Deﬁnition 3.7.3. 
If the distribution of Yalso is continuous, then the p.d.f. of Ycan be found by 
differentiating the c.d.f. G(y) .
A popular special case of Theorem 3.9.3 is the following. 
Theorem
3.9.4Linear Function of Two Random Variables . Let X1andX2have joint p.d.f. f(x 1, x 2),
and let Y=a1X1+a2X2+bwitha1×negationslash=0. Then Yhas a continuous distribution whose 
p.d.f. is 
g(y) =/integraldisplay∞
−∞f/parenleftbiggy−b−a2x2
a1, x 2/parenrightbigg1
|a1|dx 2. (3.9.2)
Proof First, we shall ﬁnd the c.d.f. Gof Ywhose derivative we will see is the function 
gin Eq. (3.9.2). For each y, let Ay={(x 1, x 2):a1x1+a2x2+b≤y}. The set Ayhas
the same general form as the set in Fig. 3.24. We shall write the integral over the set 
Aywithx2in the outer integral and x1in the inner integral. Assume that a1>0. The 
other case is similar. According to Theorem 3.9.3, 
G(y)=/integraldisplay
Ay/integraldisplay
f(x 1, x 2)dx 1dx 2=/integraldisplay∞
−∞/integraldisplay(y −b−a2x2)/a1
−∞f(x 1, x 2)dx 1dx 2.(3.9.3)
For the inner integral, perform the change of variable z=a1x1+a2x2+bwhose
inverse is x1=(z −b−a2x2)/a1, so that dx 1=dz/a 1. The inner integral, after this 
change of variable, becomes 
/integraldisplayy
−∞f/parenleftbiggz−b−a2x2
a1, x 2/parenrightbigg1
a1dz. 
We can now substitute this expression for the inner integral into Eq. (3.9.3): 
G(y)=/integraldisplay∞
−∞/integraldisplayy
−∞f/parenleftbiggz−b−a2x2
a1, x 2/parenrightbigg1
a1dzdx 2
=/integraldisplayy
−∞/integraldisplay∞
−∞f/parenleftbiggz−b−a2x2
a1, x 2/parenrightbigg1
a1dx 2dz. (3.9.4)
3.9 Functions of Two or More Random Variables 179
Letg(z) denote the inner integral on the far right side of Eq. (3.9.4). Then we have 
G(y)=/integraltexty
−∞g(z)dz , whose derivative is g(y) , the function in Eq. (3.9.2). 
The special case of Theorem 3.9.4 in which X1andX2are independent, a1=a2=1, 
andb=0 is called convolution .
Deﬁnition
3.9.1Convolution . Let X1andX2be independent continuous random variables and let 
Y=X1+X2. The distribution of Yis called the convolution of the distributions of 
X1andX2. The p.d.f. of Yis sometimes called the convolution of the p.d.f.’s of X1and
X2.
If we let the p.d.f. of Xibe fifori=1,2 in Deﬁnition 3.9.1, then Theorem 3.9.4 (with 
a1=a2=1 and b=0) says that the p.d.f. of Y=X1+X2is 
g(y) =/integraldisplay∞
−∞f1(y −z)f 2(z)dz. (3.9.5)
Equivalently, by switching the names of X1andX2, we obtain the alternative form 
for the convolution:
g(y) =/integraldisplay∞
−∞f1(z)f 2(y −z) dz. (3.9.6)
The p.d.f. found in Example 3.9.4 is the special case of (3.9.5) with f1(x) =f2(x) =
2e−2xforx > 0 and 0 otherwise. 
Example 
3.9.5An Investment Portfolio . Suppose that an investor wants to purchase both stocks and 
bonds. Let X1be the value of the stocks at the end of one year, and let X2be the 
value of the bonds at the end of one year. Suppose that X1andX2are independent.
LetX1have the uniform distribution on the interval [1000 ,4000], and let X2have the
uniform distribution on the interval [800 ,1200]. The sum Y=X1+X2is the value at 
the end of the year of the portfolio consisting of both the stocks and the bonds. We 
shall ﬁnd the p.d.f. of Y. The function f1(z)f 2(y −z) in Eq. (3.9.6) is 
f1(z)f 2(y −z) =

8.333×10 −7for 1000≤z≤4000
and 800≤y−z≤1200, 
0 otherwise. (3.9.7)
We need to integrate the function in Eq. (3.9.7) over zfor each value of yto get 
the marginal p.d.f. of Y. It is helpful to look at a graph of the set of (y, z) pairs for
which the function in Eq. (3.9.7) is positive. Figure 3.25 shows the region shaded. For 
1800< y ≤2200, we must integrate zfrom 1000 to y−800. For 2200 < y ≤4800, we 
must integrate zfromy−1200 to y−800. For 4800 < y < 5200, we must integrate z
fromy−1200 to 4000. Since the function in Eq. (3.9.7) is constant when it is positive, 
the integral equals the constant times the length of the interval of zvalues. So, the 
p.d.f. of Yis 
g(y) =

8.333×10 −7(y −1800)for 1800 < y ≤2200, 
3.333×10 −4for 2200 < y ≤4800, 
8.333×10 −7(5200−y) for 4800 < y < 5200, 
0 otherwise. ◭
As another example of the brute-force method, we consider the largest and 
smallest observations in a random sample. These functions give an idea of how spread 
out the sample is. For example, meteorologists often report record high and low 
180 Chapter 3 Random Variables and Distributions 
Figure 3.25 The region 
where the function in 
Eq. (3.9.7) is positive. 
yz
2000 3000 4000 50001000
0150020002500300035004000
temperatures for speciﬁc days as well as record high and low rainfalls for months 
and years.
Example 
3.9.6Maximum and Minimum of a Random Sample . Suppose that X1, . . . , X nform a random 
sample of size nfrom a distribution for which the p.d.f. is fand the c.d.f. is F. The 
largest value Ynand the smallest value Y1in the random sample are deﬁned as follows: 
Yn=max{X1, . . . , X n},
Y1=min{X1, . . . , X n}. (3.9.8)
Consider Ynﬁrst. Let Gnstand for its c.d.f., and let gnbe its p.d.f. For every given 
value of y ( −∞< y < ∞),
Gn(y) =Pr (Y n≤y) =Pr (X 1≤y, X 2≤y, . . . , X n≤y) 
=Pr (X 1≤y) Pr (X 2≤y) . . . Pr (X n≤y) 
=F(y)F(y) . . . F(y) =[F(y) ]n,
where the third equality follows from the fact that the Xiare independent and
the fourth follows from the fact that all of the Xihave the same c.d.f. F. Thus, 
Gn(y) =[F(y) ]n.
Now,gncan be determined by differentiating the c.d.f. Gn. The result is 
gn(y) =n[F(y) ]n−1f(y) for−∞< y < ∞.
Next, consider Y1with c.d.f. G1and p.d.f. g1. For every given value of y ( −∞<
y < ∞),
G1(y) =Pr (Y 1≤y) =1−Pr (Y 1> y) 
=1−Pr (X 1> y, X 2> y, . . . , X n> y) 
=1−Pr (X 1> y) Pr (X 2> y) . . . Pr (X n> y) 
=1−[1 −F(y) ][1 −F(y) ]. . . [1 −F(y) ]
=1−[1 −F(y) ]n.
Thus, G1(y) =1−[1 −F(y) ]n.
Then g1can be determined by differentiating the c.d.f. G1. The result is 
g1(y) =n[1 −F(y) ]n−1f(y) for−∞< y < ∞.
3.9 Functions of Two or More Random Variables 181
Figure3.26 The p.d.f. of the 
uniform distribution on the 
interval [0 ,1] together with 
the p.d.f.’s of the minimum 
and maximum of samples 
of size n=5. The p.d.f. of 
the range of a sample of size 
n=5 (see Example 3.9.7) is 
also included.
xp.d.f.
0.2 0.4 0.6 0.8 1.01
02345Single random variable
Minimum of 5
Maximum of 5
Range of 5
Figure 3.26 shows the p.d.f. of the uniform distribution on the interval [0 ,1] 
together with the p.d.f.’s of Y1andYnfor the case n=5. It also shows the p.d.f. of 
Y5−Y1, which will be derived in Example 3.9.7. Notice that the p.d.f. of Y1is highest 
near 0 and lowest near 1, while the opposite is true of the p.d.f. of Yn, as one would 
expect.
Finally, we shall determine the joint distribution of Y1andYn. For every pair 
of values (y 1, y n)such that−∞< y 1< y n<∞, the event {Y1≤y1}∩{Yn≤yn}is the 
same as {Yn≤yn}∩{Y1> y 1}c. If Gdenotes the bivariate joint c.d.f. of Y1andYn, then 
G(y1, y n)=Pr (Y 1≤y1andYn≤yn)
=Pr (Y n≤yn)−Pr (Y n≤ynandY1> y 1)
=Pr (Y n≤yn)
−Pr (y 1< X 1≤yn, y 1< X 2≤yn, . . . , y 1< X n≤yn)
=Gn(y n)−n/productdisplay
i=1Pr (y 1< X i≤yn)
=[F(y n)]n−[F(y n)−F(y 1)]n.
The bivariate joint p.d.f. gof Y1andYncan be found from the relation 
g(y 1, y n)=∂2G(y 1, y n)
∂y 1∂y n.
Thus, for −∞< y 1< y n<∞,
g(y 1, y n)=n(n−1)[F(y n)−F(y 1)]n−2f(y 1)f(y n). (3.9.9)
Also, for all other values of y1andyn,g(y 1,yn)=0. ◭
A popular way to describe how spread out is a random sample is to use the 
distance from the minimum to the maximum, which is called the range of the random 
sample. We can combine the result from the end of Example 3.9.6 with Theorem 3.9.4 
to ﬁnd the p.d.f. of the range. 
Example 
3.9.7The Distribution of the Range of a Random Sample . Consider the same situation as in 
Example 3.9.6. The random variable W=Yn−Y1is called the range of the sample. 
The joint p.d.f. g(y 1, y n)of Y1andYnwas presented in Eq. (3.9.9). We can now apply 
Theorem 3.9.4 with a1=− 1, a2=1, and b=0 to get the p.d.f. hof W:
182 Chapter 3 Random Variables and Distributions 
h(w)=/integraldisplay∞
−∞g(y n−w, y n)dy n=/integraldisplay∞
−∞g(z, z +w)dz, (3.9.10)
where, for the last equality, we have made the change of variable z=yn−w.◭
Here is a special case in which the integral of Eq. 3.9.10 can be computed in 
closed form.
Example 
3.9.8The Range of a Random Sample from a Uniform Distribution . Suppose that the nrandom
variables X1, . . . , X nform a random sample from the uniform distribution on the 
interval [0 ,1]. We shall determine the p.d.f. of the range of the sample. 
In this example, 
f(x) =/braceleftbigg1 for 0 < x < 1,
0 otherwise, 
Also,F(x) =xfor 0< x < 1. We can write g(y 1, y n)from Eq. (3.9.9) in this case as 
g(y 1, y n)=/braceleftbigg
n(n−1)(yn−y1)n−2for 0< y 1< y n<1, 
0 otherwise. 
Therefore, in Eq. (3.9.10), g(z, z +w) =0 unless 0 < w < 1 and 0 < z < 1−w. For 
values of wandzsatisfying these conditions, g(z, w +z) =n(n−1)w n−2. The p.d.f. 
in Eq. (3.9.10) is then, for 0 < w < 1, 
h(w)=/integraldisplay1−w
0n(n−1)w n−2dz =n(n−1)w n−2(1−w). 
Otherwise, h(w)=0. This p.d.f. is shown in Fig. 3.26 for the case n=5. ◭
Direct Transformation of a Multivariate p.d.f. 
Next, we state without proof a generalization of Theorem 3.8.4 to the case of several 
random variables. The proof of Theorem 3.9.5 is based on the theory of differentiable 
one-to-one transformations in advanced calculus. 
Theorem
3.9.5Multivariate Transformation . Let X1, . . . , X nhave a continuous joint distribution 
for which the joint p.d.f. is f. Assume that there is a subset Sof Rnsuch that
Pr[ (X 1, . . . , X n)∈S]=1. Deﬁne nnew random variables Y1, . . . , Y nas follows: 
Y1=r1(X 1, . . . , X n), 
Y2=r2(X 1, . . . , X n), 
...
Yn=rn(X 1, . . . , X n), (3.9.11)
where we assume that the nfunctions r1, . . . , r ndeﬁne a one-to-one differentiable 
transformation of Sonto a subset Tof Rn. Let the inverse of this transformation be 
given as follows: 
x1=s1(y 1, . . . , y n), 
x2=s2(y 1, . . . , y n), 
...
xn=sn(y 1, . . . , y n). (3.9.12)
3.9 Functions of Two or More Random Variables 183
Then the joint p.d.f. gof Y1, . . . , Y nis 
g(y 1, . . . , y n)=/braceleftbiggf(s 1, . . . , s n)|J|for(y 1, . . . , y n)∈T,
0 otherwise, (3.9.13)
where Jis the determinant 
J=det
∂s 1
∂y 1. . . ∂s 1
∂y n .........∂s n
∂y 1. . . ∂s n
∂y n

and|J|denotes the absolute value of the determinant J.
Thus, the joint p.d.f. g(y 1, . . . , y n)is obtained by starting with the joint p.d.f. 
f(x 1, . . . , x n), replacing each value xiby its expression si(y 1, . . . , y n)in terms of 
y1, . . . , y n, and then multiplying the result by |J|. This determinant Jis called the 
Jacobian of the transformation speciﬁed by the equations in (3.9.12). 
Note:TheJacobianIsaGeneralizationoftheDerivativeofth eInverse. Eqs. (3.8.3) 
and (3.9.13) are very similar. The former gives the p.d.f. of a single function of a 
single random variable. Indeed, if n=1 in (3.9.13), J=ds 1(y 1)/dy1and Eq. (3.9.13)
becomes the same as (3.8.3). The Jacobian merely generalizes the derivative of the 
inverse of a single function of one variable to nfunctions of nvariables.
Example 
3.9.9The Joint p.d.f. of the Quotient and the Product of Two Random Variables . Suppose that 
two random variables X1andX2have a continuous joint distribution for which the 
joint p.d.f. is as follows: 
f(x 1, x 2)=/braceleftbigg4x1x2for 0< x 1<1 and 0 < x 2<1, 
0 otherwise. 
We shall determine the joint p.d.f. of two new random variables Y1andY2, which are 
deﬁned by the relations 
Y1=X1
X2andY2=X1X2.
In the notation of Theorem 3.9.5, we would say that Y1=r1(X 1, X 2)andY2=
r2(X 1, X 2), where 
r1(x 1, x 2)=x1
x2andr2(x 1, x 2)=x1x2. (3.9.14)
The inverse of the transformation in Eq. (3.9.14) is found by solving the equations 
y1=r1(x 1, x 2)andy2=r2(x 1, x 2)forx1andx2in terms of y1andy2. The result is 
x1=s1(y 1, y 2)=(y 1y2)1/2,
x2=s2(y 1, y 2)=/parenleftbiggy2
y1/parenrightbigg1/2
.(3.9.15)
LetSdenote the set of points ( x1,x2) such that 0 < x 1<1 and 0 < x 2<1, so that 
Pr[ (X 1, X 2)∈S]=1. Let Tbe the set of (y 1, y 2)pairs such that (y 1, y 2)∈Tif and only 
if (s 1(y 1, y 2), s 2(y 1, y 2)) ∈S. Then Pr[ (Y 1, Y 2)∈T]=1. The transformation deﬁned by 
the equations in (3.9.14) or, equivalently, by the equations in (3.9.15) speciﬁes a one- 
to-one relation between the points in Sand the points in T.
184 Chapter 3 Random Variables and Distributions 
x2
11
0 x1Sy2
1
0 y1Ty1y2/H11005 1 
y2/H11408y1/H11005 1 
Figure3.27 The sets SandTin Example 3.9.9. 
We shall now show how to ﬁnd the set T. We know that (x 1, x 2)∈Sif and only 
if the following inequalities hold: 
x1>0, x 1<1, x 2>0,andx2<1. (3.9.16)
We can substitute the formulas for x1andx2in terms of y1andy2from Eq. (3.9.15)
into the inequalities in (3.9.16) to obtain 
(y 1y2)1/2>0, (y 1y2)1/2<1,/parenleftbiggy2
y1/parenrightbigg1/2
>0,
and/parenleftbiggy2
y1/parenrightbigg1/2
<1. (3.9.17)
The ﬁrst inequality transforms to ( y1>0 and y2>0) or ( y1<0 and y2<0). However, 
sincey1=x1/x 2, we cannot have y1<0, so we get only y1>0 and y2>0. The third 
inequality in (3.9.17) transforms to the same thing. The second inequality in (3.9.17) 
becomes y2<1/y 1. The fourth inequality becomes y2< y 1. The region Twhere
(y 1, y 2)satisfy these new inequalities is shown in the right panel of Fig. 3.27 with 
the set Sin the left panel. 
For the functions in (3.9.15), 
∂s 1
∂y 1=1
2/parenleftbiggy2
y1/parenrightbigg1/2
,∂s 1
∂y 2=1
2/parenleftbiggy1
y2/parenrightbigg1/2
,
∂s 2
∂y 1=− 1
2/parenleftBigg
y2
y3
1/parenrightBigg1/2
,∂s 2
∂y 2=1
2/parenleftbigg1
y1y2/parenrightbigg1/2
.
Hence,
J=det
1
2/parenleftbiggy2
y1/parenrightbigg1/21
2/parenleftbiggy1
y2/parenrightbigg1/2
−1
2/parenleftBigg
y2
y3
1/parenrightBigg1/2
1
2/parenleftbigg1
y1y2/parenrightbigg1/2
=1
2y1.
Sincey1>0 throughout the set T,|J|= 1/( 2y1).
The joint p.d.f. g(y 1, y 2)can now be obtained directly from Eq. (3.9.13) in the 
following way: In the expression for f(x 1, x 2), replace x1with(y 1y2)1/2, replace x2
3.9 Functions of Two or More Random Variables 185
with(y 2/y 1)1/2, and multiply the result by |J|= 1/( 2y1). Therefore, 
g(y 1, y 2)=/braceleftBigg
2/parenleftBig
y2
y1/parenrightBig
for(y 1, y 2)∈T,
0 otherwise. ◭
Example 
3.9.10 Service Time in a Queue . Let Xbe the time that the server in a single-server queue 
will spend on a particular customer, and let Ybe the rate at which the server can 
operate. A popular model for the conditional distribution of XgivenY=yis to say 
that the conditional p.d.f. of XgivenY=yis 
g1(x |y) =/braceleftbiggye −xy forx > 0, 
0 otherwise. 
LetYhave the p.d.f. f2(y) . The joint p.d.f. of (X, Y) is then g1(x |y)f 2(y) . Because 
1/Y can be interpreted as the average service time, Z=XY measures how quickly,
compared to average, that the customer is served. For example, Z=1 corresponds 
to an average service time, while Z > 1 means that this customer took longer than 
average, and Z < 1 means that this customer was served more quickly than the 
average customer. If we want the distribution of Z, we could compute the joint p.d.f. 
of (Z, Y) directly using the methods just illustrated. We could then integrate the joint 
p.d.f. over yto obtain the marginal p.d.f. of Z. However, it is simpler to transform the 
conditional distribution of XgivenY=yinto the conditional distribution of Zgiven
Y=y, since conditioning on Y=yallows us to treat Yas the constant y. Because 
X=Z/Y , the inverse transformation is x=s(z) , where s(z) =z/y . The derivative of 
this is 1 /y , and the conditional p.d.f. of ZgivenY=yis 
h1(z |y) =1
yg1/parenleftbiggz
y/vextendsingle/vextendsingle/vextendsingle/vextendsingley/parenrightbigg
.
Because Yis a rate, Y≥0 and X=Z/Y > 0 if and only if Z > 0. So, 
h1(z |y) =/braceleftbigge−zforz > 0, 
0 otherwise. (3.9.18)
Notice that h1does not depend on y, so Zis independent of Yandh1is the marginal 
p.d.f. of Z. The reader can verify all of this in Exercise 17. ◭
Note: Removing Dependence. The formula Z=XY in Example 3.9.10 makes it 
look as if Zshould depend on Y. In reality, however, multiplying Xby Yremoves the
dependence that Xalready has on Yand makes the result independent of Y. This type 
of transformation that removes the dependence of one random variable on another 
is a very powerful technique for ﬁnding marginal distributions of transformations of 
random variables.
In Example 3.9.10, we mentioned that there was another, more straightforward 
but more tedious, way to compute the distribution of Z. That method, which is useful 
in many settings, is to transform (X, Y) into(Z, W) for some uninteresting random
variable Wand then integrate wout of the joint p.d.f. All that matters in the choice 
of Wis that the transformation be one-to-one with differentiable inverse and that 
the calculations are feasible. Here is a speciﬁc example. 
Example 
3.9.11 One Function of Two Variables . In Example 3.9.9, suppose that we were interested 
only in the quotient Y1=X1/X 2rather than both the quotient and the product
Y2=X1X2. Since we already have the joint p.d.f. of (Y 1, Y 2), we will merely integrate 
y2out rather than start from scratch. For each value of y1>0, we need to look at the 
setTin Fig. 3.27 and ﬁnd the interval of y2values to integrate over. For 0 < y 1<1, 
186 Chapter 3 Random Variables and Distributions 
we integrate over 0 < y 2< y 1. For y1>1, we integrate over 0 < y 2<1/y 1. (For y1=1
both intervals are the same.) So, the marginal p.d.f. of Y1is 
g1(y 1)=

/integraltexty1
02/parenleftBig
y2
y1/parenrightBig
dy 2 for 0< y 1<1, 
/integraltext1/y 1
02/parenleftBig
y2
y1/parenrightBig
dy 2fory1>1, 
=/braceleftBiggy1for 0< y 1<1, 
1
y3
1fory1>1. 
There are other transformations that would have made the calculation of g1simpler
if that had been all we wanted. See Exercise 21 for an example. ◭
Theorem
3.9.6Linear Transformations . Let X=(X 1, . . . , X n)have a continuous joint distribution for 
which the joint p.d.f. is f. Deﬁne Y=(Y 1, . . . , Y n)by 
Y=AX , (3.9.19)
whereAis a nonsingular n×nmatrix. Then Yhas a continuous joint distribution 
with p.d.f. 
g( y)=1
|detA|f( A−1y)fory∈Rn, (3.9.20)
whereA−1is the inverse of A.
Proof EachYiis a linear combination of X1, . . . , X n. Because Ais nonsingular, the 
transformation in Eq. (3.9.19) is a one-to-one transformation of the entire space Rn
onto itself. At every point y∈Rn, the inverse transformation can be represented by 
the equation
x=A−1y. (3.9.21)
The Jacobian Jof the transformation that is deﬁned by Eq. (3.9.21) is simply J=
detA−1. Also, it is known from the theory of determinants that 
detA−1=1
detA.
Therefore, at every point y∈Rn, the joint p.d.f. g( y)can be evaluated in the fol- 
lowing way, according to Theorem 3.9.5: First, for i=1, . . . , n , the component xiin 
f(x 1, . . . , x n)is replaced with the ith component of the vector A−1y. Then, the result 
is divided by |detA|. This produces Eq. (3.9.20). 
Summary
We extended the construction of the distribution of a function of a random variable 
to the case of several functions of several random variables. If one only wants the 
distribution of one function r1of nrandom variables, the usual way to ﬁnd this is to 
ﬁrst ﬁnd n−1additional functions r2, . . . , r nso that the nfunctions together compose
a one-to-one transformation. Then ﬁnd the joint p.d.f. of the nfunctions and ﬁnally
ﬁnd the marginal p.d.f. of the ﬁrst function by integrating out the extra n−1variables. 
The method is illustrated for the cases of the sum and the range of several random 
variables.
3.9 Functions of Two or More Random Variables 187
Exercises 
1. Suppose that X1andX2are i.i.d. random variables and
that each of them has the uniform distribution on the 
interval [0 ,1]. Find the p.d.f. of Y=X1+X2.
2. For the conditions of Exercise 1, ﬁnd the p.d.f. of the 
average (X 1+X2)/ 2. 
3. Suppose that three random variables X1,X2, and X3
have a continuous joint distribution for which the joint 
p.d.f. is as follows: 
f(x 1, x 2, x 3)=/braceleftbigg8x1x2x3for 0< x i<1(i =1,2,3),
0 otherwise. 
Suppose also that Y1=X1,Y2=X1X2, and Y3=X1X2X3.
Find the joint p.d.f. of Y1,Y2, and Y3.
4. Suppose that X1andX2have a continuous joint distri- 
bution for which the joint p.d.f. is as follows: 
f(x 1, x 2)=/braceleftbiggx1+x2for 0< x 1<1 and 0 < x 2<1, 
0 otherwise. 
Find the p.d.f. of Y=X1X2.
5. Suppose that the joint p.d.f. of X1andX2is as given in 
Exercise 4. Find the p.d.f. of Z=X1/X 2.
6. LetXandYbe random variables for which the joint 
p.d.f. is as follows: 
f(x, y) =/braceleftbigg2(x +y) for 0≤x≤y≤1, 
0 otherwise. 
Find the p.d.f. of Z=X+Y.
7. Suppose that X1andX2are i.i.d. random variables and
that the p.d.f. of each of them is as follows: 
f(x) =/braceleftbigg
e−xforx > 0, 
0 otherwise. 
Find the p.d.f. of Y=X1−X2.
8. Suppose that X1, . . . , X nform a random sample of size 
nfrom the uniform distribution on the interval [0 ,1] and 
thatYn=max{X1, . . . , X n}. Find the smallest value of n
such that
Pr {Yn≥0.99 }≥ 0.95 .
9. Suppose that the nvariables X1, . . . , X nform a random 
sample from the uniform distribution on the interval [0 ,1] 
and that the random variables Y1andYnare deﬁned as 
in Eq. (3.9.8). Determine the value of Pr (Y 1≤0.1 and 
Yn≤0.8). 
10.For the conditions of Exercise 9, determine the value 
of Pr (Y 1≤0.1 and Yn≥0.8). 11.For the conditions of Exercise 9, determine the prob- 
ability that the interval from Y1to Ynwill not contain the
point 1/3.
12.LetWdenote the range of a random sample of n
observations from the uniform distribution on the interval 
[0 ,1]. Determine the value of Pr (W > 0.9).
13.Determine the p.d.f. of the range of a random sample 
of nobservations from the uniform distribution on the 
interval [−3,5]. 
14.Suppose that X1, . . . , X nform a random sample of n
observations from the uniform distribution on the interval 
[0 ,1], and let Ydenote the second largest of the observa- 
tions. Determine the p.d.f. of Y.Hint: First determine the 
c.d.f. Gof Yby noting that 
G(y)=Pr (Y ≤y) 
=Pr (At least n−1 observations ≤y). 
15.Show that if X1, X 2, . . . , X nare independent random
variables and if Y1=r1(X 1),Y2=r2(X 2), . . . , Y n=rn(X n),
thenY1,Y2, . . . , Y nare also independent random vari-
ables.
16.Suppose that X1,X2, . . . , X 5are ﬁve random vari-
ables for which the joint p.d.f. can be factored in the fol- 
lowing form for all points (x 1, x 2, . . . , x 5)∈R5:
f(x 1, x 2, . . . , x 5)=g(x 1, x 2)h(x 3, x 4, x 5), 
where gandhare certain nonnegative functions. Show 
that if Y1=r1(X 1, X 2)andY2=r2(X 3, X 4, X 5), then the 
random variables Y1andY2are independent.
17.In Example 3.9.10, use the Jacobian method (3.9.13) 
to verify that YandZare independent and that Eq.
(3.9.18) is the marginal p.d.f. of Z.
18.Let the conditional p.d.f. of XgivenYbe g1(x |y) =
3x2/y 3for 0< x < y and 0 otherwise. Let the marginal 
p.d.f. of Ybe f2(y) , where f2(y) =0 for y≤0 but is oth- 
erwise unspeciﬁed. Let Z=X/Y . Prove that ZandYare
independent and ﬁnd the marginal p.d.f. of Z.
19.LetX1andX2be as in Exercise 7. Find the p.d.f. of 
Y=X1+X2.
20.If a2=0 in Theorem 3.9.4, show that Eq. (3.9.2) be- 
comes the same as Eq. (3.8.1) with a=a1andf=f1.
21.In Examples 3.9.9 and 3.9.11, ﬁnd the marginal p.d.f. 
of Z1=X1/X 2by ﬁrst transforming to Z1andZ2=X1and
then integrating z2out of the joint p.d.f. 
188 Chapter 3 Random Variables and Distributions 
⋆3.10 Markov Chains 
A popular model for systems that change over time in a random manner is the 
Markovchainmodel.AMarkovchainisasequenceofrandomvariables,onefor 
eachtime.Ateachtime,thecorrespondingrandomvariablegivesthestateofthe 
system.Also,theconditionaldistributionofeachfuturestategiventhepaststates 
andthepresentstatedependsonlyonthepresentstate.
Stochastic Processes 
Example 
3.10.1 Occupied Telephone Lines . Suppose that a certain business ofﬁce has ﬁve telephone 
lines and that any number of these lines may be in use at any given time. During 
a certain period of time, the telephone lines are observed at regular intervals of 2 
minutes and the number of lines that are being used at each time is noted. Let X1
denote the number of lines that are being used when the lines are ﬁrst observed at the 
beginning of the period; let X2denote the number of lines that are being used when 
they are observed the second time, 2 minutes later; and in general, for n=1,2, . . . , 
letXndenote the number of lines that are being used when they are observed for the 
nth time. ◭
Deﬁnition
3.10.1 Stochastic Process . A sequence of random variables X1, X 2, . . . is called a stochastic
process or randomprocess withdiscretetimeparameter . The ﬁrst random variable X1
is called the initialstate of the process; and for n=2,3, . . . , the random variable Xn
is called the stateoftheprocessattimen .
In Example 3.10.1, the state of the process at any time is the number of lines 
being used at that time. Therefore, each state must be an integer between 0 and 5. 
Each of the random variables in a stochastic process has a marginal distribution, 
and the entire process has a joint distribution. For convenience, in this text, we will 
discuss only joint distributions for ﬁnitely many of X1, X 2, . . . at a time. The meaning 
of the phrase “discrete time parameter” is that the process, such as the numbers of 
occupied phone lines, is observed only at discrete or separated points in time, rather 
than continuously in time. In Sec. 5.4, we will introduce a different stochastic process 
(called the Poisson process) with a continuous time parameter. 
In a stochastic process with a discrete time parameter, the state of the process 
varies in a random manner from time to time. To describe a complete probability 
model for a particular process, it is necessary to specify the distribution for the 
initial state X1and also to specify for each n=1,2, . . . the conditional distribution
of the subsequent state Xn+1givenX1, . . . , X n. These conditional distributions are 
equivalent to the collection of conditional c.d.f.’s of the following form: 
Pr (X n+1≤b|X1=x1, X 2=x2, . . . , X n=xn). 
Markov Chains 
A Markov chain is a special type of stochastic process, deﬁned in terms of the 
conditional distributions of future states given the present and past states. 
Deﬁnition
3.10.2 Markov Chain . A stochastic process with discrete time parameter is a Markovchain 
if, for each time n, the conditional distributions of all Xn+jforj≥1 given X1, . . . , X n
depend only on Xnand not on the earlier states X1, . . . , X n−1. In symbols, for 
3.10 Markov Chains 189
n=1,2, . . . and for each band each possible sequence of states x1, x 2, . . . , x n,
Pr (X n+1≤b|X1=x1, X 2=x2, . . . , X n=xn)=Pr (X n+1≤b|Xn=xn). 
A Markov chain is called ﬁnite if there are only ﬁnitely many possible states. 
In the remainder of this section, we shall consider only ﬁnite Markov chains. This 
assumption could be relaxed at the cost of more complicated theory and calculation. 
For convenience, we shall reserve the symbol kto stand for the number of possible 
states of a general ﬁnite Markov chain for the remainder of the section. It will also 
be convenient, when discussing a general ﬁnite Markov chain, to name the kstates
using the integers 1 , . . . , k . That is, for each nandj,Xn=jwill mean that the chain 
is in state jat time n. In speciﬁc examples, it may prove more convenient to label the 
states in a more informative fashion. For example, if the states are the numbers of 
phone lines in use at given times (as in the example that introduced this section), we 
would label the states 0 , . . . , 5 even though k=6. 
The following result follows from the multiplication rule for conditional proba- 
bilities, Theorem 2.1.2. 
Theorem
3.10.1 For a ﬁnite Markov chain, the joint p.f. for the ﬁrst nstates equals
Pr (X 1=x1, X 2=x2, . . . , X n=xn)
=Pr (X 1=x1)Pr (X 2=x2|X1=x1)Pr (X 3=x3|X2=x2). . . 
Pr (X n=xn|Xn−1=xn−1). (3.10.1)
Also, for each nand each m > 0, 
Pr (X n+1=xn+1, X n+2=xn+2, . . . , X n+m=xn+m|Xn=xn)
=Pr (X n+1=xn+1|Xn=xn)Pr (X n+2=xn+2|Xn+1=xn+1)
. . . Pr (X n+m=xn+m|Xn+m−1=xn+m−1). (3.10.2)
Eq. (3.10.1) is a discrete version of a generalization of conditioning in sequence that 
was illustrated in Example 3.7.18 with continuous random variables. Eq. (3.10.2) is a 
conditional version of (3.10.1) shifted forward in time. 
Example 
3.10.2 Shopping for Toothpaste . In Exercise 4 in Sec. 2.1, we considered a shopper who 
chooses between two brands of toothpaste on several occasions. Let Xi=1 if the 
shopper chooses brand Aon the ith purchase, and let Xi=2 if the shopper chooses 
brand Bon the ith purchase. Then the sequence of states X1, X 2, . . . is a stochas- 
tic process with two possible states at each time. The probabilities of purchase were 
speciﬁed by saying that the shopper will choose the same brand as on the previous 
purchase with probability 1/3 and will switch with probability 2/3. Since this hap-
pens regardless of purchases that are older than the previous one, we see that this 
stochastic process is a Markov chain with 
Pr (X n+1=1|Xn=1)=1
3,Pr (X n+1=2|Xn=1)=2
3,
Pr (X n+1=1|Xn=2)=2
3,Pr (X n+1=2|Xn=2)=1
3. ◭
Exammple 3.10.2 has an additional feature that puts it in a special class of Markov 
chains. The probability of moving from one state at time nto another state at time 
n+1 does not depend on n.
190 Chapter 3 Random Variables and Distributions 
Deﬁnition
3.10.3 Transition Distributions/Stationary Transition Distributions . Consider a ﬁnite Markov 
chain with kpossible states. The conditional distributions of the state at time n+1
given the state at time n, that is, Pr (X n+1=j|Xn=i) fori, j =1, . . . , k andn=
1,2, . . . , are called the transitiondistributions of the Markov chain. If the transition 
distribution is the same for every time n(n=1,2, . . .) , then the Markov chain has 
stationarytransitiondistributions .
When a Markov chain with kpossible states has stationary transition distribu-
tions, there exist probabilities pij fori, j =1, . . . , k such that, for all n,
Pr (X n+1=j|Xn=i) =pij forn=1,2, . . . . (3.10.3)
The Markov chain in Example 3.10.2 has stationary transition distributions. For 
example, p11 =1/3. 
In the language of multivariate distributions, when a Markov chain has stationary 
transition distributions, speciﬁed by (3.10.3), we can write the conditional p.f. of Xn+1
givenXnas 
g(j |i) =pij , (3.10.4)
for all n, i, j .
Example 
3.10.3 Occupied Telephone Lines . To illustrate the application of these concepts, we shall 
consider again the example involving the ofﬁce with ﬁve telephone lines. In order 
for this stochastic process to be a Markov chain, the speciﬁed distribution for the 
number of lines that may be in use at each time must depend only on the number 
of lines that were in use when the process was observed most recently 2 minutes 
earlier and must not depend on any other observed values previously obtained. For 
example, if three lines were in use at time n, then the distribution for time n+1 must 
be the same regardless of whether 0 ,1,2,3,4, or 5 lines were in use at time n−1. 
In reality, however, the observation at time n−1 might provide some information in 
regard to the length of time for which each of the three lines in use at time nhad been
occupied, and this information might be helpful in determining the distribution for 
timen+1. Nevertheless, we shall suppose now that this process is a Markov chain. 
If this Markov chain is to have stationary transition distributions, it must be true that 
the rates at which incoming and outgoing telephone calls are made and the average 
duration of these telephone calls do not change during the entire period covered 
by the process. This requirement means that the overall period cannot include busy 
times when more calls are expected or quiet times when fewer calls are expected. For 
example, if only one line is in use at a particular observation time, regardless of when 
this time occurs during the entire period covered by the process, then there must be 
a speciﬁc probability p1jthat exactly jlines will be in use 2 minutes later. ◭
The Transition Matrix 
Example 
3.10.4 Shopping for Toothpaste . The notation for stationary transition distributions, pij ,
suggests that they could be arranged in a matrix. The transition probabilities for 
Example 3.10.2 can be arranged into the following matrix: 
P=/bracketleftBigg1
32
3
2
31
3/bracketrightBigg
. ◭
3.10 Markov Chains 191
Every ﬁnite Markov chain with stationary transition distributions has a matrix like 
the one constructed in Example 3.10.4. 
Deﬁnition
3.10.4 Transition Matrix . Consider a ﬁnite Markov chain with stationary transition distribu- 
tions given by pij =Pr (X n+1=j|Xn=i) for all n, i, j . The transition matrix of the 
Markov chain is deﬁned to be the k×kmatrixPwith elements pij . That is, 
P=
p11 . . . p1k
p21 . . . p2k
.........
pk1. . . pkk 
. (3.10.5)
A transition matrix has several properties that are apparent from its deﬁntion. 
For example, each element is nonnegative because all elements are probabilities. 
Since each row of a transition matrix is a conditional p.f. for the next state given 
some value of the current state, we have /summationtextk
j=1pij =1 for i=1, . . . , k . Indeed, row 
iof the transition matrix speciﬁes the conditional p.f. g( .|i) deﬁned in (3.10.4). 
Deﬁnition
3.10.5 Stochastic Matrix . A square matrix for which all elements are nonnegative and the 
sum of the elements in each row is 1 is called a stochasticmatrix .
It is clear that the transition matrix Pfor every ﬁnite Markov chain with stationary
transition probabilities must be a stochastic matrix. Conversely, every k×kstochastic
matrix can serve as the transition matrix of a ﬁnite Markov chain with kpossible states
and stationary transition distributions.
Example 
3.10.5 A Transition Matrix for the Number of Occupied Telephone Lines . Suppose that in the 
example involving the ofﬁce with ﬁve telephone lines, the numbers of lines being 
used at times 1 ,2, . . . form a Markov chain with stationary transition distributions. 
This chain has six possible states 0 ,1, . . . , 5, where iis the state in which exactly 
ilines are being used at a given time ( i=0,1, . . . , 5). Suppose that the transition 
matrixPis as follows: 
P=
0 1 2 3 4 5 
0 0 .1 0 .4 0 .2 0 .1 0 .1 0 .1
1 0 .2 0 .3 0 .2 0 .1 0 .1 0 .1
2 0 .1 0 .2 0 .3 0 .2 0 .1 0 .1
3 0 .1 0 .1 0 .2 0 .3 0 .2 0 .1
4 0 .1 0 .1 0 .1 0 .2 0 .3 0 .2
5 0 .1 0 .1 0 .1 0 .1 0 .4 0 .2
. (3.10.6)
(a) Assuming that all ﬁve lines are in use at a certain observation time, we shall 
determine the probability that exactly four lines will be in use at the next observation 
time. (b) Assuming that no lines are in use at a certain time, we shall determine the 
probability that at least one line will be in use at the next observation time. 
(a) This probability is the element in the matrix Pin the row corresponding to the 
state 5 and the column corresponding to the state 4. Its value is seen to be 0.4. 
(b) If no lines are in use at a certain time, then the element in the upper left corner 
of the matrix Pgives the probability that no lines will be in use at the next 
observation time. Its value is seen to be 0.1. Therefore, the probability that at 
least one line will be in use at the next observation time is 1 −0.1=0.9. ◭
192 Chapter 3 Random Variables and Distributions 
Figure3.28 The generation 
following{Aa, Aa }.aA aA
Aa aa AA aA 
Example 
3.10.6 Plant Breeding Experiment . A botanist is studying a certain variety of plant that is 
monoecious (has male and female organs in separate ﬂowers on a single plant). 
She begins with two plants I and II and cross-pollinates them by crossing male I 
with female II and female I with male II to produce two offspring for the next 
generation. The original plants are destroyed and the process is repeated as soon 
as the new generation of two plants is mature. Several replications of the study are 
run simultaneously. The botanist might be interested in the proportion of plants in 
any generation that have each of several possible genotypes for a particular gene. 
(See Example 1.6.4 on page 23.) Suppose that the gene has two alleles, Aanda.
The genotype of an individual will be one of the three combinations AA ,Aa , or aa .
When a new individual is born, it gets one of the two alleles (with probability 1/2 
each) from one of the parents, and it independently gets one of the two alleles from 
the other parent. The two offspring get their genotypes independently of each other. 
For example, if the parents have genotypes AA andAa , then an offspring will get 
Afor sure from the ﬁrst parent and will get either Aor afrom the second parent
with probability 1/2 each. Let the states of this population be the set of genotypes of 
the two members of the current population. We will not distinguish the set {AA, Aa }
from{Aa, AA }. There are then six states: {AA, AA },{AA, Aa },{AA, aa },{Aa, Aa },
{Aa, aa }, and {aa, aa }. For each state, we can calculate the probability that the next 
generation will be in each of the six states. For example, if the state is either {AA, AA }
or {aa, aa }, the next generation will be in the same state with probability 1. If the state 
is {AA, aa }, the next generation will be in state {Aa, Aa }with probability 1. The other 
three states have more complicated transitions.
If the current state is {Aa, Aa }, then all six states are possible for the next gen- 
eration. In order to compute the transition distribution, it helps to ﬁrst compute the 
probability that a given offspring will have each of the three genotypes. Figure 3.28 
illustrates the possible offspring in this state. Each arrow going down in Fig. 3.28 
is a possible inheritance of an allele, and each combination of arrows terminating 
in a genotype has probability 1/4. It follows that the probability of AA andaa are
both 1/4, while the probability of Aa is 1/2, because two different combinations of 
arrows lead to this offspring. In order for the next state to be {AA, AA }, both off- 
spring must be AA independently, so the probability of this transition is 1/16. The 
same argument implies that the probability of a transition to {aa, aa }is 1/16. A tran- 
sition to {AA, Aa }requires one offspring to be AA (probability 1/4) and the other to 
be Aa (probabilty 1/2). But the two different genotypes could occur in either order, 
so the whole probability of such a transition is 2 ×(1/4)×(1/2)=1/4. A similar ar- 
gument shows that a transition to {Aa, aa }also has probability 1/4. A transition to 
{AA, aa }requires one offspring to be AA (probability 1/4) and the other to be aa 
(probability 1/4). Once again, these can occur in two orders, so the whole probabil- 
ity is 2 ×1/4×1/4=1/8. By subtraction, the probability of a transition to {Aa, Aa }
must be 1 −1/16 −1/16 −1/4−1/4−1/8=1/4. Here is the entire transition matrix, 
which can be veriﬁed in a manner similar to what has just been done: 
3.10 Markov Chains 193

{AA, AA } {AA, Aa } {AA, aa } {Aa, Aa } {Aa, aa } {aa, aa }
{AA, AA } 1.0000 0 .0000 0 .0000 0 .0000 0 .0000 0 .0000
{AA, Aa } 0.2500 0 .5000 0 .0000 0 .2500 0 .0000 0 .0000
{AA, aa } 0.0000 0 .0000 0 .0000 1 .0000 0 .0000 0 .0000
{Aa, Aa } 0.0625 0 .2500 0 .1250 0 .2500 0 .2500 0 .0625
{Aa, aa } 0.0000 0 .0000 0 .0000 0 .2500 0 .5000 0 .2500
{aa, aa } 0.0000 0 .0000 0 .0000 0 .0000 0 .0000 1 .0000
.
◭
TheTransitionMatrixforSeveralSteps 
Example 
3.10.7 Single Server Queue . A manager usually checks the server at her store every 5 minutes 
to see whether the server is busy or not. She models the state of the server (1 =busy
or 2 =not busy) as a Markov chain with two possible states and stationary transition 
distributions given by the following matrix: 
P=
Busy Not busy
Busy 0 .9 0 .1
Not busy 0 .6 0 .4
.
The manager realizes that, later in the day, she will have to be away for 10 minutes 
and will miss one server check. She wants to compute the conditional distribution of 
the state two time periods in the future given each of the possible states. She reasons 
as follows: If Xn=1 for example, then the state will have to be either 1 or 2 at time 
n+1 even though she does not care now about the state at time n+1. But, if she 
computes the joint conditional distribution of Xn+1andXn+2givenXn=1, she can 
sum over the possible values of Xn+1to get the conditional distribution of Xn+2given
Xn=1. In symbols, 
Pr (X n+2=1|Xn=1)=Pr (X n+1=1, X n+2=1|Xn=1)
+Pr (X n+1=2, X n+2=1|Xn=1). 
By the second part of Theorem 3.10.1, 
Pr (X n+1=1, X n+2=1|Xn=1)=Pr (X n+1=1|Xn=1)Pr (X n+2=1|Xn+1=1)
=0.9×0.9=0.81 .
Similarly,
Pr (X n+1=2, X n+2=1|Xn=1)=Pr (X n+1=2|Xn=1)Pr (X n+2=1|Xn+1=2)
=0.1×0.6=0.06 .
It follows that Pr (X n+2=1|Xn=1)=0.81 +0.06 =0.87, and hence Pr (X n+2=2|Xn=
1)=1−0.87 =0.13. By similar reasoning, if Xn=2, 
Pr (X n+2=1|Xn=2)=0.6×0.9+0.4×0.6=0.78 ,
and Pr (X n+2=2|Xn=2)=1−0.78 =0.22. ◭
Generalizing the calculations in Example 3.10.7 to three or more transitions might 
seem tedious. However, if one examines the calculations carefully, one sees a pattern 
194 Chapter 3 Random Variables and Distributions 
that will allow a compact calculation of transition distributions for several steps. 
Consider a general Markov chain with kpossible states 1 , . . . , k and the transition
matrixPgiven by Eq. (3.10.5). Assuming that the chain is in state iat a given time n,
we shall now determine the probability that the chain will be in state jat time n+2. 
In other words, we shall determine the conditional probability of Xn+2=jgiven
Xn=i. The notation for this probability is p(2)
ij .
We argue as the manager did in Example 3.10.7. Let rdenote the value of Xn+1
that is not of primary interest but is helpful to the calculation. Then 
p(2)
ij =Pr (X n+2=j|Xn=i) 
=k/summationdisplay
r=1Pr (X n+1=randXn+2=j|Xn=i) 
=k/summationdisplay
r=1Pr (X n+1=r|Xn=i) Pr (X n+2=j|Xn+1=r, X n=i) 
=k/summationdisplay
r=1Pr (X n+1=r|Xn=i) Pr (X n+2=j|Xn+1=r) 
=k/summationdisplay
r=1pir prj ,
where the third equality follows from Theorem 2.1.3 and the fourth equality follows 
from the deﬁnition of a Markov chain. 
The value of p(2)
ij can be determined in the following manner: If the transition 
matrixPis squared, that is, if the matrix P2=PP is constructed, then the element in 
theith row and the jth column of the matrix P2will be /summationtextk
r=1pir prj . Therefore, p(2)
ij 
will be the element in the ith row and the jth column of P2.
By a similar argument, the probability that the chain will move from the state ito 
the state jin three steps, or p(3)
ij =Pr (X n+3=j|Xn=i) , can be found by constructing 
the matrixP3=P2P. Then the probability p(3)
ij will be the element in the ith row and 
thejth column of the matrix P3.
In general, we have the following result. 
Theorem
3.10.2 Multiple Step Transitions . Let Pbe the transition matrix of a ﬁnite Markov chain with 
stationary transition distributions. For each m=2,3, . . . , the mth power Pmof the 
matrixPhas in row iand column jthe probability p(m)
ij that the chain will move from
stateito state jin msteps.
Deﬁnition
3.10.6 Multiple Step Transition Matrix . Under the conditions of Theorem 3.10.2, the ma- 
trixPmis called the m-steptransitionmatrix of the Markov chain. 
In summary, the ith row of the m-step transition matrix gives the conditional distri-
bution of Xn+mgivenXn=ifor all i=1, . . . , k and all n, m =1,2, . . . . 
Example 
3.10.8 The Two-Step and Three-Step Transition Matrices for the Numb er of Occupied Telephone 
Lines . Consider again the transition matrix Pgiven by Eq. (3.10.6) for the Markov 
chain based on ﬁve telephone lines. We shall assume ﬁrst that ilines are in use at a 
3.10 Markov Chains 195
certain time, and we shall determine the probability that exactly jlines will be in use 
two time periods later.
If we multiply the matrix Pby itself, we obtain the following two-step transition 
matrix:
P2=
0 1 2 3 4 5 
0 0 .14 0 .23 0 .20 0 .15 0 .16 0 .12 
1 0 .13 0 .24 0 .20 0 .15 0 .16 0 .12 
2 0 .12 0 .20 0 .21 0 .18 0 .17 0 .12 
3 0 .11 0 .17 0 .19 0 .20 0 .20 0 .13 
4 0 .11 0 .16 0 .16 0 .18 0 .24 0 .15 
5 0 .11 0 .16 0 .15 0 .17 0 .25 0 .16 
. (3.10.7)
From this matrix we can ﬁnd any two-step transition probability for the chain, such 
as the following: 
i. If two lines are in use at a certain time, then the probability that four lines will 
be in use two time periods later is 0.17. 
ii. If three lines are in use at a certain time, then the probability that three lines 
will again be in use two time periods later is 0.20. 
We shall now assume that ilines are in use at a certain time, and we shall 
determine the probability that exactly jlines will be in use three time periods later. 
If we construct the matrix P3=P2P, we obtain the following three-step transi- 
tion matrix:
P3=
0 1 2 3 4 5 
0 0 .123 0 .208 0 .192 0 .166 0 .183 0 .128
1 0 .124 0 .207 0 .192 0 .166 0 .183 0 .128
2 0 .120 0 .197 0 .192 0 .174 0 .188 0 .129
3 0 .117 0 .186 0 .186 0 .179 0 .199 0 .133
4 0 .116 0 .181 0 .177 0 .176 0 .211 0 .139
5 0 .116 0 .180 0 .174 0 .174 0 .215 0 .141
. (3.10.8)
From this matrix we can ﬁnd any three-step transition probability for the chain, such 
as the following: 
i. If all ﬁve lines are in use at a certain time, then the probability that no lines will 
be in use three time periods later is 0.116. 
ii. If one line is in use at a certain time, then the probability that exactly one line 
will again be in use three time periods later is 0.207. ◭
Example 
3.10.9 Plant Breeding Experiment . In Example 3.10.6, the transition matrix has many zeros, 
since many of the transitions will not occur. However, if we are willing to wait two 
steps, we will ﬁnd that the only transitions that cannot occur in two steps are those 
from the ﬁrst state to anything else and those from the last state to anything else. 
196 Chapter 3 Random Variables and Distributions 
Here is the two-step transition matrix: 

{AA, AA } {AA, Aa } {AA, aa } {Aa, Aa } {Aa, aa } {aa, aa }
{AA, AA } 1.0000 0 .0000 0 .0000 0 .0000 0 .0000 0 .0000
{AA, Aa } 0.3906 0 .3125 0 .0313 0 .1875 0 .0625 0 .0156
{AA, aa } 0.0625 0 .2500 0 .1250 0 .2500 0 .2500 0 .0625
{Aa, Aa } 0.1406 0 .1875 0 .0313 0 .3125 0 .1875 0 .1406
{Aa, aa } 0.0156 0 .0625 0 .0313 0 .1875 0 .3125 0 .3906
{aa, aa } 0.0000 0 .0000 0 .0000 0 .0000 0 .0000 1 .0000
.
Indeed, if we look at the three-step or the four-step or the general m-step transition
matrix, the ﬁrst and last rows will always be the same. ◭
The ﬁrst and last states in Example 3.10.9 have the property that, once the chain gets 
into one of those states, it can’t get out. Such states occur in many Markov chains 
and have a special name. 
Deﬁnition
3.10.7 Absorbing State . In a Markov chain, if pii =1 for some state i, then that state is called 
an absorbingstate .
In Example 3.10.9, there is positive probability of getting into each absorbing state 
in two steps no matter where the chain starts. Hence, the probability is 1 that the 
chain will eventually be absorbed into one of the absorbing states if it is allowed to 
run long enough.
The Initial Distribution 
Example 
3.10.10 Single Server Queue . The manager in Example 3.10.7 enters the store thinking that the 
probability is 0.3 that the server will be busy the ﬁrst time that she checks. Hence, the 
probability is 0.7 that the server will be not busy. These values specify the marginal 
distribution of the state at time 1, X1. We can represent this distribution by the vector 
v=(0.3,0.7)that gives the probabilities of the two states at time 1 in the same order 
that they appear in the transition matrix. ◭
The vector giving the marginal distribution of X1in Example 3.10.10 has a special 
name.
Deﬁnition
3.10.8 Probability Vector/Initial Distribution . A vector consisting of nonnegative numbers 
that add to 1 is called a probability vector . A probability vector whose coordinates 
specify the probabilities that a Markov chain will be in each of its states at time 1 is 
called the initialdistribution of the chain or the intialprobabilityvector .
For Example 3.10.2, the initial distribution was given in Exercise 4 in Sec. 2.1 as 
v=(0.5,0.5).
The initial distribution and the transition matrix together determine the entire 
joint distribution of the Markov chain. Indeed, Theorem 3.10.1 shows how to con- 
struct the joint distribution of the chain from the initial probability vector and the 
transition matrix. Letting v=(v 1, . . . , v k)denote the initial distribution, Eq. (3.10.1)
can be rewritten as 
Pr (X 1=x1, X 2=x2, . . . , X n=xn)=vx1px1x2. . . pxn−1xn. (3.10.9)
3.10 Markov Chains 197
The marginal distributions of states at times later than 1 can be found from the 
joint distribution.
Theorem
3.10.3 Marginal Distributions at Times Other Than 1 . Consider a ﬁnite Markov chain with 
stationary transition distributions having initial distribution vand transition matrix
P. The marginal distribution of Xn, the state at time n, is given by the probability 
vectorvP n−1.
Proof The marginal distribution of Xncan be found from Eq. (3.10.9) by summing 
over the possible values of x1, . . . , x n−1. That is, 
Pr (X n=xn)=k/summationdisplay
xn−1=1. . . k/summationdisplay
x2=1k/summationdisplay
x1=1vx1px1x2px2x3. . . pxn−1xn. (3.10.10)
The innermost sum in Eq. (3.10.10) for x1=1, . . . , k involves only the ﬁrst two factors
vx1px1x2and produces the x2coordinate of vP . Similarly, the next innermost sum 
overx2=1, . . . , k involves only the x2coordinate of vP andpx2x3and produces the
x3coordinate of vPP=vP 2. Proceeding in this way through all n−1 summations 
produces the xncoordinate of vP n−1.
Example 
3.10.11 Probabilities for the Number of Occupied Telephone Lines . Consider again the ofﬁce 
with ﬁve telephone lines and the Markov chain for which the transition matrix Pis 
given by Eq. (3.10.6). Suppose that at the beginning of the observation process at 
timen=1, the probability that no lines will be in use is 0.5, the probability that one 
line will be in use is 0.3, and the probability that two lines will be in use is 0.2. Then 
the initial probability vector is v=(0.5,0.3,0.2,0,0,0). We shall ﬁrst determine the 
distribution of the number of lines in use at time 2, one period later. 
By an elementary computation it will be found that 
vP =(0.13 ,0.33 ,0.22 ,0.12 ,0.10 ,0.10 ). 
Since the ﬁrst component of this probability vector is 0.13, the probability that no 
lines will be in use at time 2 is 0.13; since the second component is 0.33, the probability 
that exactly one line will be in use at time 2 is 0.33; and so on. 
Next, we shall determine the distribution of the number of lines that will be in 
use at time 3. 
By use of Eq. (3.10.7), it can be found that 
vP 2=(0.133,0.227,0.202,0.156,0.162,0.120). 
Since the ﬁrst component of this probability vector is 0.133, the probability that 
no lines will be in use at time 3 is 0.133; since the second component is 0.227, the 
probability that exactly one line will be in use at time 3 is 0.227; and so on. ◭
Stationary Distributions 
Example 
3.10.12 A Special Initial Distribution for Telephone Lines . Suppose that the initial distribution 
for the number of occupied telephone lines is 
v=(0.119,0.193,0.186,0.173,0.196,0.133). 
It can be shown, by matrix multiplication, that vP =v. This means that if vis the 
initial distribution, then it is also the distribution after one transition. Hence, it will 
also be the distribution after two or more transitions as well. ◭
198 Chapter 3 Random Variables and Distributions 
Deﬁnition
3.10.9 Stationary Distribution . Let Pbe the transition matrix for a Markov chain. A proba- 
bility vector vthat satisﬁes vP =vis called a stationarydistribution for the Markov
chain.
The initial distribution in Example 3.10.12 is a stationary distribution for the tele- 
phone lines Markov chain. If the chain starts in this distribution, the distribution stays 
the same at all times. Every ﬁnite Markov chain with stationary transition distribu- 
tions has at least one stationary distribution. Some chains have a unique stationary 
distribution.
Note:AStationaryDistributionDoesNotMeanThattheChain isNotMoving. It 
is important to note that vP ngives the probabilities that the chain is in each of 
its states after ntransitions, calculated before the initial state of the chain or any 
transitions are observed. These are different from the probabilities of being in the 
various states after observing the initial state or after observing any of the intervening 
transitions. In addition, a stationary distribution does not imply that the Markov 
chain is staying put. If a Markov chain starts in a stationary distribution, then for 
each state i, the probability that the chain is in state iafterntransitions is the same 
as the probability that it is state iat the start. But the Markov chain can still move 
around from one state to the next at each transition. The one case in which a Markov 
chain does stay put is after it moves into an absorbing state. A distribution that is 
concentrated solely on absorbing states will necessarily be stationary because the 
Markov chain will never move if it starts in such a distribution. In such cases, all of 
the uncertainty surrounds the initial state, which will also be the state after every 
transition.
Example 
3.10.13 Stationary Distributions for the Plant Breeding Experiment . Consider again the experi- 
ment described in Example 3.10.6. The ﬁrst and sixth states, {AA, AA }and{aa, aa },
respectively, are absorbing states. It is easy to see that every initial distribution of the 
formv=(p, 0,0,0,0,1−p) for 0≤p≤1 has the property that vP =v. Suppose 
that the chain is in state 1 with probability pand in state 6 with probability 1 −p
at time 1. Because these two states are absorbing states, the chain will never move 
and the event X1=1 is the same as the event that Xn=1 for all n. Similarly, X1=6
is the same as Xn=6. So, thinking ahead to where the chain is likely to be after n
transitions, we would also say that it will be in state 1 with probability pand in state 
6 with probability 1 −p. ◭
MethodforFindingStationaryDistributions We can rewrite the equation vP =v
that deﬁnes stationary distributions as v[P−I]=0, where Iis a k×kidentity matrix
and0is a k-dimensional vector of all zeros. Unfortunately, this system of equations 
has lots of solutions even if there is a unique stationary distribution. The reason is 
that whenever vsolves the system, so does cvfor all real c(including c=0). Even 
though the system has kequations for kvariables, there is at least one redundant 
equation. However, there is also one missing equation. We need to require that the 
solution vector vhas coordinates that sum to 1. We can ﬁx both of these problems by 
replacing one of the equations in the original system by the equation that says that 
the coordinates of vsum to 1. 
To be speciﬁc, deﬁne the matrix Gto be P−Iwith its last column replaced by 
a column of all ones. Then, solve the equation 
3.10 Markov Chains 199
vG =(0, . . . , 0,1). (3.10.11)
If there is a unique stationary distribution, we will ﬁnd it by solving (3.10.11). In this 
case, the matrix Gwill have an inverse G−1that satisﬁes
GG −1=G−1G=I.
The solution of (3.10.11) will then be 
v=(0, . . . , 0,1)G−1,
which is easily seen to be the bottom row of the matrix G−1. This was the method 
used to ﬁnd the stationary distribution in Example 3.10.12. If the Markov chain 
has multiple stationary distributions, then the matrix Gwill be singular, and this 
method will not ﬁnd any of the stationary distributions. That is what would happen 
in Example 3.10.13 if one were to apply the method. 
Example 
3.10.14 Stationary Distribution for Toothpaste Shopping . Consider the transition matrix P
given in Example 3.10.4. We can construct the matrix Gas follows: 
P−I=/bracketleftbigg−2
32
3
2
3−2
3/bracketrightbigg
; hence G=/bracketleftbigg−2
31
2
31/bracketrightbigg
.
The inverse of Gis 
G−1=/bracketleftbigg−3
43
4
1
21
2/bracketrightbigg
.
We now see that the stationary distribution is the bottom row of G−1,v=(1/2,1/2).
◭
There is a special case in which it is known that a unique stationary distribution 
exists and it has special properties. 
Theorem
3.10.4 If there exists msuch that every element of Pmis strictly positive, then 
.the Markov chain has a unique stationary distribution v,
.limn→∞Pnis a matrix with all rows equal to v, and 
.no matter with what distribution the Markov chain starts, its distribution after 
nsteps converges to vas n→∞ .
We shall not prove Theorem 3.10.4, although some evidence for the second 
claim can be seen in Eq. (3.10.8), where the six rows of P3are much more alike
than the rows of Pand they are very similar to the stationary distribution given in 
Example 3.10.12. The third claim in Theorem 3.10.4 actually follows easily from the 
second claim. In Sec. 12.5, we shall introduce a method that makes use of the third 
claim in Theorem 3.10.4 in order to approximate distributions of random variables 
when those distributions are difﬁcult to calculate exactly. 
The transition matrices in Examples 3.10.2, 3.10.5, and 3.10.7 satisfy the condi- 
tions of Theorem 3.10.4. The following example has a unique stationary distribution 
but does not satisfy the conditions of Theorem 3.10.4. 
Example 
3.10.15 Alternating Chain . Let the transition matrix for a two-state Markov chain be 
P=/bracketleftbigg0 1 
1 0 /bracketrightbigg
.
200 Chapter 3 Random Variables and Distributions 
The matrix Gis easy to construct and invert, and we ﬁnd that the unique stationary 
distribution is v=(0.5,0.5). However, as mincreases,Pmalternates between Pand
the 2×2 identity matrix. It does not converge and never does it have all elements 
strictly positive. If the initial distribution is (v 1, v 2), the distribution after nsteps
alternates between (v 1, v 2)and(v 2, v 1). ◭
Another example that fails to satisfy the conditions of Theorem 3.10.4 is the 
gambler’s ruin problem from Sec. 2.4. 
Example 
3.10.16 Gambler’s Ruin . In Sec. 2.4, we described the gambler’s ruin problem, in which a 
gambler wins one dollar with probability pand loses one dollar with probability 1 −p
on each play of a game. The sequence of amounts held by the gambler through the 
course of those plays forms a Markov chain with two absorbing states, namely, 0 and 
k. There are k−1 other states, namely, 1 , . . . , k −1. (This notation violates our use of 
kto stand for the number of states, which is k+1 in this example. We felt this was less 
confusing than switching from the original notation of Sec. 2.4.) The transition matrix 
has ﬁrst and last row being (1,0, . . . , 0)and(0, . . . , 1), respectively. The ith row (for 
i=1, . . . , k −1) has 0 everywhere except in coordinate i−1 where it has 1 −pand
in coordinate i+1 where it has p. Unlike Example 3.10.15, this time the sequence 
of matrices Pmconverges but there is no unique stationary distribution. The limit 
of Pmhas as its last column the numbers a0, . . . , a k, where aiis the probability that 
the fortune of a gambler who starts with idollars reaches kdollars before it reaches 
0 dollars. The ﬁrst column of the limit has the numbers 1 −a0, . . . , 1−akand the
rest of the limit matrix is all zeros. The stationary distributions have the same form 
as those in Example 3.10.13, namely, all probability is in the absorbing states. ◭
Summary
A Markov chain is a stochastic process, a sequence of random variables giving the 
states of the process, in which the conditional distribution of the state at the next 
time given all of the past states depends on the past states only through the most 
recent state. For Markov chains with ﬁnitely many states and stationary transition 
distributions, the transitions over time can be described by a matrix giving the prob- 
abilities of transition from the state indexing the row to the state indexing the column 
(the transition matrix P). The initial probability vector vgives the distribution of the 
state at time 1. The transition matrix and initial probability vector together allow 
calculation of all probabilities associated with the Markov chain. In particular, Pn
gives the probabilities of transitions over ntime periods, and vP ngives the distri-
bution of the state at time n+1. A stationary distribution is a probability vector v
such thatvP =v. Every ﬁnite Markov chain with stationary transition distributions 
has at least one stationary distribution. For many Markov chains, there is a unique 
stationary distribution and the distribution of the chain after ntransitions converges
to the stationary distribution as ngoes to ∞.
Exercises 
1. Consider the Markov chain in Example 3.10.2 with ini- 
tial probability vector v=(1/2,1/2).a. Find the probability vector specifying the probabili- 
ties of the states at time n=2. 
b. Find the two-step transition matrix. 
3.10 Markov Chains 201
2. Suppose that the weather can be only sunny or cloudy 
and the weather conditions on successive mornings form 
a Markov chain with stationary transition probabilities. 
Suppose also that the transition matrix is as follows: 
Sunny Cloudy
Sunny 0.7 0.3
Cloudy 0.6 0.4
a. If it is cloudy on a given day, what is the probability 
that it will also be cloudy the next day? 
b. If it is sunny on a given day, what is the probability 
that it will be sunny on the next two days? 
c. If it is cloudy on a given day, what is the probability 
that it will be sunny on at least one of the next three 
days?
3. Consider again the Markov chain described in Exer- 
cise 2. 
a. If it is sunny on a certain Wednesday, what is the 
probability that it will be sunny on the following 
Saturday?
b. If it is cloudy on a certain Wednesday, what is the 
probability that it will be sunny on the following 
Saturday?
4. Consider again the conditions of Exercises 2 and 3. 
a. If it is sunny on a certain Wednesday, what is the 
probability that it will be sunny on both the following 
Saturday and Sunday?
b. If it is cloudy on a certain Wednesday, what is the 
probability that it will be sunny on both the following 
Saturday and Sunday?
5. Consider again the Markov chain described in Exer- 
cise 2. Suppose that the probability that it will be sunny 
on a certain Wednesday is 0.2 and the probability that it 
will be cloudy is 0.8. 
a. Determine the probability that it will be cloudy on 
the next day, Thursday. 
b. Determine the probability that it will be cloudy on 
Friday. 
c. Determine the probability that it will be cloudy on 
Saturday.
6. Suppose that a student will be either on time or late for 
a particular class and that the events that he is on time or 
late for the class on successive days form a Markov chain 
with stationary transition probabilities. Suppose also that 
if he is late on a given day, then the probability that he will 
be on time the next day is 0.8. Furthermore, if he is on time 
on a given day, then the probability that he will be late the 
next day is 0.5. a. If the student is late on a certain day, what is the 
probability that he will be on time on each of the next 
three days?
b. If the student is on time on a given day, what is the 
probability that he will be late on each of the next 
three days?
7. Consider again the Markov chain described in Exer- 
cise 6. 
a. If the student is late on the ﬁrst day of class, what is 
the probability that he will be on time on the fourth 
day of class? 
b. If the student is on time on the ﬁrst day of class, what 
is the probability that he will be on time on the fourth 
day of class? 
8. Consider again the conditions of Exercises 6 and 7. 
Suppose that the probability that the student will be late 
on the ﬁrst day of class is 0.7 and that the probability that 
he will be on time is 0.3. 
a. Determine the probability that he will be late on the 
second day of class. 
b. Determine the probability that he will be on time on 
the fourth day of class. 
9. Suppose that a Markov chain has four states 1, 2, 3, 4 
and stationary transition probabilities as speciﬁed by the 
following transition matrix:

1 2 3 4 
1 1 /4 1 /4 0 1 /2
2 0 1 0 0 
3 1 /2 0 1 /2 0 
4 1 /4 1 /4 1 /4 1 /4
.
a. If the chain is in state 3 at a given time n, what is the 
probability that it will be in state 2 at time n+2? 
b. If the chain is in state 1 at a given time n, what is the 
probability that it will be in state 3 at time n+3? 
10.LetX1denote the initial state at time 1 of the Markov 
chain for which the transition matrix is as speciﬁed in 
Exercise 5, and suppose that the initial probabilities are 
as follows: 
Pr (X 1=1)=1/8,Pr (X 1=2)=1/4,
Pr (X 1=3)=3/8,Pr (X 1=4)=1/4.
Determine the probabilities that the chain will be in 
states 1, 2, 3, and 4 at time nfor each of the following values 
of n:(a)n=2; (b)n=3; (c)n=4. 
11.Each time that a shopper purchases a tube of tooth- 
paste, she chooses either brand Aor brand B. Suppose that 
the probability is 1/3 that she will choose the same brand 
202 Chapter 3 Random Variables and Distributions 
chosen on her previous purchase, and the probability is 
2/3 that she will switch brands.
a. If her ﬁrst purchase is brand A, what is the probability 
that her ﬁfth purchase will be brand B?
b. If her ﬁrst purchase is brand B, what is the probability 
that her ﬁfth purchase will be brand B?
12.Suppose that three boys A,B, and Care throwing a
ball from one to another. Whenever Ahas the ball, he 
throws it to Bwith a probability of 0.2 and to Cwith a
probability of 0.8. Whenever Bhas the ball, he throws it 
to Awith a probability of 0.6 and to Cwith a probability of 
0.4. Whenever Chas the ball, he is equally likely to throw 
it to either Aor B.
a. Consider this process to be a Markov chain and con- 
struct the transition matrix.
b. If each of the three boys is equally likely to have the 
ball at a certain time n, which boy is most likely to 
have the ball at time n+2? 
13.Suppose that a coin is tossed repeatedly in such a way 
that heads and tails are equally likely to appear on any 
given toss and that all tosses are independent, with the
following exception: Whenever either three heads or three 
tails have been obtained on three successive tosses, then 
the outcome of the next toss is always of the opposite type. 
At time n(n≥3), let the state of this process be speciﬁed 
by the outcomes on tosses n−2, n−1, and n. Show that 
this process is a Markov chain with stationary transition 
probabilities and construct the transition matrix.
14.There are two boxes AandB, each containing red and 
green balls. Suppose that box Acontains one red ball and
two green balls and box Bcontains eight red balls and two
green balls. Consider the following process: One ball is 
selected at random from box A, and one ball is selected 
at random from box B. The ball selected from box Ais then placed in box Band the ball selected from box Bis 
placed in box A. These operations are then repeated indef- 
initely. Show that the numbers of red balls in box Aform a
Markov chain with stationary transition probabilities, and 
construct the transition matrix of the Markov chain. 
15.Verify the rows of the transition matrix in Exam- 
ple 3.10.6 that correspond to current states {AA, Aa }and
{Aa, aa }.
16.Let the initial probability vector in Example 3.10.6 be 
v=(1/16 ,1/4,1/8,1/4,1/4,1/16 ). Find the probabilities 
of the six states after one generation. 
17.Return to Example 3.10.6. Assume that the state at 
timen−1 is {Aa, aa }.
a. Suppose that we learn that Xn+1is {AA, aa }. Find the 
conditional distribution of Xn. (That is, ﬁnd all the 
probabilities for the possible states at time ngiven
that the state at time n+1 is {AA, aa }.) 
b. Suppose that we learn that Xn+1is {aa, aa }. Find the 
conditional distribution of Xn.
18.Return to Example 3.10.13. Prove that the stationary 
distributions described there are the only stationary dis-
tributions for that Markov chain.
19.Find the unique stationary distribution for the Markov 
chain in Exercise 2. 
20.The unique stationary distribution in Exercise 9 is v=
(0,1,0,0). This is an instance of the following general re- 
sult: Suppose that a Markov chain has exactly one absorb- 
ing state. Suppose further that, for each non-absorbing 
statek, there is nsuch that the probability is positive of 
moving from state kto the absorbing state in nsteps. Then 
the unique stationary distribution has probability 1 in the 
absorbing state. Prove this result. 
3.11 Supplementary Exercises 
1. Suppose that XandYare independent random vari-
ables, that Xhas the uniform distribution on the integers 
1,2,3,4,5 (discrete), and that Yhas the uniform distribu-
tion on the interval [0 ,5] (continuous). Let Zbe a random 
variable such that Z=Xwith probability 1/2 and Z=Y
with probability 1/2. Sketch the c.d.f. of Z.
2. Suppose that XandYare independent random vari-
ables. Suppose that Xhas a discrete distribution concen- 
trated on ﬁnitely many distinct values with p.f. f1. Suppose 
thatYhas a continuous distribution with p.d.f. f2. Let 
Z=X+Y. Show that Zhas a continuous distribution and ﬁnd its p.d.f. Hint: First ﬁnd the conditional p.d.f. of Zgiven
X=x.
3. Suppose that the random variable Xhas the following
c.d.f.: 
F(x) =

0 for x≤0, 
2
5x for 0< x ≤1, 
3
5x−1
5for 1< x ≤2, 
1 for x > 2. 
Verify that Xhas a continuous distribution, and determine 
the p.d.f. of X.
3.11 Supplementary Exercises 203
4. Suppose that the random variable Xhas a continuous 
distribution with the following p.d.f.: 
f(x) =1
2e−|x|for−∞< x < ∞.
Determine the value x0such that F(x 0)=0.9, where F(x) 
is the c.d.f. of X.
5. Suppose that X1andX2are i.i.d. random variables,
and that each has the uniform distribution on the interval 
[0 ,1]. Evaluate Pr (X 2
1+X2
2≤1).
6. For each value of p > 1, let 
c(p) =∞/summationdisplay
x=11
xp.
Suppose that the random variable Xhas a discrete distri- 
bution with the following p.f.: 
f(x) =1
c(p)x pforx=1,2, . . . . 
a. For each ﬁxed positive integer n, determine the prob- 
ability that Xwill be divisible by n.
b. Determine the probability that Xwill be odd. 
7. Suppose that X1andX2are i.i.d. random variables,
each of which has the p.f. f(x) speciﬁed in Exercise 6. 
Determine the probability that X1+X2will be even. 
8. Suppose that an electronic system comprises four com- 
ponents, and let Xjdenote the time until component jfails
to operate (j =1,2,3,4). Suppose that X1,X2,X3, and X4
are i.i.d. random variables, each of which has a continuous 
distribution with c.d.f. F(x) . Suppose that the system will 
operate as long as both component 1 and at least one of 
the other three components operate. Determine the c.d.f. 
of the time until the system fails to operate. 
9. Suppose that a box contains a large number of tacks 
and that the probability Xthat a particular tack will land 
with its point up when it is tossed varies from tack to tack 
in accordance with the following p.d.f.: 
f(x) =/braceleftbigg
2(1−x) for 0< x < 1, 
0 otherwise. 
Suppose that a tack is selected at random from the box 
and that this tack is then tossed three times independently. 
Determine the probability that the tack will land with its
point up on all three tosses. 
10.Suppose that the radius Xof a circle is a random 
variable having the following p.d.f.: 
f(x) =/braceleftbigg1
8(3x+1)for 0< x < 2, 
0 otherwise. 
Determine the p.d.f. of the area of the circle. 11.Suppose that the random variable Xhas the following
p.d.f.: 
f(x) =/braceleftbigg
2e−2xforx > 0, 
0 otherwise. 
Construct a random variable Y=r(X) that has the uni-
form distribution on the interval [0 ,5]. 
12.Suppose that the 12 random variables X1, . . . , X 12 are
i.i.d. and each has the uniform distribution on the interval 
[0 ,20]. For j=0,1, . . . , 19, let Ijdenote the interval ( j,
j+1). Determine the probability that none of the 20 dis- 
joint intervals Ijwill contain more than one of the random 
variables X1, . . . , X 12 .
13.Suppose that the joint distribution of XandYis uni- 
form over a set Ain the xy -plane. For which of the follow- 
ing sets AareXandYindependent?
a. A circle with a radius of 1 and with its center at the 
origin
b. A circle with a radius of 1 and with its center at the 
point(3,5)
c. A square with vertices at the four points (1,1),
(1,−1),(−1,−1), and (−1,1)
d. A rectangle with vertices at the four points (0,0),
(0,3),(1,3), and (1,0)
e. A square with vertices at the four points (0,0),(1,1),
(0,2), and (−1,1)
14.Suppose that XandYare independent random vari-
ables with the following p.d.f.’s: 
f1(x) =/braceleftbigg1 for 0 < x < 1, 
0 otherwise, 
f2(y) =/braceleftbigg
8yfor 0< y < 1
2,
0 otherwise. 
Determine the value of Pr (X > Y) .
15.Suppose that, on a particular day, two persons Aand
Barrive at a certain store independently of each other. 
Suppose that Aremains in the store for 15 minutes and B
remains in the store for 10 minutes. If the time of arrival 
of each person has the uniform distribution over the hour 
between 9:00 a.m. and 10:00 a.m., what is the probability 
thatAandBwill be in the store at the same time? 
16.Suppose that XandYhave the following joint p.d.f.: 
f(x, y) =/braceleftbigg
2(x +y) for 0< x < y < 1, 
0 otherwise. 
Determine (a)Pr (X < 1/2);(b)the marginal p.d.f. of X;
and(c)the conditional p.d.f. of Ygiven that X=x.
204 Chapter 3 Random Variables and Distributions 
17.Suppose that XandYare random variables. The mar- 
ginal p.d.f. of Xis 
f(x) =/braceleftbigg
3x2for 0< x < 1, 
0 otherwise. 
Also, the conditional p.d.f. of Ygiven that X=xis 
g(y |x) =/braceleftBigg
3y2
x3for 0< y < x ,
0 otherwise. 
Determine (a)the marginal p.d.f. of Yand(b)the condi-
tional p.d.f. of Xgiven that Y=y.
18.Suppose that the joint distribution of XandYis uni- 
form over the region in the xy -plane bounded by the four 
linesx=− 1, x=1, y=x+1, and y=x−1. Determine 
(a)Pr (XY > 0)and(b)the conditional p.d.f. of Ygiven
thatX=x.
19.Suppose that the random variables X,Y, and Zhave
the following joint p.d.f.: 
f(x, y, z) =/braceleftbigg
6 for 0 < x < y < z < 1, 
0 otherwise. 
Determine the univariate marginal p.d.f.’s of X,Y, and Z.
20.Suppose that the random variables X,Y, and Zhave
the following joint p.d.f.: 
f(x, y, z) =/braceleftbigg
2 for 0 < x < y < 1 and 0 < z < 1, 
0 otherwise. 
Evaluate Pr (3X > Y |1<4Z < 2).
21.Suppose that XandYare i.i.d. random variables, and 
that each has the following p.d.f.: 
f(x) =/braceleftbigg
e−xforx > 0, 
0 otherwise. 
Also, let U=X/(X +Y) andV=X+Y.
a. Determine the joint p.d.f. of UandV.
b. AreUandVindependent?
22.Suppose that the random variables XandYhave the
following joint p.d.f.: 
f(x, y) =/braceleftbigg
8xy for 0≤x≤y≤1, 
0 otherwise. 
Also, let U=X/Y andV=Y.
a. Determine the joint p.d.f. of UandV.
b. AreXandYindependent?
c. AreUandVindependent?
23.Suppose that X1, . . . , X nare i.i.d. random variables,
each having the following c.d.f.: 
F(x) =/braceleftbigg0 for x≤0, 
1−e−xforx > 0. LetY1=min{X1, . . . , X n}andYn=max{X1, . . . , X n}. De- 
termine the conditional p.d.f. of Y1given that Yn=yn.
24.Suppose that X1,X2, and X3form a random sample of 
three observations from a distribution having the follow- 
ing p.d.f.: 
f(x) =/braceleftbigg
2xfor 0< x < 1, 
0 otherwise. 
Determine the p.d.f. of the range of the sample. 
25.In this exercise, we shall provide an approximate jus- 
tiﬁcation for Eq. (3.6.6). First, remember that if aandb
are close together, then 
/integraldisplayb
ar(t)dt ≈(b −a)r /parenleftbigga+b
2/parenrightbigg
. (3.11.1)
Throughout this problem, assume that XandYhave joint
p.d.f. f.
a. Use (3.11.1) to approximate Pr (y −ǫ < Y ≤y+ǫ) .
b. Use (3.11.1) with r(t) =f(s, t) for ﬁxed sto approx- 
imate
Pr (X ≤xandy−ǫ < Y ≤y+ǫ) 
=/integraldisplayx
−∞/integraldisplayy+ǫ
y−ǫf(s, t) dt ds. 
c. Show that the ratio of the approximation in part (b) 
to the approximation in part (a) is /integraltextx
−∞g1(s |y) ds .
26.LetX1, X 2be two independent random variables each 
with p.d.f. f1(x) =e−xforx > 0 and f1(x) =0 for x≤0. Let 
Z=X1−X2andW=X1/X 2.
a. Find the joint p.d.f. of X1andZ.
b. Prove that the conditional p.d.f. of X1givenZ=0 is 
g1(x 1|0)=/braceleftbigg
2e−2x1forx1>0, 
0 otherwise. 
c. Find the joint p.d.f. of X1andW.
d. Prove that the conditional p.d.f. of X1givenW=1 is 
h1(x 1|1)=/braceleftbigg
4x1e−2x1forx1>0, 
0 otherwise. 
e. Notice that{Z=0}={W=1}, but the conditional 
distribution of X1givenZ=0 is not the same as the 
conditional distribution of X1givenW=1. This dis- 
crepancy is known as the Borel paradox . In light 
of the discussion that begins on page 146 about 
how conditional p.d.f.’s are not like conditioning on 
events of probability 0, show how “ Zvery close to 
0” is not the same as “ Wvery close to 1.” Hint: Draw
a set of axes for x1andx2, and draw the two sets 
{(x 1, x 2):|x1−x2|< ǫ }and{(x 1, x 2):|x1/x 2−1|< ǫ }
and see how much different they are.
3.11 Supplementary Exercises 205
27.Three boys A,B, and Care playing table tennis. In 
each game, two of the boys play against each other and 
the third boy does not play. The winner of any given game 
nplays again in game n+1 against the boy who did not 
play in game n, and the loser of game ndoes not play in 
gamen+1. The probability that Awill beat Bin any game 
that they play against each other is 0.3, the probability that 
Awill beat Cis 0.6, and the probability that Bwill beat
Cis 0.8. Represent this process as a Markov chain with 
stationary transition probabilities by deﬁning the possible 
states and constructing the transition matrix.28.Consider again the Markov chain described in Exer- 
cise 27.(a)Determine the probability that the two boys
who play against each other in the ﬁrst game will play 
against each other again in the fourth game. (b)Show that
this probability does not depend on which two boys play 
in the ﬁrst game. 
29.Find the unique stationary distribution for the Markov 
chain in Exercise 27. 
Probability and Statistics
Chapter 
4Expectation 
4.1 The Expectation ofa RandomVariable 
4.2PropertiesofExpectations 
4.3Variance 
4.4Moments
4.5The Mean and the Median 4.6CovarianceandCorrelation
4.7ConditionalExpectation
4.8Utility
4.9SupplementaryExercises
4.1 The Expectation of a Random Variable 
The distribution of a random variable Xcontains all of the probabilistic infor- 
mationabout X.Theentiredistributionof X,however,isusuallytoocumbersome 
forpresentingthisinformation.Summariesofthedistribution,suchastheaverage 
value,orexpectedvalue,canbeusefulforgivingpeopleanideaofwhereweexpect 
Xtobewithouttryingtodescribetheentiredistribution.Theexpectedvaluealso 
playsanimportantroleintheapproximationmethodsthatariseinChapter6.
Expectation for a Discrete Distribution 
Example 
4.1.1 Fair Price for a Stock . An investor is considering whether or not to invest $18 per 
shareinastockforoneyear.Thevalueofthestockafteroneyear,indollars,willbe 
18 +X,where Xistheamountbywhichthepricechangesovertheyear.Atpresent 
Xis unknown, and the investor would like to compute an “average value” for Xin 
ordertocomparethereturnsheexpectsfromtheinvestmenttowhatshewouldget 
by putting the $18 inthe bank at5%interest. ◭
TheideaofﬁndinganaveragevalueasinExample4.1.1arisesinmanyapplications 
that involve a random variable. One popular choice is what we call the meanor 
expectedvalue or expectation .
The intuitive idea of the mean of a random variable is that it is the weighted 
average of the possible values of the random variable with the weights equal to the 
probabilities.
Example 
4.1.2 Stock Price Change . Suppose that the change in price of the stock in Example 4.1.1 
is a random variable Xthat can assume only the four different values −2,0,1, and 
4,andthatPr (X =− 2)=0.1,Pr (X =0)=0.4,Pr (X =1)=0.3,andPr (X =4)=0.2. 
Then the weightedavarageofthese values is 
−2(0.1)+0(0.4)+1(0.3)+4(0.2)=0.9.
Theinvestornowcomparesthiswiththeinterestthatwouldbeearnedon$18at5%
foroneyear,whichis18 ×0.05 =0.9dollars.Fromthispointofview,thepriceof$18 
seemsfair. ◭
207
208Chapter4 Expectation 
The calculation in Example 4.1.2 generalizes easily to every random variable that 
assumes only ﬁnitely many values. Possible problems arise with random variables 
thatassumemorethanﬁnitelymanyvalues,especiallywhenthecollectionofpossible 
valuesis unbounded. 
Deﬁnition
4.1.1 Mean of Bounded Discrete Random Variable . Let Xbe a bounded discrete random 
variablewhosep.f.is f.The expectationof X,denotedby E(X),isanumberdeﬁned 
as follows: 
E(X)=/summationdisplay
Allxxf(x). (4.1.1)
Theexpectationof Xisalsoreferredtoasthe meanof Xorthe expectedvalueof X.
In Example 4.1.2, E(X)=0.9. Notice that 0.9 is not one of the possible values of X
in thatexample. This is typically the case with discrete randomvariables. 
Example 
4.1.3 Bernoulli Random Variable . Let Xhave the Bernoulli distribution with parameter p,
thatis,assumethat Xtakesonlythetwovalues0and1withPr (X =1)=p.Thenthe 
meanof Xis 
E(X)=0×(1−p)+1×p=p. ◭
If Xis unbounded, it might still be possible to deﬁne E(X)as the weighted 
averageofits possible values. However, some care is needed. 
Deﬁnition
4.1.2 Mean of General Discrete Random Variable . Let Xbeadiscreterandomvariablewhose 
p.f. is f. Suppose thatatleastone ofthe following sums is ﬁnite: 
/summationdisplay
Positive xxf(x),/summationdisplay
Negativexxf(x). (4.1.2)
Thenthe mean,expectation ,or expectedvalue of Xissaidto existandisdeﬁnedtobe 
E(X)=/summationdisplay
Allxxf(x). (4.1.3)
Ifboth ofthe sums in(4.1.2) are inﬁnite, then E(X)doesnotexist .
The reason that the expectation fails to exist if both of the sums in (4.1.2) are 
inﬁnite is that, in such cases, the sum in (4.1.3) is not well-deﬁned. It is known from 
calculus that the sum of an inﬁnite series whose positive and negative terms both 
addtoinﬁnityeitherfailstoconvergeorcanbemadetoconvergetomanydifferent 
values by rearranging the terms in different orders. We don’t want the meaning of 
expected value to depend on arbitrary choices about what order to add numbers. If 
onlyoneoftwosumsin(4.1.3)isinﬁinte,thentheexpectedvalueisalsoinﬁnitewith 
the same sign as that of the sum that is inﬁnite. If both sums are ﬁnite, then the sum 
in (4.1.3) converges anddoesn’tdepend on the order in whichthe terms are added. 
Example 
4.1.4 The Mean of XDoes Not Exist . Let Xbe a randomvariable whose p.f. is 
f(x)=

1
2|x|(|x|+ 1)if x=± 1,±2,±3,...,
0 otherwise. 
4.1 TheExpectationofaRandomVariable 209
It can be veriﬁed that this function satisﬁes the conditions required to be a p.f. The 
two sums in (4.1.2) are 
−∞/summationdisplay
x=− 1x1
2|x|(|x|+ 1)=−∞ and∞/summationdisplay
x=1x1
2x(x +1)=∞;
hence,E(X)does not exist. ◭
Example 
4.1.5 An Inﬁnite Mean . Let Xbe a randomvariable whose p.f. is 
f(x)=

1
x(x +1)if x=1,2,3,...,
0 otherwise. 
The sumover negative values in Eq. (4.1.2) is 0, sothe meanof Xexistsand is 
E(X)=∞/summationdisplay
x=1x1
x(x +1)=∞.
Wesay thatthe mean of Xis inﬁnitein this case. ◭
Note: The Expectation of XDepends Only on the Distribution of X.Although
E(X)is called the expectation of X, it depends only on the distribution of X. Every 
tworandomvariablesthathavethesamedistributionwillhavethesameexpectation
eveniftheyhavenothingtodowitheachother.Forthisreason,weshalloftenrefer 
totheexpectationofadistributionevenifwedonothaveinmindarandomvariable 
with that distribution.
Expectation for a Continuous Distribution 
Theideaofcomputingaweightedaverageofthepossiblevaluescanbegeneralized 
to continuous random variables by using integrals instead of sums. The distinction 
between bounded and unbounded random variables arises in this case for the same 
reasons.
Deﬁnition
4.1.3 Mean of Bounded Continuous Random Variable . Let Xbe a bounded continuous 
random variable whose p.d.f. is f. The expectation of X, denoted E(X), is deﬁned 
as follows: 
E(X)=/integraldisplay∞
−∞xf(x)dx. (4.1.4)
Once again,the expectationis alsocalled the meanor the expectedvalue .
Example 
4.1.6 Expected Failure Time . An appliance has a maximum lifetime of one year. The time 
Xuntilitfails is a randomvariable with a continuous distribution having p.d.f. 
f(x)=/braceleftbigg2xfor 0<x< 1, 
0 otherwise. 
Then 
E(X)=/integraldisplay1
0x(2x)dx =/integraldisplay1
02x2dx =2
3.
Wecan also say thatthe expectation ofthedistribution withp.d.f. fis 2/3. ◭
210Chapter4 Expectation 
For general continuous randomvariables, we modify Deﬁnition 4.1.2. 
Deﬁnition
4.1.4 Mean of General Continuous Random Variable . Let Xbeacontinuousrandomvariable 
whosep.d.f. is f. Suppose thatatleastone ofthe following integrals is ﬁnite: 
/integraldisplay∞
0xf(x)dx,/integraldisplay0
−∞xf(x)dx. (4.1.5)
Then the mean,expectation , or expected value of Xis said to existand is deﬁned to 
be 
E(X)=/integraldisplay∞
−∞xf(x)dx. (4.1.6)
Ifboth ofthe integrals in(4.1.5) are inﬁnite, then E(X)doesnotexist .
Example 
4.1.7 Failure after Warranty . A product has a warranty of one year. Let Xbe the time at 
whichtheproduct fails. Supposethat Xhas a continuous distributionwith the p.d.f. 
f(x)=/braceleftBigg
0 for x< 1, 
2
x3forx≥1. 
The expected time to failure is then 
E(X)=/integraldisplay∞
1x2
x3dx =/integraldisplay∞
12
x2dx =2. ◭
Example 
4.1.8 A Mean That Does Not Exist . Suppose that a random variable Xhas a continuous 
distribution for which thep.d.f. is as follows: 
f(x)=1
π(1+x2)for−∞<x< ∞. (4.1.7)
This distribution is called the Cauchy distribution . We can verify the fact that /integraltext∞
−∞f(x)dx =1by using the following standard resultfromelementary calculus: 
d
dx tan−1x=1
1+x2for−∞<x< ∞.
The two integrals in (4.1.5) are 
/integraldisplay∞
0x
π(1+x2)dx =∞and/integraldisplay0
−∞x
π(1+x2)dx =−∞;
hence, the mean of Xdoes notexist. ◭
Interpretation of the Expectation 
Relation of the Mean to the Center of Gravity The expectation of a random 
variable or, equivalently, the mean of its distribution can be regarded as being the 
centerofgravityofthatdistribution.Toillustratethisconcept,consider,forexample, 
the p.f. sketched in Fig. 4.1. The x-axis may be regarded as a long weightless rod to 
whichweightsareattached.Ifaweightequalto f(x j)isattachedtothisrodateach 
pointxj, then the rod will be balanced ifitis supported atthe point E(X).
Now consider the p.d.f. sketched in Fig. 4.2. In this case, the x-axis may be 
regarded as a long rod over which the mass varies continuously. If the density of 
4.1 TheExpectationofaRandomVariable 211
Figure 4.1 The mean of a 
discrete distribution.
x3x2x1f(x1)f(x2)f(x3)f(x4)
f(x5)
x4 x x4E(X)
Figure 4.2 The mean of a 
continuous distribution.
x E(X)f(x)
the rod at each point xis equal to f(x), then the center of gravity of the rod will be 
locatedatthepoint E(X),andtherodwillbebalancedifitissupportedatthatpoint. 
Itcanbeseenfromthisdiscussionthatthemeanofadistributioncanbeaffected 
greatly by even a very small change in the amount of probability that is assigned to 
a large value of x. For example, the mean of the distribution represented by the p.f. 
inFig.4.1canbemovedtoanyspeciﬁedpointonthe x-axis,nomatterhowfarfrom 
the origin that point may be, by removing an arbitrarily small but positive amount 
of probability from one of the points xjand adding this amount of probability at a 
point far enoughfromthe origin.
Supposenowthatthep.f.orp.d.f. fofsomedistributionissymmetricwithrespect 
toagivenpoint x0onthe x-axis.Inotherwords,supposethat f(x 0+δ)=f(x 0−δ)
for all values of δ. Also assume that the mean E(X)of this distribution exists. In 
accordancewiththeinterpretationthatthemeanisatthecenterofgravity,itfollows 
thatE(X)mustbeequalto x0,whichisthepointofsymmetry.Thefollowingexample 
emphasizes the fact that it is necessary to make certain that the mean E(X)exists
beforeitcan be concluded that E(X)=x0.
Example 
4.1.9 The Cauchy Distribution . Consider again the p.d.f. speciﬁed by Eq. (4.1.7), which is 
sketchedinFig.4.3.Thisp.d.f.issymmetricwithrespecttothepoint x=0.Therefore, 
ifthemeanoftheCauchydistributionexisted,itsvaluewouldhavetobe0.However, 
we saw in Example 4.1.8 thatthe mean of Xdoes notexist.
The reason for the nonexistence of the mean of the Cauchy distribution is as 
follows: When the curve y=f(x)is sketched as in Fig. 4.3, its tails approach the x-
axis rapidly enough to permit the total area under the curve to be equal to 1. On 
the other hand, if each value of f(x)is multiplied by xand the curve y=xf(x)is 
sketched, as in Fig. 4.4, the tails of this curve approach the x-axis so slowly that the 
total areabetweenthe x-axis andeachpartofthe curve is inﬁnite. ◭
212Chapter4 Expectation 
Figure 4.3 The p.d.f. of a 
Cauchy distribution.
0 xf(x)
1
p
1 1 /H110021 /H110022 /H110023 2 3 
√31
√3/H11002
Figure 4.4 The curve 
y=xf(x)for the Cauchy
distribution.
xf(x)
1
2p
1 /H110021 /H110022 /H110023 2 3 
1
2p/H11002
The Expectation of a Function 
Example 
4.1.10 Failure Rate and Time to Failure . Supposethatappliancesmanufacturedbyaparticular 
company fail at a rate of Xper year, where Xis currently unknown and hence is a 
random variable. If we are interested in predicting how long such an appliance will 
last before failure, we might use the mean of 1 /X . How can we calculate the mean 
of Y=1/X ? ◭
Functions of a Single Random Variable If Xis a random variable for which the 
p.d.f. is f, then the expectation of each real-valued function r(X)can be found by 
applying the deﬁnition of expectation to the distribution of r(X)as follows: Let 
Y=r(X), determine the probability distribution of Y, and then determine E(Y)
by applying either Eq. (4.1.1) or Eq. (4.1.4). For example, suppose that Yhas a
continuous distribution with thep.d.f. g. Then 
E[r(X)]=E(Y)=/integraldisplay∞
−∞yg(y)dy, (4.1.8)
ifthe expectation exists. 
Example 
4.1.11 Failure Rate and Time to Failure . In Example 4.1.10, suppose thatthe p.d.f. of Xis 
f(x)=/braceleftbigg
3x2if0 <x< 1, 
0 otherwise. 
4.1 TheExpectationofaRandomVariable 213
Letr(x)=1/x . Usingthe methods ofSec. 3.8, we can ﬁnd thep.d.f. of Y=r(X)as 
g(y)=/braceleftbigg
3y−4if y> 1, 
0 otherwise. 
The mean of Yis then 
E(Y)=/integraldisplay∞
0y3y−4dy =3
2. ◭
Although the method of Example 4.1.11 can be used to ﬁnd the mean of a 
continuous random variable, it is not actually necessary to determine the p.d.f. of 
r(X)in order to calculate the expectation E[r(X)]. In fact, it can be shown that the 
valueof E[r(X)]can always be calculated directly using the following result. 
Theorem
4.1.1 Law of the Unconscious Statistician . Let Xbe a random variable, and let rbe a real- 
valued functionofa real variable. If Xhas a continuous distribution, then 
E[r(X)]=/integraldisplay∞
−∞r(x)f(x)dx, (4.1.9)
ifthe mean exists. If Xhas a discrete distribution, then 
E[r(X)]=/summationdisplay
Allxr(x)f(x), (4.1.10)
ifthe mean exists. 
Proof A general proof will not be given here. However, we shall provide a proof 
for two special cases. First, suppose that the distribution of Xis discrete. Then the 
distribution of Ymust also be discrete. Let gbe the p.f. of Y. For this case, 
/summationdisplay
yyg(y)=/summationdisplay
yyPr[ r(X)=y]
=/summationdisplay
yy/summationdisplay
x:r(x)=yf(x)
=/summationdisplay
y/summationdisplay
x:r(x)=yr(x)f(x)=/summationdisplay
xr(x)f(x).
Hence,Eq.(4.1.10)yieldsthesamevalueasonewouldobtainfromDeﬁnition4.1.1 
applied to Y.
Second, suppose that the distribution of Xis continuous. Suppose also, as in 
Sec.3.8,that r(x)iseitherstrictlyincreasingorstrictlydecreasingwithdifferentiable 
inverses(y). Then, ifwe change variables in Eq. (4.1.9) from xto y=r(x),
/integraldisplay∞
−∞r(x)f(x)dx =/integraldisplay∞
−∞yf [s(y)]/vextendsingle/vextendsingle/vextendsingle/vextendsingleds(y)
dy /vextendsingle/vextendsingle/vextendsingle/vextendsingledy.
Itnow follows fromEq. (3.8.3) thatthe rightside ofthis equation is equal to 
/integraldisplay∞
−∞yg(y)dy.
Hence, Eqs. (4.1.8) and (4.1.9) yield the same value. 
214Chapter4 Expectation 
Theorem 4.1.1 is called the law of the unconscious statistician because many peo- 
ple treat Eqs. (4.1.9) and (4.1.10) as the deﬁnition of E[r(X)] and forget that the 
deﬁnition ofthe mean of Y=r(X)is given in Deﬁnitions 4.1.2 and 4.1.4. 
Example 
4.1.12 Failure Rate and Time to Failure . In Example 4.1.11, we can apply Theorem 4.1.1 to 
ﬁnd
E(Y)=/integraldisplay1
01
x3x2dx =3
2,
thesameresult we gotinExample 4.1.11. ◭
Example 
4.1.13 Determining the Expectation of X1/2. Supposethatthep.d.f.of XisasgiveninExam- 
ple 4.1.6andthat Y=X1/2. Then, by Eq. (4.1.9), 
E(Y)=/integraldisplay1
0x1/2(2x)dx =2/integraldisplay1
0x3/2dx =4
5. ◭
Note:InGeneral, E[g(X)]→negationslash=g(E(X)).InExample4.1.13,themeanof X1/2is4/5. 
The mean of Xwas computed in Example 4.1.6 as 2/3. Note that 4 /5→negationslash=(2/3)1/2. In 
fact, unlessgis a linear function, it is generally the case that E[g(X)]→negationslash=g(E(X)). A 
linearfunction gdoes satisfyE[g(X)]=g(E(X)), as we shall see in Theorem4.2.1. 
Example 
4.1.14 Option Pricing . Suppose that common stock in the up-and-coming company A is 
currently priced at $200 per share. As an incentive to get you to work for company 
A,youmightbeofferedanoptiontobuyacertainnumberofsharesofthestock,one 
yearfromnow,atapriceof$200.Thiscouldbequitevaluableifyoubelievedthatthe 
stock was very likely to rise in price over the next year. For simplicity, suppose that 
thepriceXofthestockoneyearfromnowisadiscreterandomvariablethatcantake 
onlytwovalues(indollars):260and180.Let pbetheprobabilitythat X=260.You 
want to calculate the value of these stock options, either because you contemplate 
the possibility of selling them or because you want to compare Company A’s offer 
towhatothercompaniesareoffering.Let Ybethevalueoftheoptionforoneshare 
whenitexpiresinoneyear.Sincenobodywouldpay$200forthestockiftheprice X
is less than $200, the value of the stock option is 0 if X=180. If X=260, one could 
buythestockfor$200pershareandthenimmediatelysellitfor$260.Thisbringsina 
proﬁtof$60pershare.(Forsimplicity,weshallignoredividendsandthetransaction 
costsofbuying and selling stocks.) Then Y=h(X)where
h(x)=/braceleftbigg0 if x=180, 
60 if x=260. 
Assume that an investor could earn 4% risk-free on any money invested for this 
sameyear.(Assumethatthe4%includesanycompounding.)Ifnootherinvestment 
options were available, a fair cost of the option would then be what is called the 
present value of E(Y)in one year. This equals the value csuch thatE(Y)=1.04 c.
That is, the expected value of the option equals the amount of money the investor 
wouldhaveafter oneyear withoutbuying theoption.We can ﬁnd E(Y)easily:
E(Y)=0×(1−p)+60 ×p=60 p.
So, thefair price ofanoptionto buy one share would be c=60 p/ 1.04 =57 .69 p.
Howshouldonedeterminetheprobability p?Thereisastandardmethodused 
intheﬁnanceindustryforchoosing pinthisexample.Thatmethodistoassumethat 
4.1 TheExpectationofaRandomVariable 215
thepresentvalueofthemeanof X(thestockpriceinoneyear)isequaltothecurrent 
valueofthestockprice.Thatis,assumethattheexpectedvalueofbuyingoneshare 
ofstockandwaitingoneyeartosellisthesameastheresultofinvestingthecurrent 
cost of the stock risk-free for one year (multiplying by 1.04 in this example). In our 
example, this means E(X)=200×1.04. Since E(X)=260p+180(1−p), we set 
200×1.04 =260p+180(1−p),
and obtainp=0.35. The resulting price of an option to buy one share for $200 in 
one year would be $57 .69 ×0.35 =$20.19. This price is called the risk-neutral price
oftheoption. Onecanprove(seeExercise14inthissection)thatanypriceotherthan 
$20.19for the optionwould leadto unpleasantconsequences inthemarket. ◭
FunctionsofSeveralRandomVariables 
Example 
4.1.15 The Expectation of a Function of Two Variables . Let XandYhave a joint p.d.f., and 
suppose that we want the mean of X2+Y2. The most straightforward but most 
difﬁcultwaytodothiswouldbetousethemethodsofSec.3.9toﬁndthedistribution 
of Z=X2+Y2andthen applythe deﬁnition ofmeanto Z. ◭
ThereisaversionofTheorem4.1.1forfunctionsofmorethanonerandomvariable. 
Itsproof is notgiven here. 
Theorem
4.1.2 Law of the Unconscious Statistician . Suppose that X1,...,X nare random variables
withthejointp.d.f. f(x 1,...,x n).Let rbeareal-valuedfunctionof nrealvariables,
andsupposethat Y=r(X 1,...,X n).Then E(Y)canbedetermineddirectlyfromthe 
relation
E(Y)=/integraldisplay
...
Rn/integraldisplay
r(x 1,...,x n)f(x 1,...,x n)dx 1...dx n,
if the mean exists. Similarly, if X1,...,X nhave a discrete joint distribution with p.f. 
f(x 1,...,x n), the meanof Y=r(X 1,...,X n)is 
E(Y)=/summationdisplay
Allx1,...,xnr(x 1,...,x n)f(x 1,...,x n),
ifthe mean exists. 
Example 
4.1.16 Determining the Expectation of a Function of Two Variables . Supposethatapoint( X,Y)
ischosenatrandomfromthesquare Scontainingallpoints( x,y)suchthat0 ≤x≤1
and0≤y≤1. We shall determine the expectedvalue of X2+Y2.
SinceXandYhave the uniform distribution over the square S, and since the 
area of Sis 1, thejointp.d.f. of XandYis 
f(x,y)=/braceleftbigg1 for (x,y)∈S,
0 otherwise. 
Therefore, 
E(X 2+Y2)=/integraldisplay∞
−∞/integraldisplay∞
−∞(x 2+y2)f(x,y)dxdy 
=/integraldisplay1
0/integraldisplay1
0(x 2+y2)dxdy =2
3. ◭
216Chapter4 Expectation 
Note:MoreGeneralDistributions. InExample3.2.7,weintroducedatypeofdistri- 
butionthatwasneitherdiscretenorcontinuous.Itispossibletodeﬁneexpectations 
for such distributions also. The deﬁnition is rather cumbersome, and we shall not 
pursue ithere. 
Summary
The expectation, expected value, or mean of a random variable is a summary of its 
distribution. If the probability distribution is thought of as a distribution of mass 
alongtherealline,thenthemeanisthecenterofmass.Themeanofafunction rofa 
randomvariable Xcanbecalculateddirectlyfromthedistributionof Xwithoutﬁrst
ﬁndingthedistributionof r(X).Similarly,themeanofafunctionofarandomvector 
Xcan be calculated directly fromthe distribution of X.
Exercises 
1. Suppose that Xhas the uniform distribution on the 
interval[a,b ]. Find the mean of X.
2. If an integer between 1 and 100 is to be chosen at 
random, whatis theexpected value? 
3. In a class of 50 students, the number of students niof 
eachageiis shown in the following table: 
Agei n i
18 20 
19 22 
20 4 
21 3 
25 1 
Ifastudentistobeselectedatrandomfromtheclass,what 
is the expected value ofhis age? 
4. Supposethatonewordistobeselectedatrandomfrom 
thesentence the girl put on her beautiful red hat .If X
denotesthenumberoflettersinthewordthatisselected, 
whatis the value of E(X)?
5. Supposethatoneletteristobeselectedatrandomfrom 
the 30 letters in the sentence given in Exercise 4. If Y
denotes the number of letters in the word in which the 
selectedletterappears, whatis the value of E(Y)?
6. Suppose that a random variable Xhas a continuous 
distributionwiththep.d.f. fgiveninExample4.1.6.Find 
theexpectationof1 /X .
7. Supposethatarandomvariable Xhastheuniformdis-
tributionontheinterval[0 ,1].Showthattheexpectation 
of1 /X is inﬁnite. 8. Supposethat XandYhaveacontinuousjointdistribu- 
tionfor which thejointp.d.f. is as follows: 
f(x,y)=/braceleftbigg12 y2for 0≤y≤x≤1, 
0 otherwise. 
Findthe value of E(XY).
9. Suppose that a point is chosen at random on a stick of 
unit length and that the stick is broken into two pieces at 
that point. Find the expected value of the length of the 
longer piece.
10.Suppose that a particle is released at the origin of 
thexy -plane and travels into the half-plane where x> 0. 
Supposethattheparticletravelsinastraightlineandthat 
the angle between the positive half of the x-axis and this
lineis α,whichcanbeeitherpositiveornegative.Suppose, 
ﬁnally,thattheangle αhastheuniformdistributiononthe 
interval[−π/ 2,π/ 2].Let Ybetheordinateofthepointat 
which the particle hits the vertical line x=1. Show that 
the distribution of Yis a Cauchy distribution. 
11.Suppose that the random variables X1,...,X nform
a random sample of size nfrom the uniform distribution
on the interval [0 ,1]. Let Y1=min{X1,...,X n}, and let 
Yn=max{X1,...,X n}.Find E(Y 1)andE(Y n).
12.Suppose that the random variables X1,...,X nform
arandomsampleofsize nfromacontinuousdistribution 
for which the c.d.f. is F, and let the random variables Y1
andYnbe deﬁned as in Exercise 11. Find E[F(Y 1)] and 
E[F(Y n)]. 
13.Astockcurrentlysellsfor$110pershare.Lettheprice 
ofthestockattheendofaone-yearperiodbe X,whichwill 
takeoneofthevalues$100or$300.Supposethatyouhave 
the option to buy shares of this stock at $150 per share 
at the end of that one-year period. Suppose that money 
4.2 Properties ofExpectations 217
couldearn5.8%risk-freeoverthatone-yearperiod.Find 
the risk-neutralprice for theoption to buy oneshare. 
14.Consider the situation of pricing a stock option as in 
Example4.1.14.Wewanttoprovethatapriceotherthan 
$20.19fortheoptiontobuyoneshareinoneyearfor$200 
would be unfair in some way. 
a. Suppose that an investor (who has several shares of 
the stockalready) makesthe followingtransactions.
She buys three more shares of the stock at $200 per 
share and sells four options for $20.19 each. The in- 
vestor must borrow the extra $519.24 necessary to 
make these transactions at 4% for the year. At the 
end of the year, our investor might have to sell four 
shares for $200 each to the person who bought the 
options. In any event, she sells enough stock to pay 
back the amount borrowed plus the 4 percent inter- 
est. Prove that the investor has the same net worth
(withinroundingerror)attheendoftheyearasshe 
would have had without making these transactions,
nomatterwhathappenstothestockprice.(Acombi- 
nationofstocksandoptionsthatproducesnochange 
in networth is called a risk-freeportfolio. )
b. Consider the same transactions as in part (a), but 
this time suppose that the option price is $ xwhere
x< 20 .19. Prove that our investor loses |4.16 x−84 |
dollars of net worth no matter what happens to the 
stock price.c. Consider the same transactions as in part (a), but 
this time suppose that the option price is $ xwhere
x> 20 .19. Prove that our investor gains 4 .16 x−84 
dollars of net worth no matter what happens to the 
stock price.
The situations in parts (b) and (c) are called arbi-
trageopportunities .Suchopportunitiesrarelyexistforany 
length of time in ﬁnancial markets. Imagine what would 
happenifthethreesharesandfouroptionswerechanged 
to threemillionshares andfour million options. 
15.InExample4.1.14,weshowedhowtopriceanoption 
tobuyoneshareofastockataparticularpriceatapartic- 
ular time in the future. This type of option is called a call
option.A putoption isanoptiontosellashareofastock 
at a particular price $ yat a particular time in the future. 
(If you don’t own any shares when you wish to exercise 
the option, you can always buy one at the market price 
and then sell it for $ y.) The same sort of reasoning as in 
Example 4.1.14 could be used to price a put option. Con- 
sider the same stock as in Example 4.1.14 whose price in 
oneyearis Xwiththesamedistributionasintheexample 
and the same risk-free interest rate. Find the risk-neutral 
price for an option to sell one share of that stock in one 
yearatapriceof$220. 
16.LetYbe a discrete random variable whose p.f. is the 
functionfin Example 4.1.4. Let X=|Y|. Prove that the 
distribution of Xhas thep.d.f. inExample 4.1.5. 
4.2 Properties of Expectations 
Inthissection,wepresentsomeresultsthatsimplifythecalculationofexpectations 
forsomecommonfunctionsofrandomvariables.
Basic Theorems 
Supposethat Xisarandomvariableforwhichtheexpectation E(X)exists.Weshall 
present several resultspertaining to the basic properties ofexpectations. 
Theorem
4.2.1Linear Function . If Y=aX +b, where a andb are ﬁnite constants,then 
E(Y)=aE(X)+b.
Proof We ﬁrst shall assume, for convenience, that Xhas a continuous distribution 
for whichthe p.d.f. is f. Then 
E(Y)=E(aX +b)=/integraldisplay∞
−∞(ax +b)f(x)dx 
=a/integraldisplay∞
−∞xf(x)dx +b/integraldisplay∞
−∞f(x)dx 
=aE(X)+b.
A similar proofcan be given for a discrete distribution. 
218Chapter4 Expectation 
Example 
4.2.1Calculating the Expectation of a Linear Function . Suppose that E(X)=5. Then 
E(3X−5)=3E(X)−5=10 
and
E(−3X+15 )=− 3E(X)+15 =0. ◭
The following resultfollows fromTheorem4.2.1with a=0. 
Corollary 
4.2.1If X=cwith probability 1, then E(X)=c.
Example 
4.2.2Investment . An investor is trying to choose between two possible stocks to buy for 
athree-monthinvestment.Onestockcosts$50pershareandhasarateofreturnof 
R1dollarspershareforthethree-monthperiod,where R1isarandomvariable.The 
secondstockcosts$30pershareandhasarateofreturnof R2pershareforthesame
three-month period. The investor has a total of $6000 to invest. For this example, 
suppose that the investor will buy shares of only one stock. (In Example 4.2.3, we 
shall consider strategies in which the investor buys more than one stock.) Suppose 
thatR1has the uniform distribution on the interval [ −10 ,20] and thatR2has the
uniform distribution on the interval [ −4.5,10]. We shall ﬁrst compute the expected 
dollar value of investing in each of the two stocks. For the ﬁrst stock, the $6000 will 
purchase 120 shares, so the return will be 120 R1, whose mean is 120 E(R 1)=600. 
(Solve Exercise 1 in Sec. 4.1 to see why E(R 1)=5.) For the second stock, the $6000 
willpurchase200shares,sothereturnwillbe200 R2,whosemeanis200 E(R 2)=550. 
The ﬁrststock has a higher expected return. 
In addition to calculating expected return, we should also ask which of the two 
investments is riskier. We shall now compute the value at risk (VaR) at probability 
level 0.97 for each investment. (See Example 3.3.7 on page 113.) VaR will be the 
negative of the 1 −0.97 =0.03 quantile for the return on each investment. For the 
ﬁrststock,thereturn120 R1hastheuniformdistributionontheinterval[ −1200,2400]
(see Exercise 14 in Sec. 3.8) whose 0.03 quantile is (according to Example 3.3.8 on 
page 114) 0.03 ×2400+0.97 ×(−1200)=− 1092. So VaR =1092. For the second 
stock, the return 200 R2has the uniform distribution on the interval [ −900,2000]
whose 0.03 quantile is 0 .03 ×2000+0.97 ×(−900)=− 813. So VaR =813. Even 
though the ﬁrst stock has higher expected return, the second stock seems to be 
slightlylessriskyintermsofVaR.Howshouldwebalanceriskandexpectedreturn 
tochoosebetweenthetwopurchases?Onewaytoanswerthisquestionisillustrated 
in Example 4.8.10, after we learn aboututility. ◭
Theorem
4.2.2If there exists a constant such that Pr (X ≥a)=1, then E(X)≥a. If there exists a 
constantb such thatPr (X ≤b)=1, then E(X)≤b.
Proof Weshallassumeagain,forconvenience,that Xhasacontinuousdistribution 
for which the p.d.f. is f, and we shall suppose ﬁrst that Pr (X ≥a)=1. Because Xis 
boundedbelow, the second integral in (4.1.5) is ﬁnite. Then 
E(X)=/integraldisplay∞
−∞xf(x)dx =/integraldisplay∞
axf(x)dx 
≥/integraldisplay∞
aaf(x)dx =aPr (X ≥a)=a.
The proof of the other part of the theorem and the proof for a discrete distribution 
aresimilar.
4.2 Properties ofExpectations 219
Itfollows fromTheorem4.2.2 thatifPr (a ≤X≤b)=1, then a≤E(X)≤b.
Theorem
4.2.3Suppose that E(X)=aand that either Pr (X ≥a)=1 or Pr (X ≤a)=1. Then 
Pr (X =a)=1. 
Proof We shall provide a proof for the case in which Xhas a discrete distribution 
and Pr (X ≥a)=1. The other cases are similar. Let x1,x 2,...include every value
x>a such that Pr (X =x)> 0, ifany. Let p0=Pr (X =a). Then, 
E(X)=p0a+∞/summationdisplay
j=1xjPr (X =xj). (4.2.1)
EachxjinthesumontherightsideofEq.(4.2.1)isgreaterthan a.Ifwereplaceall 
ofthe xj’s by a, the sumcan’tgetlarger, and hence 
E(X)≥p0a+∞/summationdisplay
j=1aPr (X =xj)=a. (4.2.2)
Furthermore,theinequalityinEq.(4.2.2)willbestrictifthereisevenone x>a with
Pr (X =x)> 0. This contradicts E(X)=a. Hence, there can be no x>a such that
Pr (X =x)> 0. 
Theorem
4.2.4If X1,...,X narenrandom variables such that each expectation E(X i)is ﬁnite 
(i=1,...,n), then 
E(X 1+...+Xn)=E(X 1)+...+E(X n).
Proof Weshallﬁrstassumethat n=2andalso,forconvenience,that X1andX2have
a continuous jointdistribution for which the jointp.d.f. is f. Then 
E(X 1+X2)=/integraldisplay∞
−∞/integraldisplay∞
−∞(x 1+x2)f(x 1,x 2)dx 1dx 2
=/integraldisplay∞
−∞/integraldisplay∞
−∞x1f(x 1,x 2)dx 1dx 2+/integraldisplay∞
−∞/integraldisplay∞
−∞x2f(x 1,x 2)dx 1dx 2
=/integraldisplay∞
−∞/integraldisplay∞
−∞x1f(x 1,x 2)dx 2dx 1+/integraldisplay∞
−∞x2f2(x 2)dx 2
=/integraldisplay∞
−∞x1f1(x 1)dx 1+/integraldisplay∞
−∞x2f2(x 2)dx 2
=E(X 1)+E(X 2),
wheref1andf2are the marginal p.d.f.’s of X1andX2. The proof for a discrete 
distribution is similar. Finally, the theorem can be established for each positive 
integernby aninduction argument. 
Itshouldbeemphasizedthat,inaccordancewithTheorem4.2.4,theexpectation 
of the sum of several random variables always equals the sum of their individual 
expectations,regardlessofwhattheirjointdistributionis.Eventhoughthejointp.d.f. 
of X1andX2appearedintheproofofTheorem4.2.4,onlythemarginalp.d.f.’sﬁgured 
intothecalculation of E(X 1+X2).
The nextresultfollows easily fromTheorems 4.2.1 and4.2.4. 
Corollary 
4.2.2Assumethat E(X i)is ﬁnite for i=1,...,n . For all constants a1,...,a nandb,
E(a 1X1+...+anXn+b)=a1E(X 1)+...+anE(X n)+b.
220Chapter4 Expectation 
Example 
4.2.3Investment Portfolio . Supposethattheinvestorwith$6000inExample4.2.2canbuy 
sharesofbothofthetwostocks.Supposethattheinvestorbuys s1sharesoftheﬁrst 
stock at $50 per share and s2shares of the second stock at $30 per share. Such a 
combination of investments is called a portfolio . Ignoring possible problems with 
fractional shares, the values of s1ands2must satisfy
50 s1+30 s2=6000,
in order to invest the entire $6000. The return on this portfolio will be s1R1+s2R2.
The mean returnwill be 
s1E(R 1)+s2E(R 2)=5s1+2.75 s2.
For example, if s1=54 and s2=110, thenthe meanreturn is 572.5. ◭
Example 
4.2.4Sampling without Replacement . Suppose that a box contains red balls and blue balls 
andthattheproportionofredballsintheboxis p(0≤p≤1).Supposethat nballsare
selectedfromtheboxatrandom withoutreplacement ,andlet Xdenotethenumber
ofred balls thatare selected. We shall determine the value of E(X).
We shall begin by deﬁning nrandom variables X1,...,X nas follows: For i=
1,...,n , let Xi=1if the ith ball that is selected is red, and let Xi=0 if the ith ball 
is blue. Since the nballs are selected without replacement, the random variables
X1,...,X nare dependent. However, the marginal distribution of each Xican be 
derived easily (see Exercise 10 of Sec. 1.7). We can imagine that all the balls are 
arranged in the box in some random order, and that the ﬁrst nballs in this arrange- 
ment are selected. Because of randomness, the probability that the ith ball in the 
arrangementwill be red is simply p. Hence, for i=1,...,n ,
Pr (X i=1)=pand Pr (X i=0)=1−p. (4.2.3)
Therefore, E(X i)=1(p)+0(1−p)=p.
From the deﬁnition of X1,...,X n, it follows that X1+...+Xnis equal to the 
total number of red balls that are selected. Therefore, X=X1+...+Xnand, by 
Theorem4.2.4, 
E(X)=E(X 1)+...+E(X n)=np. (4.2.4)
◭
Note:InGeneral, E[g(X)]→negationslash=g(E(X)).Theorems4.2.1and4.2.4implythatif gisa 
linearfunctionofarandomvector X,then E[g(X)]=g(E(X)).Foranonlinearfunc- 
tiong, we have already seen Example 4.1.13 in which E[g(X)]→negationslash=g(E(X)). Jensen’s 
inequality (Theorem 4.2.5) gives a relationship between E[g(X)] and g(E(X))for
anotherspecial class offunctions. 
Deﬁnition
4.2.1Convex Functions . Afunction gofavectorargumentis convexif,forevery α∈(0,1),
andeveryxandy,
g[αx+(1−α)y]≥αg(x)+(1−α)g(y).
The proof of Theorem 4.2.5 is not given, but one special case is left to the reader in 
Exercise 13.
Theorem
4.2.5Jensen’s Inequality . Let gbe a convex function, and let Xbe a random vector with 
ﬁnitemean. Then E[g(X)]≥g(E(X)).
4.2 Properties ofExpectations 221
Example 
4.2.5Sampling with Replacement . Suppose again that in a box containing red balls and 
blue balls, the proportion of red balls is p(0≤p≤1). Suppose now, however, that 
a random sample of nballs is selected from the box withreplacement . If Xdenotes
the number of red balls in the sample, then Xhas the binomial distribution with
parametersnandp, as described in Sec. 3.1. We shall now determine the value 
of E(X).
Asbefore,for i=1,...,n ,let Xi=1ifthe ithballthatisselectedisred,andlet 
Xi=0 otherwise. Then, as before, X=X1+...+Xn. In this problem, the random 
variablesX1,...,X nare independent, and the marginal distribution of each Xiis 
againgivenbyEq.(4.2.3).Therefore, E(X i)=pfori=1,...,n ,anditfollowsfrom 
Theorem4.2.4 that 
E(X)=np. (4.2.5)
Thus, the mean of the binomial distribution with parameters nandpis np . The 
p.f. f(x)of this binomial distribution is given by Eq. (3.1.4), and the mean can be 
computed directlyfromthep.f. as follows: 
E(X)=n/summationdisplay
x=0x/parenleftbiggn
x/parenrightbigg
pxqn−x. (4.2.6)
Hence, by Eq. (4.2.5), the value ofthe sumin Eq. (4.2.6) must be np . ◭
It is seen from Eqs. (4.2.4) and (4.2.5) that the expected number of red balls 
in a sample of nballs is np , regardless of whether the sample is selected with or 
withoutreplacement.However,thedistributionofthenumberofredballsisdifferent 
depending on whether sampling is done with or without replacement (for n> 1). 
For example, Pr (X =n)is always smaller in Example 4.2.4 where sampling is done 
withoutreplacementthaninExample4.2.5wheresamplingisdonewithreplacement, 
if n> 1. (See Exercise 27 in Sec. 4.9.) 
Example 
4.2.6Expected Number of Matches . Suppose that a person types nletters, types the ad- 
dresses on nenvelopes, and then places each letter in an envelope in a random 
manner. Let Xbe the number of letters that are placed in the correct envelopes. 
We shall ﬁnd the mean of X. (In Sec. 1.10, we did a more difﬁcult calculation with 
this same example.) 
For i=1,...,n ,let Xi=1ifthe ithletterisplacedinthecorrectenvelope,and 
letXi=0 otherwise. Then,for i=1,...,n ,
Pr (X i=1)=1
nand Pr (X i=0)=1−1
n.
Therefore, 
E(X i)=1
nfori=1,...,n.
SinceX=X1+...+Xn, itfollows that 
E(X)=E(X 1)+...+E(X n)
=1
n+...+1
n=1.
Thus, the expected value of the number of correct matches of letters and envelopes 
is 1, regardless ofthe value of n. ◭
222Chapter4 Expectation 
Expectation of a Product of Independent Random Variables 
Theorem
4.2.6If X1,...,X narenindependentrandomvariablessuchthateachexpectation E(X i)
is ﬁnite (i=1,...,n), then 
E/parenleftBiggn/productdisplay
i=1Xi/parenrightBigg
=n/productdisplay
i=1E(X i).
Proof We shall again assume, for convenience, that X1,...,X nhave a continuous 
joint distribution for which the joint p.d.f. is f. Also, we shall let fidenote the mar-
ginalp.d.f.of Xi(i=1,...,n).Then,sincethevariables X1,...,X nareindependent,
itfollows thatatevery point (x 1,...,x n)∈Rn,
f(x 1,...,x n)=n/productdisplay
i=1fi(x i).
Therefore, 
E/parenleftBiggn/productdisplay
i=1Xi/parenrightBigg
=/integraldisplay∞
−∞.../integraldisplay∞
−∞/parenleftBiggn/productdisplay
i=1xi/parenrightBigg
f(x 1,...,x n)dx 1...dx n
=/integraldisplay∞
−∞.../integraldisplay∞
−∞/bracketleftBiggn/productdisplay
i=1xifi(x i)/bracketrightBigg
dx 1...dx n
=n/productdisplay
i=1/integraldisplay∞
−∞xifi(x i)dx i=n/productdisplay
i=1E(X i).
The prooffor a discrete distributionis similar. 
ThedifferencebetweenTheorem4.2.4andTheorem4.2.6shouldbeemphasized. 
Ifitisassumedthateachexpectationisﬁnite,theexpectationofthesumofagroup 
of random variables is always equal to the sum of their individual expectations. 
However,theexpectationoftheproductofagroupofrandomvariablesis notalways
equal to the product of their individual expectations. If the random variables are 
independent , then this equality will also hold. 
Example 
4.2.7Calculating the Expectation of a Combination of Random Varia bles . Suppose that X1,
X2,and X3areindependentrandomvariablessuchthat E(X i)=0and E(X 2
i)=1for 
i=1,2,3. We shall determine the value of E[X2
1(X 2−4X3)2]. 
SinceX1,X2, and X3are independent, it follows that the two random variables 
X2
1and(X 2−4X3)2are alsoindependent. Therefore, 
E[X2
1(X 2−4X3)2]=E(X 2
1)E [(X 2−4X3)2]
=E(X 2
2−8X2X3+16 X2
3)
=E(X 2
2)−8E(X 2X3)+16 E(X 2
3)
=1−8E(X 2)E(X 3)+16 
=17 . ◭
Example 
4.2.8Repeated Filtering . Aﬁltrationprocessremovesarandomproportionofparticulates 
in water to which it is applied. Suppose that a sample of water is subjected to this 
process twice. Let X1be the proportion of the particulates that are removed by 
the ﬁrst pass. Let X2be the proportion of what remains after the ﬁrst pass that 
4.2 Properties ofExpectations 223
is removed by the second pass. Assume that X1andX2are independent random
variables with common p.d.f. f(x)=4x3for 0<x< 1 and f(x)=0 otherwise. Let 
Ybe the proportion of the original particulates that remain in the sample after two 
passes. Then Y=(1−X1)(1−X2). Because X1andX2are independent, so too are 
1−X1and1−X2.Since1 −X1and1−X2havethesamedistribution,theyhavethe
samemean, call it µ. Itfollows that Yhas meanµ2. We can ﬁnd µas 
µ=E(1−X1)=/integraldisplay1
0(1−x1)4x3
1dx 1=1−4
5=0.2.
Itfollows that E(Y)=0.22=0.04. ◭
Expectation for Nonnegative Distributions 
Theorem
4.2.7Integer-Valued Random Variables . Let Xbe a random variable that can take only the 
values 0,1,2,....Then 
E(X)=∞/summationdisplay
n=1Pr (X ≥n). (4.2.7)
Proof First, wecanwrite 
E(X)=∞/summationdisplay
n=0nPr (X =n)=∞/summationdisplay
n=1nPr (X =n). (4.2.8)
Next, considerthe following triangulararray ofprobabilities: 
Pr (X =1)Pr (X =2)Pr (X =3)...
Pr (X =2)Pr (X =3)...
Pr (X =3)...
...
We can compute the sum of all the elements in this array in two different ways 
becauseallofthesummandsarenonnegative.First,wecanaddtheelementsineach 
column of the array and then add these column totals. Thus, we obtain the value /summationtext∞
n=1nPr (X =n).Second,wecanaddtheelementsineachrowofthearrayandthen 
add theserow totals. Inthis way we obtain the value /summationtext∞
n=1Pr (X ≥n). Therefore, 
∞/summationdisplay
n=1nPr (X =n)=∞/summationdisplay
n=1Pr (X ≥n).
Eq.(4.2.7) nowfollows fromEq. (4.2.8).
Example 
4.2.9Expected Number of Trials . Supposethatapersonrepeatedlytriestoperformacertain 
taskuntilheissuccessful.Supposealsothattheprobabilityofsuccessoneachgiven 
trial is p(0<p< 1)and that all trials are independent. If Xdenotes the number
of the trial on which the ﬁrst success is obtained, then E(X)can be determined as 
follows.
Since at least one trial is always required, Pr (X ≥1)=1. Also, for n=2,3,...,
at least ntrials will be required if and only if none of the ﬁrst n−1trials results in 
success. Therefore, 
Pr (X ≥n)=(1−p)n−1.
224Chapter4 Expectation 
By Eq. (4.2.7), itfollows that 
E(X)=1+(1−p)+(1−p)2+...=1
1−(1−p)=1
p. ◭
Theorem 4.2.7 has a more general version that applies to all nonnegative random 
variables.
Theorem
4.2.8General Nonnegative Random Variable . Let Xbeanonnegativerandomvariablewith 
c.d.f. F. Then 
E(X)=/integraldisplay∞
0[1 −F(x)]dx. (4.2.9)
The proofofTheorem4.2.8 is leftto the reader in Exercises 1 and 2 in Sec. 4.9. 
Example 
4.2.10 Expected Waiting Time . Let Xbethetimethatacustomerspendswaitingforservice 
in a queue. Supposethatthe c.d.f. of Xis 
F(x)=/braceleftbigg0 if x≤0, 
1−e−2xif x> 0. 
Then the meanof Xis 
E(X)=/integraldisplay∞
0e−2xdx =1
2. ◭
Summary
Themeanofalinearfunctionofarandomvectoristhelinearfunctionofthemean. 
Inparticular,themeanofasumisthesumofthemeans.Asanexample,themeanof 
the binomial distribution with parameters nandpis np . No such relationship holds 
in general for nonlinear functions. For independent random variables, the mean of 
theproductis the productofthe means. 
Exercises 
1. Suppose that the return R(in dollars per share) of a 
stockhastheuniformdistributionontheinterval[ −3,7]. 
Suppose also, that each share of the stock costs $1.50. 
LetYbe the net return (total return minus cost) on an 
investmentof10 shares ofthe stock. Compute E(Y).
2. Suppose that three random variables X1,X2,X3form
a random sample from a distribution for which the mean 
is 5. Determine the value of 
E(2X1−3X2+X3−4).
3. Suppose that three random variables X1,X2,X3form
a random sample from the uniform distribution on the 
interval[0 ,1]. Determine the value of E[(X 1−2X2+X3)2].
4. Suppose that the random variable Xhas the uniform
distribution on the interval [0 ,1], that the random vari- 
ableYhas the uniform distribution on the interval [5 ,9], 
and thatXandYare independent. Suppose also that a
rectangleistobeconstructedforwhichthelengthsoftwo 
adjacentsidesare XandY.Determinetheexpectedvalue 
oftheareaoftherectangle. 
5. Suppose that the variables X1,...,X nform a random 
sample of size nfrom a given continuous distribution on 
the real line for which the p.d.f. is f. Find the expecta- 
tionofthenumberofobservationsinthesamplethatfall 
within a speciﬁedinterval a≤x≤b.
4.3 Variance 225
6. Suppose that a particle starts at the origin of the real 
line and moves along the line in jumps of one unit. For 
eachjump,theprobabilityis p(0≤p≤1)thattheparticle
will jump one unit to the left and the probability is 1 −p
that the particle will jump one unit to the right. Find the 
expectedvalueofthepositionoftheparticleafter njumps.
7. Supposethatoneachplayofacertaingameagambler 
is equally likely to win or to lose. Suppose that when he 
wins, his fortune is doubled, and that when he loses, his 
fortune is cut in half. If he begins playing with a given 
fortunec, what is the expected value of his fortune after 
nindependent playsofthe game? 
8. Suppose that a class contains 10 boys and 15 girls, and 
suppose that eight students are to be selected at random 
from the class without replacement. Let Xdenote the
number of boys that are selected, and let Ydenote the
number ofgirls thatareselected. Find E(X −Y).
9. Suppose that the proportion of defective items in a 
large lot is p, and suppose that a random sample of n
itemsisselectedfromthelot.Let Xdenotethenumberof 
defectiveitemsinthesample,andlet Ydenotethenumber
ofnondefective items. Find E(X −Y).
10.Suppose that a fair coin is tossed repeatedly until a 
headisobtainedfortheﬁrsttime. (a)Whatistheexpected 
number of tosses that will be required? (b)What is the 
expected number of tails that will be obtained before the 
ﬁrsthead is obtained? 
11.Suppose that a fair coin is tossed repeatedly until ex- 
actlykheadshavebeenobtained.Determinetheexpected
numberoftossesthatwillberequired. Hint:Representthe
total number of tosses Xin the form X=X1+...+Xk,whereXiis the number of tosses required to obtain the 
ith head after i−1heads havebeenobtained. 
12.Supposethatthetworeturnrandomvariables R1and
R2inExamples4.2.2and4.2.3areindependent.Consider 
the portfolio at the end of Example 4.2.3 with s1=54 
shares of the ﬁrst stock and s2=110 shares of the second 
stock.
a. Provethatthechangeinvalue Xoftheportfoliohas 
thep.d.f. 
f(x)
=

3.87 ×10 −7(x +1035)if −1035<x< 560, 
6.1728×10 −4if560 ≤x≤585,
3.87 ×10 −7(2180−x)if585 <x< 2180, 
0 otherwise. 
Hint:LookatExample 3.9.5. 
b. Findthevalueatrisk(VaR)atprobabilitylevel0.97 
for theportfolio.
13.Prove the special case of Theorem 4.2.5 in which the 
functiongis twice continuously differentiable and Xis 
one-dimensional. You may assume that a twice continu- 
ouslydifferentiableconvexfunctionhasnonnegativesec-
ondderivative. Hint:Expandg(X)arounditsmeanusing
Taylor’s theorem with remainder. Taylor’s theorem with 
remaindersaysthatif g(x)hastwocontinuousderivatives
g′andg′′at x=x0, then there exists ybetweenx0andx
suchthat
g(x)=g(x 0)+(x −x0)g ′(x 0)+(x −x0)2
2g′′(y).
4.3 Variance 
Although the mean of a distribution is a useful summary, it does not convey 
very much information about the distribution. For example, a random variable 
Xwith mean 2 has the same mean as the constant random variable Ysuch that 
Pr (Y =2)=1evenif Xisnotconstant.Todistinguishthedistributionof Xfrom
the distribution of Yin this case, it might be useful to give some measure of how 
spread out the distribution of Xis. The variance of Xis one such measure. The 
standarddeviationof Xisthesquarerootofthevariance.Thevariancealsoplays 
animportantroleintheapproximationmethodsthatariseinChapter6.
Example 
4.3.1Stock Price Changes . Considertheprices AandBoftwostocksatatimeonemonthin 
thefuture.Assumethat Ahastheuniformdistributionontheinterval[25 ,35]andB
hastheuniformdistributionontheinterval[15 ,45].Itiseasytosee(fromExercise1 
in Sec. 4.1) that both stocks have a mean price of 30. But the distributions are very 
different. For example, Awill surely be worth at least 25 while Pr (B< 25 )=1/3. 
ButBhasmoreupsidepotentialalso.Thep.d.f.’softhesetworandomvariablesare 
plottedin Fig. 4.5. ◭
226Chapter4 Expectation 
Figure 4.5 The p.d.f.’s of 
two uniform distributions
in Example 4.3.1. Both 
distributions have mean
equal to 30, but they are 
spread outdifferently.
10 020 30 40 50 60 0.02 0.04 0.06 0.08 0.10 0.12 Uniform on [25,35]
Uniform on [15,45]
xp.d.f.
Deﬁnitions of the Variance and the Standard Deviation 
Although the two random prices in Example 4.3.1 have the same mean, price B
is more spread out than price A, and it would be good to have a summary of the 
distributionthat makesthis easyto see. 
Deﬁnition
4.3.1Variance/Standard Deviation . Let Xbearandomvariablewithﬁnitemean µ=E(X).
The varianceof X, denoted by Var (X), is deﬁned as follows: 
Var (X)=E[(X −µ)2]. (4.3.1)
If Xhas inﬁnite mean or if the mean of Xdoes not exist, we say that Var (X)does
notexist.The standarddeviationof XisthenonnegativesquarerootofVar (X)ifthe 
varianceexists.
If the expectation in Eq. (4.3.1) is inﬁnite, we say that Var (X)and the standard
deviationof Xare inﬁnite.
When only one random variable is being discussed, it is common to denote its 
standard deviation by the symbol σ, and the variance is denoted by σ2. When more 
than one random variable is being discussed, the name of the random variable is 
included as a subscript to the symbol σ, e.g., σXwould be the standard deviation of 
Xwhileσ2
Ywould be the variance of Y.
Example 
4.3.2Stock Price Changes . Returntothetworandomvariables AandBinExample4.3.1. 
UsingTheorem4.1.1, we can compute 
Var (A)=/integraldisplay35 
25 (a −30 )21
10 da =1
10 /integraldisplay5
−5x2dx =1
10 x3
3/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle5
x=− 5=25 
3,
Var (B)=/integraldisplay45 
15 (b −30 )21
30 db =1
30 /integraldisplay15 
−15 y2dy =1
30 y3
3/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle15 
y=− 15 =75 .
So, Var (B)is nine times as large as Var (A). The standard deviations of AandBare
σA=2.87and σB=8.66. ◭
Note: Variance Depends Only on the Distribution. The variance and standard 
deviation of a random variable Xdepend only on the distribution of X, just as 
the expectation of Xdepends only on the distribution. Indeed, everything that can 
be computed from the p.f. or p.d.f. depends only on the distribution. Two random 
4.3 Variance 227
variables with the same distribution will have the same variance, even if they have 
nothing to do with each other. 
Example 
4.3.3Variance and Standard Deviation of a Discrete Distribution . Suppose that a random 
variableXcan take each of the ﬁve values −2,0,1,3, and 4 with equal probability. 
Weshall determine the variance and standarddeviation of X.
In this example, 
E(X)=1
5(−2+0+1+3+4)=1.2.
Letµ=E(X)=1.2, and deﬁne W=(X −µ)2. Then Var (X)=E(W). We can easily 
computethe p.f. fof W:
x−2 0 1 3 4 
w 10.24 1.44 0.04 3.24 7.84
f(w) 1/5 1/5 1/5 1/5 1/5
Itfollows that 
Var (X)=E(W)=1
5[10.24 +1.44 +0.04 +3.24 +7.84]=4.56 .
The standard deviation of Xis the square rootofthe variance, namely, 2 .135. ◭
There is an alternative method for calculating the variance of a distribution, 
which is often easier touse. 
Theorem
4.3.1Alternative Method for Calculating the Variance . For every random variable X,
Var (X)=E(X 2)−[E(X)]2.
Proof LetE(X)=µ. Then 
Var (X)=E[(X −µ)2]
=E(X 2−2µX +µ2)
=E(X 2)−2µE(X)+µ2
=E(X 2)−µ2.
Example 
4.3.4Variance of a Discrete Distribution . Once again, consider the random variable Xin 
Example 4.3.3, which takes each of the ﬁve values −2,0,1,3, and 4 with equal 
probability. We shall use Theorem 4.3.1 to compute Var (X). In Example 4.3.3, we 
computedthemeanof Xas µ=1.2. To use Theorem4.3.1, we need 
E(X 2)=1
5[(−2)2+02+12+32+42]=6.
BecauseE(X)=1.2, Theorem4.3.1says that 
Var (X)=6−(1.2)2=4.56 ,
which agrees with thecalculationin Example 4.3.3. ◭
Thevariance(aswellasthestandarddeviation)ofadistributionprovidesamea- 
sureofthespreadordispersionofthedistributionarounditsmean µ.Asmallvalueof 
thevarianceindicatesthattheprobabilitydistributionistightlyconcentratedaround 
228Chapter4 Expectation 
µ; a large value of the variance typically indicates that the probability distribution 
has a wide spread around µ. However, the variance of a distribution, as well as its 
mean,canbemadearbitrarilylargebyplacingevenaverysmallbutpositiveamount 
ofprobability far enoughfromthe origin on the real line. 
Example 
4.3.5Slight Modiﬁcation of a Bernoulli Distribution . Let Xbe a discrete random variable 
with thefollowingp.d.f.: 
f(x)=

0.5 if x=0, 
0.499 if x=1, 
0.001 if x=10 ,000, 
0 otherwise. 
There is a sense in which the distribution of Xdiffers very little from the Bernoulli
distribution with parameter 0.5. However, the mean and variance of Xare quite
different from the mean and variance of the Bernoulli distribution with parame- 
ter 0.5. LetYhave the Bernoulli distribution with parameter 0.5. In Example 4.1.3, 
we computed the mean of Yas E(Y)=0.5. Since Y2=Y,E(Y 2)=E(Y)=0.5, so 
Var (Y)=0.5−0.52=0.25.Themeansof XandX2arealsostraightforwardcalcula-
tions:
E(X)=0.5×0+0.499×1+0.001×10 ,000=10 .499
E(X 2)=0.5×02+0.499×12+0.001×10 ,0002=100,000.499.
SoVar (X)=99 ,890.27.Themeanandvarianceof Xaremuchlargerthanthemean
andvariance of Y. ◭
Properties of the Variance 
Weshallnowpresentseveraltheoremsthatstatebasicpropertiesofthevariance.In 
thesetheoremsweshallassumethatthevariancesofalltherandomvariablesexist. 
The ﬁrsttheoremconcerns thepossible values ofthe variance. 
Theorem
4.3.2For each X, Var (X)≥0. If Xis a bounded random variable, then Var (X)must exist
andbe ﬁnite. 
Proof Because Var (X)is the mean of a nonnegative random variable (X −µ)2, it 
must be nonnegative according to Theorem 4.2.2. If Xis bounded, then the mean 
exists, and hence the variance exists. Furthermore, if Xis bounded the so too is 
(X −µ)2, so the variance mustbe ﬁnite. 
Thenexttheoremshowsthatthevarianceofarandomvariable Xcannotbe0unless 
theentire probability distributionof Xis concentrated ata single point. 
Theorem
4.3.3Var (X)=0 ifandonly ifthereexists a constantc such thatPr (X =c)=1. 
Proof Suppose ﬁrst that there exists a constant csuch that Pr (X =c)=1. Then 
E(X)=c, and Pr[ (X −c)2=0] =1. Therefore, 
Var (X)=E[(X −c)2]=0.
Conversely, suppose that Var (X)=0. Then Pr[ (X −µ)2≥0] =1 but 
E[(X −µ)2]=0. Itfollows fromTheorem4.2.3 that 
Pr[ (X −µ)2=0] =1.
Hence, Pr (X =µ)=1. 
4.3 Variance 229
Figure 4.6 The p.d.f. of a 
random variable Xtogether
with the p.d.f.’s of X+3and 
−X. Note that the spreads of 
all three distributions appear
thesame.
0.5 1.0 1.5 p.d.f. of x
p.d.f. of x/H11001 3 
p.d.f. of x
xp.d.f.
/H11002202 4 6 
Theorem
4.3.4For constants aandb, let Y=aX +b. Then 
Var (Y)=a2Var (X),
andσY=|a|σX.
Proof If E(X)=µ, then E(Y)=aµ +bby Theorem4.2.1. Therefore, 
Var (Y)=E[(aX +b−aµ −b)2]=E[(aX −aµ)2]
=a2E[(X −µ)2]=a2Var (X).
Taking thesquare rootofVar (Y)yields|a|σX.
It follows from Theorem 4.3.4 that Var (X +b)=Var (X)for every constant b.
Thisresultisintuitivelyplausible,sinceshiftingtheentiredistributionof Xadistance 
of bunits along the real line will change the mean of the distribution by bunits but
theshiftwillnotaffectthedispersionofthedistributionarounditsmean.Figure4.6 
shows the p.d.f. a random variable Xtogether with the p.d.f. of X+3 to illustrate 
howa shiftofthe distribution does notaffectthe spread. 
Similarly,itfollowsfromTheorem4.3.4thatVar (−X)=Var (X).Thisresultalso 
isintuitivelyplausible,sincereﬂectingtheentiredistributionof Xwithrespecttothe 
originofthereallinewillresultinanewdistributionthatisthemirrorimageofthe 
original one. The mean will be changed from µto −µ, but the total dispersion of 
thedistributionarounditsmeanwillnotbeaffected.Figure4.6showsthep.d.f.ofa 
randomvariable Xtogetherwiththep.d.f.of −Xtoillustratehowareﬂectionofthe 
distribution does notaffectthe spread.
Example 
4.3.6Calculating the Variance and Standard Deviation of a Linear Function . Considerthesame 
randomvariable XasinExample4.3.3,whichtakeseachoftheﬁvevalues −2,0,1,3, 
and4withequalprobability.Weshalldeterminethevarianceandstandarddeviation 
of Y=4X−7. 
In Example 4.3.3, we computed the mean of Xas µ=1.2 and the variance as 
4.56.By Theorem4.3.4, 
Var (Y)=16Var (X)=72 .96 .
Also, the standarddeviation σof Yis 
σY=4σX=4(4.56 )1/2=8.54 . ◭
230Chapter4 Expectation 
Thenexttheoremprovidesanalternativemethodforcalculatingthevarianceof 
a sumofindependentrandomvariables. 
Theorem
4.3.5If X1,...,X nare independent randomvariables with ﬁnitemeans, then 
Var (X 1+...+Xn)=Var (X 1)+...+Var (X n).
Proof Suppose ﬁrstthat n=2. If E(X 1)=µ1andE(X 2)=µ2, then 
E(X 1+X2)=µ1+µ2.
Therefore, 
Var (X 1+X2)=E[(X 1+X2−µ1−µ2)2]
=E[(X 1−µ1)2+(X 2−µ2)2+2(X 1−µ1)(X2−µ2)]
=Var (X 1)+Var (X 2)+2E[(X 1−µ1)(X2−µ2)].
SinceX1andX2are independent,
E[(X 1−µ1)(X2−µ2)]=E(X 1−µ1)E(X 2−µ2)
=(µ 1−µ1)(µ2−µ2)
=0.
Itfollows, therefore, that 
Var (X 1+X2)=Var (X 1)+Var (X 2).
Thetheoremcannowbeestablishedforeachpositiveinteger nbyaninduction 
argument.
It should be emphasized that the random variables in Theorem 4.3.5 must be 
independent.Thevarianceofthesumofrandomvariablesthatarenotindependent 
will be discussed in Sec. 4.6. By combining Theorems 4.3.4 and 4.3.5, we can now 
obtain thefollowingcorollary.
Corollary 
4.3.1If X1,...,X nareindependentrandomvariableswithﬁnitemeans,andif a1,...,a n
andbarearbitrary constants, then 
Var (a 1X1+...+anXn+b)=a2
1Var (X 1)+...+a2
nVar (X n).
Example 
4.3.7Investment Portfolio . An investor with $100,000 to invest wishes to construct a port- 
folio consisting of shares of one or both of two available stocks and possibly some 
ﬁxed-rate investments. Suppose that the two stocks have random rates of return R1
andR2per share for a period of one year. Suppose that R1has a distribution with 
mean6andvariance55,while R2hasmean4andvariance28.Supposethattheﬁrst 
stock costs $60 per share and the second costs $48 per share. Suppose that money 
canalsobeinvestedataﬁxedrateof3.6percentperyear.Theportfoliowillconsist 
of s1sharesoftheﬁrststock, s2sharesofthesecondstock,andallremainingmoney 
($ s3) invested atthe ﬁxed rate. The return on this portfolio will be 
s1R1+s2R2+0.036s3,
wherethe coefﬁcients areconstrained by 
60 s1+48 s2+s3=100,000, (4.3.2)
4.3 Variance 231
Figure 4.7 The set of all 
means and variances of 
investment portfolios in 
Example 4.3.7. The solid 
vertical line shows the range
of possible variances for 
portfoloios with a mean of 
7000.
4000 5000 6000 7000 8000 9000 10,0000Efficient portfolio
with mean 7000Efficient portfoliosRange of variances
Mean of portfolio returnVariance of portfolio return 1.5/H1100310 8
1/H1100310 8
5/H1100310 7
2.55/H1100310 7
aswellas s1,s 2,s 3≥0.Fornow,weshallassumethat R1andR2areindependent.The 
meanandthevarianceofthe return onthe portfolio will be 
E(s 1R1+s2R2+0.036s3)=6s1+4s2+0.036s3,
Var (s 1R1+s2R2+0.036s3)=55 s2
1+28 s2
2.
One method for comparing a class of portfolios is to say that portfolio A is at least 
asgoodasportfolioBifthemeanreturnforAisatleastaslargeasthemeanreturn 
for B and if the variance for A is no larger than the variance of B. (See Markowitz, 
1987, for a classic treatment of such methods.) The reason for preferring smaller 
variance is that large variance is associated with large deviations from the mean, 
and for portfolios with a common mean, some of the large deviations are going to 
have to be below the mean, leading to the risk of large losses. Figure 4.7 is a plot 
of the pairs (mean, variance) for all of the possible portfolios in this example. That 
is, for each (s 1,s 2,s 3)that satisfy (4.3.2), there is a point in the outlined region of 
Fig.4.7.Thepointstotherightandtowardthebottomarethosethathavethelargest 
mean return for a ﬁxed variance, and the ones that have the smallest variance for 
a ﬁxed mean return. These portfolios are called efﬁcient. For example, suppose that 
theinvestorwouldlikeameanreturnof7000.Theverticallinesegmentabove7000 
onthehorizontalaxisinFig.4.7indicatesthepossiblevariancesofallportfolioswith 
meanreturnof7000.Amongthese,theportfoliowiththesmallestvarianceisefﬁcient 
andisindicatedinFig.4.7.Thisportfoliohas s1=524.7, s2=609.7, s3=39 ,250,and 
variance2.55 ×10 7.So,everyportfoliowithmeanreturngreaterthan7000musthave 
variancelargerthan2 .55 ×10 7,andeveryportfoliowithvariancelessthan2 .55 ×10 7
must havemean returnsmaller than 7000. ◭
The Variance of a Binomial Distribution 
We shall now consider again the method of generating a binomial distribution pre- 
sented in Sec. 4.2. Suppose that a box contains red balls and blue balls, and that the 
proportionofredballsis p(0≤p≤1).Supposealsothatarandomsampleof nballs
is selected from the box with replacement. For i=1,...,n , let Xi=1if the ith ball 
thatisselectedisred,andlet Xi=0otherwise.If Xdenotesthetotalnumberofred 
ballsinthesample,then X=X1+...+XnandXwillhavethebinomialdistribution
with parameters nandp.
232Chapter4 Expectation 
Figure 4.8 Two binomial 
distributions with the same
mean (2.5) but different
variances.
0.05 0.10 0.15 0.20 0.25 0.30 
xp.f.
204 6 8 10 n/H11005 5, p/H11005 0.5
n/H11005 10, p/H11005 0.25
SinceX1,...,X nareindependent,itfollows fromTheorem4.3.5 that 
Var (X)=n/summationdisplay
i=1Var (X i).
According to Example 4.1.3, E(X i)=pfori=1,...,n . Since X2
i=Xifor eachi,
E(X 2
i)=E(X i)=p. Therefore, by Theorem4.3.1, 
Var (X i)=E(X 2
i)−[E(X i)]2
=p−p2=p(1−p).
Itnow follows that 
Var (X)=np(1−p). (4.3.3)
Figure 4.8 compares two different binomial distributions with the same mean 
(2.5) but different variances (1.25 and 1.875). One can see how the p.f. of the distri- 
bution with the larger variance ( n=10, p=0.25) is higher at more extreme values 
and lower at more central values than is the p.f. of the distribution with the smaller 
variance(n=5, p=0.5).Similarly,Fig.4.5comparestwouniformdistributionswith 
thesamemean(30)anddifferentvariances(8.33and75).Thesamepatternappears, 
namely that the distribution with larger variance has higher p.d.f. at more extreme 
values andlowerp.d.f. atmore central values. 
Interquartile Range 
Example 
4.3.8The Cauchy Distribution . In Example 4.1.8, we saw a distribution (the Cauchy dis- 
tribution) whose mean did not exist, and hence its variance does not exist. But, we 
might still want to describe how spread out such a distribution is. For example, if X
hastheCauchydistributionand Y=2X,itstandstoreasonthat Yistwiceasspread 
out as Xis, buthowdowe quantify this? ◭
There is a measure of spread that exists for every distribution, regardless of 
whetherornotthedistributionhasameanorvariance.RecallfromDeﬁnition3.3.2 
that the quantile function for a random variable is the inverse of the c.d.f., and it is 
deﬁnedfor everyrandomvariable.
4.3 Variance 233
Deﬁnition
4.3.2Interquartile Range (IQR) . Let Xbearandomvariablewithquantilefunction F−1(p)
for 0<p< 1. The interquartilerange(IQR) is deﬁned to be F−1(0.75 )−F−1(0.25 ).
In words, the IQR is the length of the interval that contains the middle half of the 
distribution.
Example 
4.3.9The Cauchy Distribution . Let Xhave the Cauchy distribution. The c.d.f. Fof Xcan
be found using a trigonometric substitution in the followingintegral: 
F(x)=/integraldisplayx
−∞dy 
π(1+y2)=1
2+tan−1(x)
π,
where tan−1(x)is the principal inverse of the tangent function, taking values from 
−π/ 2 to π/ 2 as xruns from−∞to ∞. The quantile function of Xis then F−1(p)=
tan[π(p −1/2)]for 0 <p< 1. The IQRis 
F−1(0.75 )−F−1(0.25 )=tan(π/ 4)−tan(−π/ 4)=2.
It is not difﬁcult to show that, if Y=2X, then the IQR of Yis 4. (See Exercise 14.) 
◭
Summary
Thevarianceof X,denotedbyVar (X),isthemeanof[ X−E(X)]2andmeasureshow
spread out the distribution of Xis. The variance also equals E(X 2)−[E(X)]2. The 
standarddeviationisthesquarerootofthevariance.Thevarianceof aX +b,where 
aandbareconstants,is a2Var (X).Thevarianceofthesumofindependentrandom 
variables is the sum of the variances. As an example, the variance of the binomial 
distribution with parameters nandpis np(1−p). The interquartile range (IQR) is 
the difference between the 0.75 and 0.25 quantiles. The IQR is a measure of spread 
that existsfor everydistribution.
Exercises 
1. Suppose that Xhas the uniform distribution on the 
interval [0 ,1]. Compute the variance of X.
2. Suppose that one word is selected at random from the 
sentence the girl put on her beautiful red hat . If X
denotesthenumberoflettersinthewordthatisselected, 
what is the value ofVar (X)?
3. For all numbers aandbsuch thata<b , ﬁnd the vari- 
ance ofthe uniformdistribution on the interval[ a,b ]. 
4. Supposethat Xisarandomvariableforwhich E(X)=
µand Var (X)=σ2. Show that 
E[X(X −1)]=µ(µ −1)+σ2.
5. LetXbe a random variable for which E(X)=µand
Var (X)=σ2,andlet cbeanarbitraryconstant.Showthat 
E[(X −c)2]=(µ −c)2+σ2.6. Suppose that XandYare independent random vari-
ables whose variances exist and such that E(X)=E(Y).
Showthat
E[(X −Y)2]=Var (X)+Var (Y).
7. Suppose that XandYare independent random vari-
ables for which Var (X)=Var (Y)=3. Find the values of 
(a)Var (X −Y)and(b)Var (2X−3Y+1).
8. Construct an example of a distribution for which the 
mean is ﬁnite butthe variance is inﬁnite. 
9. LetXhave the discrete uniform distribution on the 
integers 1,...,n . Compute the variance of X.Hint:You 
may wish to use the formula /summationtextn
k=1k2=n(n+1).(2n+
1)/ 6. 
234Chapter4 Expectation 
10.Considertheexampleefﬁcientportfolioattheendof 
Example 4.3.7. Suppose that Rihas the uniform distribu-
tionon the interval [ ai,b i]for i=1,2. 
a. Find the two intervals [ a1,b 1]and [ a2,b 2]. Hint:The 
intervalsaredeterminedbythemeansandvariances. 
b. Findthevalueatrisk(VaR)fortheexampleportfolio 
atprobabilitylevel0.97. Hint:ReviewExample3.9.5
toseehowtoﬁndthep.d.f.ofthesumoftwouniform 
random variables.11.LetXhave the uniform distribution on the interval 
[0 ,1].FindtheIQRof X.
12.LetXhave the p.d.f. f(x)=exp(−x)forx≥0, and 
f(x)=0 for x< 0. FindtheIQR of X.
13.LetXhavethebinomialdistributionwithparameters
5 and 0.3. Find the IQR of X.Hint:Return to Exam- 
ple3.3.9 andTable 3.1. 
14.LetXbearandomvariablewhoseinterquartilerange 
is η.Let Y=2X.Provethattheinterquartilerangeof Yis 
2η.
4.4 Moments 
For a random variable X, the means of powers Xk(called moments) for k> 
2have useful theoretical properties, and some of them are used for additional 
summaries of a distribution. The moment generating function is a related tool 
that aids in deriving distributions of sums of independent random variables and 
limitingpropertiesofdistributions.
Existence of Moments 
For each random variable Xand every positive integer k, the expectation E(X k)is 
called thekth moment of X . In particular, in accordance with this terminology, the 
meanof Xis the ﬁrstmomentof X.
It is said that the kth moment exists if and only if E(|X|k)< ∞. If the random 
variableXis bounded, that is, if there are ﬁnite numbers aandbsuch that Pr (a ≤
X≤b)=1,thenallmomentsof Xmustnecessarilyexist.Itispossible,however,that 
allmomentsof Xexisteventhough Xisnotbounded.Itisshowninthenexttheorem 
that ifthe kth momentof Xexists, then all moments oflower order must also exist. 
Theorem
4.4.1If E(|X|k)< ∞for some positive integer k, then E(|X|j)< ∞for every positive
integerjsuch thatj<k .
Proof Weshallassume,forconvenience,thatthedistributionof Xiscontinuousand 
the p.d.f. is f. Then 
E(|X|j)=/integraldisplay∞
−∞|x|jf(x)dx 
=/integraldisplay
|x|≤ 1|x|jf(x)dx +/integraldisplay
|x|>1|x|jf(x)dx 
≤/integraldisplay
|x|≤ 11.f(x)dx +/integraldisplay
|x|>1|x|kf(x)dx 
≤Pr (|X|≤ 1)+E(|X|k).
By hypothesis, E(|X|k)< ∞. It therefore follows that E(|X|j)< ∞. A similar proof 
holdsfor a discrete or a more general type ofdistribution. 
In particular, it follows from Theorem 4.4.1 that if E(X 2)< ∞, then both the 
mean of Xand the variance of Xexist. Theorem 4.4.1 extends to the case in which 
4.4 Moments 235
jandkare arbitrary positive numbers rather than just integers. (See Exercise 15 in 
this section.)We will notmake use ofsuch a resultin this text, however. 
Central Moments Suppose that Xis a random variable for which E(X)=µ. For 
everypositiveinteger k,theexpectation E[(X −µ)k]iscalledthe kth centralmoment
of Xor the kth momentofXaboutthemean . In particular, in accordance with this 
terminology, the variance of Xis the second centralmomentof X.
For every distribution, the ﬁrstcentral momentmustbe 0 because 
E(X −µ)=µ−µ=0.
Furthermore, if the distribution of Xis symmetric with respect to its mean µ, and if 
the central moment E[(X −µ)k]exists for a given odd integer k, then the value of 
E[(X −µ)k]willbe0becausethepositiveandnegativetermsinthisexpectationwill 
cancel one another.
Example 
4.4.1A Symmetric p.d.f . Suppose that Xhas a continuous distribution for which the p.d.f. 
hasthe followingform:
f(x)=ce −(x −3)2/2for−∞<x< ∞.
Weshall determine the meanof Xandallthe centralmoments.
Itcan be shown thatfor every positive integer k,
/integraldisplay∞
−∞|x|ke−(x −3)2/2dx< ∞.
Hence,allthemomentsof Xexist.Furthermore,since f(x)issymmetricwithrespect 
to the point x=3, then E(X)=3. Because of this symmetry, it also follows that 
E[(X −3)k]=0 for every odd positive integer k. For even k=2n, we can ﬁnd a 
recursive formula for the sequence of central moments. First, let y=x−µin all 
theintegralfomulas. Then, for n≥1, the 2 nth central momentis 
m2n=/integraldisplay∞
−∞y2nce −y2/2dy.
Use integration by parts with u=y2n−1anddv =ye −y2/2dy . It follows that du =
(2n−1)y 2n−2dy andv=− e−y2/2. So, 
m2n=/integraldisplay∞
−∞udv =uv |∞
y=−∞−/integraldisplay∞
−∞vdu 
=− y2n−1e−y2/2/vextendsingle/vextendsingle/vextendsingle∞
y=−∞+(2n−1)/integraldisplay∞
−∞y2n−2ce −y2/2dy 
=(2n−1)m 2(n −1).
Becausey0=1, m0is just the integral of the p.d.f.; hence, m0=1. It follows that 
m2n=/producttextn
i=1(2i−1)forn=1,2,....So,forexample, m2=1, m4=3, m6=15,andso 
on. ◭
Skewness In Example 4.4.1, we saw that the odd central moments are all 0 for a 
distributionthatissymmetric.Thisleadstothefollowingdistributionalsummarythat 
is used to measure lackofsymmetry. 
Deﬁnition
4.4.1Skewness . Let Xbearandomvariablewithmean µ,standarddeviation σ,andﬁnite 
thirdmoment.The skewness of Xis deﬁned to be E[(X −µ)3]/σ 3.
236Chapter4 Expectation 
The reason for dividing the third central moment by σ3is to make the skewness 
measureonly thelackofsymmetry rather than the spread ofthe distribution. 
Example 
4.4.2Skewness of Binomial Distributions . Let Xhavethebinomialdistributionwithparam-
eters 10 and 0.25. The p.f. of this distribution appears in Fig. 4.8. It is not difﬁcult to 
see that the p.f. is not symmetric. The skewness can be computed as follows: First, 
note that themeanis µ=10 ×0.25 =2.5and thatthe standard deviation is 
σ=(10 ×0.25 ×0.75 )1/2=1.369.
Second,compute
E[(X −2.5)3]=(0−2.5)3/parenleftbigg10 
0/parenrightbigg
0.25 00.75 10 +...+(10 −2.5)3/parenleftbigg10 
10 /parenrightbigg
0.25 00 0.75 0
=0.9375.
Finally, the skewness is 
0.9375
1.3693=0.3652.
Forcomparison,theskewnessofthebinomialdistributionwithparameters10and0.2 
is 0.4743, and the skewness of the binomial distribution with parameters 10 and 0.3 
is 0.2761. The absolute value of the skewness increases as the probability of success 
movesawayfrom0.5.Itisstraightforwardtoshowthattheskewnessofthebinomial 
distributionwithparameters nandpisthenegativeoftheskewnessofthebinomial 
distribution with parameters nand1−p. (See Exercise 16 inthis section.) ◭
Moment Generating Functions 
We shall now consider a different way to characterize the distribution of a random 
variable that is more closely related to its moments than to where its probability is 
distributed.
Deﬁnition
4.4.2Moment Generating Function . Let Xbe a random variable. For each real number t,
deﬁne
ψ(t)=E(e tX ). (4.4.1)
Thefunction ψ(t)iscalledthe momentgeneratingfunction (abbreviatedm.g.f.)of X.
Note: The Moment Generating Function of XDepends Only on the Distribution 
of X.Since the m.g.f. is the expected value of a function of X, it must depend only 
on the distribution of X. If XandYhave the same distribution, they must have the
samem.g.f. 
If the random variable Xis bounded, then the expectation in Eq. (4.4.1) must 
be ﬁnite for all values of t. In this case, therefore, the m.g.f. of Xwill be ﬁnite for all 
valuesof t.Ontheotherhand,if Xisnotbounded,thenthem.g.f.mightbeﬁnitefor 
some values of tand might not be ﬁnite for others. It can be seen from Eq. (4.4.1), 
however,thatforeveryrandomvariable X,them.g.f. ψ(t)mustbeﬁniteatthepoint 
t=0 and atthatpointits value mustbe ψ(0)=E(1)=1. 
The nextresultexplains how the name “momentgenerating function” arose. 
Theorem
4.4.2LetXbearandomvariableswhosem.g.f. ψ(t)isﬁniteforallvaluesof tinsomeopen 
interval around the point t=0. Then, for each integer n> 0, the nth moment of X,
4.4 Moments 237
E(X n),isﬁniteandequalsthe nthderivative ψ(n)(t)at t=0.Thatis, E(X n)=ψ(n)(0)
forn=1,2,....
Wesketch the proofatthe end ofthis section. 
Example 
4.4.3Calculating an m.g.f . Suppose that Xis a random variable for which the p.d.f. is as 
follows:
f(x)=/braceleftbigg
e−xforx> 0, 
0 otherwise. 
Weshall determine the m.g.f. of XandalsoVar (X).
For each real number t,
ψ(t)=E(e tX )=/integraldisplay∞
0etx e−xdx 
=/integraldisplay∞
0e(t−1)x dx.
Theﬁnalintegralinthisequationwillbeﬁniteifandonlyif t< 1.Therefore, ψ(t)is 
ﬁnite onlyfor t< 1. For each such value of t,
ψ(t)=1
1−t.
Sinceψ(t)is ﬁnite for all values of tin an open interval around the point t=0, 
all momentsof Xexist. The ﬁrsttwo derivatives of ψare
ψ′(t)=1
(1−t)2andψ′′(t)=2
(1−t)3.
Therefore, E(X)=ψ′(0)=1and E(X 2)=ψ′′(0)=2. Itnowfollows that 
Var (X)=ψ′′(0)−[ψ′(0)]2=1. ◭
Properties of Moment Generating Functions 
We shall now present three basic theorems pertaining to moment generating func- 
tions.
Theorem
4.4.3LetXbearandomvariableforwhichthem.g.f.is ψ1;let Y=aX +b,where aandb
aregivenconstants;andlet ψ2denotethem.g.f.of Y.Thenforeveryvalueof tsuch
thatψ1(at)is ﬁnite, 
ψ2(t)=ebtψ1(at). (4.4.2)
Proof By the deﬁnition ofanm.g.f., 
ψ2(t)=E(e tY )=E[et(aX +b)]=ebtE(e atX )=ebtψ1(at).
Example 
4.4.4Calculating the m.g.f. of a Linear Function . Suppose that the distribution of Xis as 
speciﬁed in Example 4.4.3. We saw thatthe m.g.f. of Xfort< 1is 
ψ1(t)=1
1−t.
If Y=3−2X, then the m.g.f. of Yis ﬁnite for t> −1/2 and will have the value 
ψ2(t)=e3tψ1(−2t)=e3t
1+2t. ◭
238Chapter4 Expectation 
The next theorem shows that the m.g.f. of the sum of an arbitrary number of 
independentrandomvariableshasaverysimpleform.Becauseofthisproperty,the 
m.g.f. is an importanttool in the study ofsuch sums. 
Theorem
4.4.4Supposethat X1,...,X narenindependentrandomvariables;andfor i=1,...,n ,
letψidenotethem.g.f.of Xi.Let Y=X1+...+Xn,andletthem.g.f.of Ybedenoted 
by ψ. Then for every value of tsuch thatψi(t)is ﬁnite for i=1,...,n ,
ψ(t)=n/productdisplay
i=1ψi(t). (4.4.3)
Proof By deﬁnition, 
ψ(t)=E(e tY )=E[et(X 1+...+Xn)]=E/parenleftBiggn/productdisplay
i=1etX i/parenrightBigg
.
Since the random variables X1,...,X nare independent, it follows from Theo- 
rem 4.2.6that
E/parenleftBiggn/productdisplay
i=1etX i/parenrightBigg
=n/productdisplay
i=1E(e tX i).
Hence,
ψ(t)=n/productdisplay
i=1ψi(t).
The Moment Generating Function for the Binomial Distribution Suppose that
a random variable Xhas the binomial distribution with parameters nandp. In 
Sections4.2and4.3,themeanandthevarianceof Xweredeterminedbyrepresenting 
Xasthesumof nindependentrandomvariables X1,...,X n.Inthisrepresentation, 
thedistribution ofeach variable Xiis as follows: 
Pr (X i=1)=pand Pr (X i=0)=1−p.
Weshall now use this representationtodetermine the m.g.f. of X=X1+...+Xn.
Since each of the random variables X1,...,X nhas the same distribution, the
m.g.f. ofeach variable will be the same. For i=1,...,n , the m.g.f. of Xiis 
ψi(t)=E(e tX i)=(e t)Pr (X i=1)+(1)Pr (X i=0)
=pe t+1−p.
Itfollows fromTheorem4.4.4 thatthe m.g.f. of Xin this case is 
ψ(t)=(pe t+1−p)n. (4.4.4)
Uniqueness of Moment Generating Functions We shall now state one more im- 
portant property of the m.g.f. The proof of this property is beyond the scope of this 
book andis omitted. 
Theorem
4.4.5Ifthem.g.f.’softworandomvariables X1andX2areﬁniteandidenticalforallvalues
of tin an open interval around the point t=0, then the probability distributions of 
X1andX2must be identical. 
4.4 Moments 239
Theorem4.4.5isthejustiﬁcationfortheclaimmadeatthestartofthisdiscussion, 
namely, that the m.g.f. is another way to characterize the distribution of a random 
variable.
TheAdditivePropertyoftheBinomialDistribution Momentgeneratingfunctions
provide a simple way to derive the distribution of the sum of two independent 
binomial randomvariables withthesame secondparameter.
Theorem
4.4.6If X1andX2areindependentrandomvariables,andif Xihasthebinomialdistribu-
tion with parameters niandp(i=1,2), then X1+X2has the binomial distribution
with parameters n1+n2andp.
ProofL et ψidenote them.g.f. of Xifori=1,2. Itfollows fromEq. (4.4.4) that 
ψi(t)=(pe t+1−p)ni.
Letψdenotethem.g.f. of X1+X2. Then, by Theorem4.4.4, 
ψ(t)=(pe t+1−p)n1+n2.
It can be seen from Eq. (4.4.4) that this function ψis the m.g.f. of the binomial 
distributionwithparameters n1+n2andp.Hence,byTheorem4.4.5,thedistribution 
of X1+X2must be thatbinomial distribution. 
Sketch of the Proof of Theorem 4.4.2 
First, we indicate why all moments of Xare ﬁnite. Let t> 0 be such that both ψ(t)
andψ(−t)areﬁnite. Deﬁne g(x)=etx +e−tx . Notice that 
E[g(X)]=ψ(t)+ψ(−t)< ∞. (4.4.5)
On every bounded interval of xvalues,g(x)is bounded. For each integer n> 0, as 
|x|→∞,g(x)is eventually larger than |x|n. It follows from these facts and (4.4.5) 
thatE|Xn|<∞.
Althoughitisbeyondthescopeofthisbook,itcanbeshownthatthederivative 
ψ′(t)exists at the point t=0, and that at t=0, the derivative of the expectation in 
Eq. (4.4.1) mustbe equal to theexpectation ofthederivative.Thus, 
ψ′(0)=/bracketleftbiggd
dtE(e tX )/bracketrightbigg
t=0=E/bracketleftbigg/parenleftbigg d
dtetX /parenrightbigg
t=0/bracketrightbigg
.
But/parenleftbiggd
dtetX /parenrightbigg
t=0=(Xe tX )t=0=X.
Itfollows that 
ψ′(0)=E(X).
In other words, the derivative ofthem.g.f. ψ(t)at t=0 is the mean of X.
Furthermore,itcanbeshownthatitispossibletodifferentiate ψ(t)anarbitrary 
number of times at the point t=0. For n=1,2,...,thenth derivative ψ(n)(0)at 
t=0 will satisfy the following relation: 
ψ(n)(0)=/bracketleftbiggdn
dtnE(e tX )/bracketrightbigg
t=0=E/bracketleftbigg/parenleftbigg dn
dtnetX /parenrightbigg
t=0/bracketrightbigg
=E[(X netX )t=0]=E(X n).
240Chapter4 Expectation 
Thus, ψ′(0)=E(X),ψ′′(0)=E(X 2),ψ′′′(0)=E(X 3), and so on. Hence, we see that 
the m.g.f., if it is ﬁnite in an open interval around t=0, can be used to generate all 
ofthe moments ofthe distribution by taking derivatives at t=0. 
Summary
Ifthe kthmomentofarandomvariableexists,thensodoesthe jthmomentforevery 
j<k . The moment generating function of X,ψ(t)=E(e tX ), if it is ﬁnite for tin a 
neighborhood of 0, can be used to ﬁnd moments of X. The kth derivative of ψ(t)at 
t=0 is E(X k). The m.g.f. characterizes the distribution in the sense that all random 
variables thathave thesame m.g.f. have the same distribution. 
Exercises 
1. If Xhastheuniformdistributionontheinterval[ a,b ], 
whatis the value ofthe ﬁfth central momentof X?
2. If Xhastheuniformdistributionontheinterval[ a,b ], 
write a formula for every evencentral momentof X.
3. Supposethat Xisarandomvariableforwhich E(X)=
1, E(X 2)=2, and E(X 3)=5. Find the value of the third 
centralmomentof X.
4. Suppose that Xis a random variable such that E(X 2)
is ﬁnite. (a)Show thatE(X 2)≥[E(X)]2.(b)Show that
E(X 2)=[E(X)]2if and only if there exists a constant c
suchthatPr (X =c)=1. Hint:Var (X)≥0. 
5. Suppose that Xis a random variable with mean µand
varianceσ2, and that the fourth moment of Xis ﬁnite. 
Showthat
E[(X −µ)4]≥σ4.
6. Suppose that Xhas the uniform distribution on the 
interval[a,b ]. Determine the m.g.f. of X.
7. Supposethat Xisarandomvariableforwhichthem.g.f. 
is as follows: 
ψ(t)=1
4(3et+e−t)for−∞<t< ∞.
Find the mean and the variance of X.
8. Supposethat Xisarandomvariableforwhichthem.g.f. 
is as follows: 
ψ(t)=et2+3tfor−∞<t< ∞.
Find the mean and the variance of X.
9. LetXbearandomvariablewithmean µandvariance
σ2, and let ψ1(t)denote the m.g.f. of Xfor−∞<t< ∞.
Letcbe a given positive constant, and let Ybe a random variablefor whichthem.g.f. is 
ψ2(t)=ec[ψ1(t)−1] for−∞<t< ∞.
Find expressions for the mean and the variance of Yin 
termsofthemean andthe variance of X.
10.Suppose that the random variables XandYare i.i.d.
and thatthe m.g.f.ofeachis 
ψ(t)=et2+3tfor−∞<t< ∞.
Findthe m.g.f. of Z=2X−3Y+4. 
11.Suppose that Xis a random variable for which the 
m.g.f.is as follows: 
ψ(t)=1
5et+2
5e4t+2
5e8tfor−∞<t< ∞.
Find the probability distribution of X.Hint:It is a simple 
discrete distribution.
12.Suppose that Xis a random variable for which the 
m.g.f.is as follows: 
ψ(t)=1
6(4+et+e−t)for−∞<t< ∞.
Findthe probabilitydistributionof X.
13.LetXhave the Cauchy distribution (see Example
4.1.8). Prove thatthe m.g.f. ψ(t)is ﬁnite only for t=0. 
14.LetXhave p.d.f. 
f(x)=/braceleftbiggx−2if x> 1, 
0 otherwise. 
Prove that the m.g.f. ψ(t)is ﬁnite for all t≤0 but for no 
t> 0. 
4.5 TheMeanandtheMedian 241
15.Prove the following extension of Theorem 4.4.1: If 
E(|X|a)< ∞forsomepositivenumber a,then E(|X|b)< 
∞foreverypositivenumber b<a .Givetheproofforthe 
case in which Xhas a discrete distribution. 16.LetXhavethebinomialdistributionwithparameters
nandp. Let Yhave the binomial distribution with pa-
rametersnand 1−p. Prove that the skewness of Yis the 
negative of the skewness of X.Hint:LetZ=n−Xand
showthatZhas the samedistribution as Y.
17.FindtheskewnessofthedistributioninExample4.4.3. 
4.5 The Mean and the Median 
Althoughthemeanofadistributionisameasureofcentrallocation, themedian 
(see Deﬁnition 3.3.3) is also a measure of central location for a distribution.
Thissectionpresentssomecomparisonsandcontrastsbetweenthesetwolocation
summariesofadistribution.
The Median
It was mentioned in Sec. 4.1 that the mean of a probability distribution on the real 
line will be at the center of gravity of that distribution. In this sense, the mean of a 
distributioncanberegardedasthe centerofthedistribution.Thereisanotherpoint 
on the line that might also be regarded as the center of the distribution. Suppose 
that there is a point m0that divides the total probability into two equal parts, that 
is, the probability to the left of m0is 1/2, and the probability to the right of m0is 
also 1/2. For a continuous distribution, the median of the distribution introduced 
in Deﬁnition 3.3.3 is such a number. If there is such an m0, it could legitimately be 
calledacenterofthedistribution.Itshouldbenoted,however,thatforsomediscrete 
distributionstherewillnotbeanypointatwhichthetotalprobabilityisdividedinto 
two parts that are exactly equal. Moreover, for other distributions, which may be 
eitherdiscreteorcontinuous,therewillbemorethanonesuchpoint.Therefore,the 
formal deﬁnition of a median, which will now be given, must be general enough to 
includethesepossibilities.
Deﬁnition
4.5.1Median. Let Xbe a random variable. Every number mwith the following property
is called a medianofthe distributionof X:
Pr (X ≤m)≥1/2 and Pr (X ≥m)≥1/2.
Another way to understand this deﬁnition is that a median is a point mthat
satisﬁes the following two requirements: First, if mis included with the values of X
to the leftof m, then 
Pr (X ≤m)≥Pr (X>m).
Second,if mis included withthe values of Xto the rightof m, then 
Pr (X ≥m)≥Pr (X<m).
If there is a number msuch that Pr (X<m)=Pr (X>m), that is, if the number m
does actually divide the total probability into two equal parts, then mwill of course 
be a median ofthedistribution of X(seeExercise 16).
Note: Multiple Medians. One can prove that every distribution must have at least 
one median. Indeed, the 1/2 quantile from Deﬁnition 3.3.2 is a median. (See Exer- 
cise 1.) For some distributions, every number in some interval is a median. In such 
242Chapter4 Expectation 
cases,the1/2quantileistheminimumofthesetofallmedians.Whenawholeinterval 
of numbers are medians of a distribution, some writers refer to the midpoint of the 
intervalas the median. 
Example 
4.5.1The Median of a Discrete Distribution . Suppose that Xhas the following discrete
distribution:
Pr (X =1)=0.1,Pr (X =2)=0.2,
Pr (X =3)=0.3,Pr (X =4)=0.4.
Thevalue3isamedianofthisdistributionbecausePr (X ≤3)=0.6,whichisgreater 
than 1/2, and Pr (X ≥3)=0.7, which is also greater than 1/2. Furthermore, 3 is the 
unique median ofthis distribution. ◭
Example 
4.5.2A Discrete Distribution for Which the Median Is Not Unique . Suppose that Xhas the
followingdiscrete distribution:
Pr (X =1)=0.1,Pr (X =2)=0.4,
Pr (X =3)=0.3,Pr (X =4)=0.2.
Here,Pr (X ≤2)=1/2,andPr (X ≥3)=1/2.Therefore,everyvalueof mintheclosed 
interval 2≤m≤3will be a median of this distribution. The most popular choice of 
median ofthis distribution would be the midpoint2.5. ◭
Example 
4.5.3The Median of a Continuous Distribution . Supposethat Xhasacontinuousdistribution 
forwhichthe p.d.f. is as follows: 
f(x)=/braceleftbigg
4x3for 0<x< 1, 
0 otherwise. 
The unique median ofthis distribution will be the number msuch that
/integraldisplaym
04x3dx =/integraldisplay1
m4x3dx =1
2.
This number is m=1/21/4. ◭
Example 
4.5.4A Continuous Distribution for Which the Median Is Not Unique . Supposethat Xhasa
continuous distribution for which thep.d.f. is as follows: 
f(x)=

1/2 for 0 ≤x≤1, 
1 for 2 .5≤x≤3, 
0 otherwise. 
Here, for every value of min the closed interval 1 ≤m≤2.5, Pr (X ≤m)=Pr (X ≥
m)=1/2. Therefore, every value of min the interval 1 ≤m≤2.5is a median of this 
distribution. ◭
Comparison of the Mean and the Median 
Example 
4.5.5Last Lottery Number . In a state lottery game, a three-digit number from 000 to 999 
is drawn each day. After several years, all but one of the 1000 possible numbers has 
been drawn. A lottery ofﬁcial would like to predict how much longer it will be until 
that missing number is ﬁnally drawn. Let Xbe the number of days ( X=1 being 
tomorrow)untilthatnumberappears.Itisnotdifﬁculttodeterminethedistribution 
of X, assuming that all 1000 numbers are equally likely to be drawn each day and 
4.5 TheMeanandtheMedian 243
thatthedrawsareindependent.Let Axstandfortheeventthatthemissingnumber
is drawn on day xforx=1,2,....Then {X=1}= A1, and for x> 1, 
{X=x}= Ac
1∩...∩Ac
x−1∩Ax.
Since theAxevents are independent and all have probability 0 .001, it is easy to see 
that thep.f. of Xis 
f(x)=/braceleftbigg
0.001(0.999)x−1forx=1,2,...
0 otherwise. 
But,thelotteryofﬁcialwantstogiveasingle-numberpredictionforwhenthenumber 
will be drawn. What summary of the distribution would be appropriate for this 
prediction? ◭
The lottery ofﬁcial in Example 4.5.5 wants some sort of “average” or “middle” 
number to summarize the distribution of the number of days until the last number 
appears. Presumably she wants a prediction that is neither excessively large nor too 
small. Either the mean or a median of Xcan be used as such a summary of the 
distribution.Someimportantpropertiesofthemeanhavealreadybeendescribedin 
thischapter,andseveralmorepropertieswillbegivenlaterinthebook.However,for 
manypurposesthemedianisamoreusefulmeasureofthemiddleofthedistribution 
than is the mean. For example, every distribution has a median, but not every 
distribution has a mean. As illustrated in Example 4.3.5, the mean of a distribution 
canbemadeverylargebyremovingasmallbutpositiveamountofprobabilityfrom 
anypartofthedistributionandassigningthisamounttoasufﬁcientlylargevalueof x.
Ontheotherhand,themedianmaybeunaffectedbyasimilarchangeinprobabilities. 
If any amount of probability is removed from a value of xlarger than the median
and assigned to an arbitrarily large value of x, the median of the new distribution 
willbethesameasthatoftheoriginaldistribution.InExample4.3.5,allnumbersin 
the interval [0 ,1]are medians of both random variables XandYdespite the large
difference in their means. 
Example 
4.5.6Annual Incomes . Suppose that the mean annual income among the families in a 
certaincommunityis$30,000.Itispossiblethatonlyafewfamiliesinthecommunity 
actuallyhaveanincomeaslargeas$30,000,butthosefewfamilieshaveincomesthat 
are very much larger than $30,000. As an extreme example, suppose that there are 
100 families and 99 of them have income of $1,000 while the other one has income 
of$2,901,000.If,however,themedianannualincomeamongthefamiliesis$30,000, 
then atleastone-halfofthe families musthaveincomes of$30,000 or more. ◭
The median has oneconvenientpropertythatthe mean doesnot have.
Theorem
4.5.1One-to-One Function . Let Xbe a random variable that takes values in an interval I
of real numbers. Let rbe a one-to-one function deﬁned on the interval I. If mis a 
median of X, then r(m)is a median of r(X).
Proof LetY=r(X). We need to show that Pr (Y ≥r(m))≥1/2 and Pr (Y ≤r(m))≥
1/2.Since risone-to-oneontheinterval I,itmustbeeitherincreasingordecreasing 
over the interval I. If ris increasing, then Y≥r(m)if and only if X≥m, so Pr (Y ≥
r(m))=Pr (X ≥m)≥1/2.Similarly, Y≤r(m)ifandonlyif X≤mandPr (Y ≤r(m))≥
1/2 also.If risdecreasing,then Y≥r(m)ifandonlyif X≤m.Theremainderofthe 
proof is then similar tothe preceding. 
244Chapter4 Expectation 
Weshallnowconsidertwospeciﬁccriteriabywhichthepredictionofarandom 
variableXcan be judged. By the ﬁrst criterion, the optimal prediction that can be 
made is the mean. By the second criterion, the optimal prediction is the median. 
Minimizing the Mean Squared Error 
Supposethat Xisarandomvariablewithmean µandvarianceσ2.Supposealsothat 
thevalueof Xistobeobservedinsomeexperiment,butthisvaluemustbepredicted 
beforetheobservationcanbemade.Onebasisformakingthepredictionistoselect 
somenumber dforwhichtheexpectedvalueofthesquareoftheerror X−dwillbe 
a minimum. 
Deﬁnition
4.5.2Mean Squared Error/M.S.E. . Thenumber E[(X −d)2]iscalledthe meansquarederror
(M.S.E.)ofthe prediction d.
The next result shows that the number dfor which the M.S.E. is minimized is 
E(X).
Theorem
4.5.2LetXbe a random variable with ﬁnite variance σ2, and let µ=E(X). For every 
numberd,
E[(X −µ)2]≤E[(X −d)2]. (4.5.1)
Furthermore, there will beequalityin the relation(4.5.1) ifand only if d=µ.
Proof For every value of d,
E[(X −d)2]=E(X 2−2dX +d2)
=E(X 2)−2dµ +d2. (4.5.2)
TheﬁnalexpressioninEq.(4.5.2)issimplyaquadraticfunctionof d.Byelementary 
differentiation it will be found that the minimum value of this function is attained 
whend=µ. Hence, in order to minimize the M.S.E., the predicted value of X
shouldbeitsmean µ.Furthermore,whenthispredictionisused,theM.S.E.issimply 
E[(X −µ)2]=σ2.
Example 
4.5.7Last Lottery Number . In Example 4.5.5, we discussed a state lottery in which one 
number had never yet been drawn. Let Xstand for the number of days until that 
last number is eventually drawn. The p.f. of Xwas computedin Example 4.5.5 as 
f(x)=/braceleftbigg
0.001(0.999)x−1forx=1,2,...
0 otherwise. 
Wecan compute the mean of Xas 
E(X)=∞/summationdisplay
x=1x0.001(0.999)x−1=0.001∞/summationdisplay
x=1x(0.999)x−1. (4.5.3)
Atﬁrst,thissumdoesnotlooklikeonethatiseasytocompute.However,itisclosely 
relatedto the general sum 
g(y)=∞/summationdisplay
x=0yx=1
1−y,
4.5 TheMeanandtheMedian 245
if0 <y< 1.Usingpropertiesofpowerseriesfromcalculus,weknowthatthederiva- 
tiveof g(y)canbefoundbydifferentiatingtheindividualtermsofthepowerseries. 
Thatis, 
g′(y)=∞/summationdisplay
x=0xy x−1=∞/summationdisplay
x=1xy x−1,
for0<y< 1.Butwealsoknowthat g′(y)=1/(1−y)2.ThelastsuminEq.(4.5.3)is 
g′(0.999)=1/(0.001)2. Itfollows that 
E(X)=0.0011
(0.001)2=1000. ◭
Minimizing the Mean Absolute Error 
Another possible basis for predicting the value of a random variable Xis to choose 
somenumber dfor whichE(|X−d|)willbe a minimum. 
Deﬁnition
4.5.3Mean Absolute Error/M.A.E . Thenumber E(|X−d|)iscalledthe meanabsoluteerror
(M.A.E.) ofthe prediction d.
We shall now show that the M.A.E. is minimized when the chosen value of dis a 
median ofthe distribution of X.
Theorem
4.5.3LetXbearandomvariablewithﬁnitemean,andlet mbeamedianofthedistribution 
of X. For every number d,
E(|X−m|)≤E(|X−d|). (4.5.4)
Furthermore, there will be equality in the relation (4.5.4) if and only if dis also a 
median ofthe distribution of X.
Proof For convenience, we shall assume that Xhas a continuous distribution for 
whichthep.d.f.is f.Theproofforanyothertypeofdistributionissimilar.Suppose 
ﬁrst thatd>m . Then 
E(|X−d|)−E(|X−m|)=/integraldisplay∞
−∞(|x−d|−|x−m|)f(x)dx 
=/integraldisplaym
−∞(d −m)f(x)dx +/integraldisplayd
m(d +m−2x)f(x)dx +/integraldisplay∞
d(m −d)f(x)dx 
≥/integraldisplaym
−∞(d −m)f(x)dx +/integraldisplayd
m(m −d)f(x)dx +/integraldisplay∞
d(m −d)f(x)dx 
=(d −m)[Pr(X ≤m)−Pr (X>m)]. (4.5.5)
Sincemis a median ofthe distributionof X, itfollows that 
Pr (X ≤m)≥1/2≥Pr (X>m). (4.5.6)
The ﬁnal difference in the relation (4.5.5) is therefore nonnegative. Hence, 
E(|X−d|)≥E(|X−m|). (4.5.7)
Furthermore, there can be equality in the relation (4.5.7) only if the inequalities in 
relations(4.5.5)and(4.5.6)areactuallyequalities.Acarefulanalysisshowsthatthese 
inequalitieswill be equalities only if dis alsoa medianofthe distribution of X.
The prooffor every value of dsuch thatd<m is similar. 
246Chapter4 Expectation 
Example 
4.5.8Last Lottery Number . InExample4.5.5,inordertocomputethemedianof X,wemust 
ﬁndthesmallestnumber xsuchthat the c.d.f. F(x)≥0.5. For integer x, we have 
F(x)=x/summationdisplay
n=10.001(0.999)n−1.
Wecan use the popular formula 
x/summationdisplay
n=0yn=1−yx+1
1−y
to see that, for integer x≥1, 
F(x)=0.0011−(0.999)x
1−0.999=1−(0.999)x.
Setting this equal to 0.5 and solving for xgivesx=692.8; hence, the median of Xis 
693.Themedianisuniquebecause F(x)nevertakestheexactvalue0.5foranyinteger
x. The median of Xis much smaller than the mean of 1000 found in Example 4.5.7. 
◭
The reason that the mean is so much larger than the median in Examples 4.5.7 
and 4.5.8 is that the distribution has probability at arbitrarily large values but is 
boundedbelow.Theprobabilityattheselargevaluespullsthemeanupbecausethere 
is no probability at equally small values to balance. The median is not affected by 
howtheupperhalfoftheprobabilityisdistributed.Thefollowingexampleinvolves 
a symmetricdistribution. Here, the mean and median(s) are more similar. 
Example 
4.5.9Predicting a Discrete Uniform Random Variable . Suppose that the probability is 1/6 
thatarandomvariable Xwilltakeeachofthefollowingsixvalues:1 ,2,3,4,5,6.We 
shall determine the prediction for which the M.S.E. is minimum and the prediction 
for whichthe M.A.E.is minimum. 
In this example, 
E(X)=1
6(1+2+3+4+5+6)=3.5.
Therefore, the M.S.E. will be minimized by the unique value d=3.5. 
Also, every number min the closed interval 3 ≤m≤4 is a median of the given 
distribution. Therefore, the M.A.E. will be minimized by every value of dsuch that
3≤d≤4 and only by such a value of d. Because the distribution of Xis symmetric, 
themeanof Xis alsoa median of X. ◭
Note:WhentheM.A.E.andM.S.E.AreFinite. Wenotedthatthemedianexistsfor 
every distribution, but the M.A.E. is ﬁnite if and only if the distribution has a ﬁnite 
mean.Similarly,theM.S.E.isﬁniteifandonlyifthedistributionhasaﬁnitevariance. 
Summary
A median of Xis any number msuch that Pr (X ≤m)≥1/2 and Pr (X ≥m)≥1/2. 
To minimize E(|X−d|)by choice of d, one must choose dto be a median of X. To 
minimizeE[(X −d)2]by choice of d, one mustchoose d=E(X).
4.5 TheMeanandtheMedian 247
Exercises 
1. Provethatthe1/2quantileasdeﬁnedinDeﬁnition3.3.2 
is a median as deﬁned inDeﬁnition 4.5.1. 
2. Supposethatarandomvariable Xhasadiscretedistri- 
bution for which thep.f. is as follows: 
f(x)=/braceleftbigg
cx forx=1,2,3,4,5,6, 
0 otherwise. 
Determine allthe mediansofthis distribution. 
3. Suppose that a random variable Xhas a continuous 
distribution for which thep.d.f. is as follows: 
f(x)=/braceleftbigg
e−xforx> 0, 
0 otherwise. 
Determine all themedians ofthis distribution. 
4. In a small community consisting of 153 families, the 
number of families that have kchildren(k =0,1,2,...)
is given in the following table: 
Number of Number of 
children families
0 21 
1 40 
2 42 
3 27 
4 or more 23 
Determine the mean and the median of the number of 
childrenperfamily.(Forthemean,assumethatallfamilies 
with four or more children have only four children. Why 
doesn’tthis pointmatter for the median?) 
5. Supposethatanobservedvalueof Xisequallylikelyto 
come from a continuous distribution for which the p.d.f. 
is for from one for which the p.d.f. is g. Suppose that 
f(x)> 0 for 0 <x< 1 and f(x)=0 otherwise, and sup- 
pose also that g(x)> 0 for 2 <x< 4 and g(x)=0 other- 
wise. Determine: (a)themean and
(b)themedianofthe distribution of X.
6. Suppose that a random variable Xhas a continuous 
distribution forwhich thep.d.f. fis as follows: 
f(x)=/braceleftbigg
2xfor 0<x< 1, 
0 otherwise. 
Determinethe valueof dthat minimizes
(a)E[(X −d)2]and (b)E(|X−d|).
7. Supposethataperson’sscore Xonacertainexamina- 
tion will be a number in the interval 0 ≤X≤1 and that Xhas a continuous distribution for which the p.d.f. is as 
follows:
f(x)=/braceleftBigg
x+1
2for 0≤x≤1,
0 otherwise. 
Determine the prediction of Xthat minimizes (a)the
M.S.E. and (b)the M.A.E.
8. Suppose that the distribution of a random variable 
Xis symmetric with respect to the point x=0 and that 
E(X 4)< ∞. Show that E[(X −d)4] is minimized by the 
valued=0. 
9. Suppose that a ﬁre can occur at any one of ﬁve points 
alongaroad.Thesepointsarelocatedat −3,−1,0,1,and 
2inFig.4.9.Supposealsothattheprobabilitythateachof 
thesepointswillbethelocationofthenextﬁrethatoccurs 
alongthe roadis as speciﬁed in Fig.4.9. 
/H1100230.2
0.1 0.10.4
0.2
Road /H110021 0 1 2
Figure 4.9 Probabilities forExercise 9.
a. At what point along the road should a ﬁre engine 
wait in order to minimize the expected value of the 
square of the distance that it must travel to the next 
ﬁre?
b. Where should the ﬁre engine wait to minimize the 
expected value of the distance that it must travel to 
thenextﬁre?
10.If nhouses are located at various points along a 
straight road, at what point along the road should a store 
be located in order to minimize the sum of the distances 
from thenhouses to the store? 
11.LetXbe a random variable having the binomial dis- 
tributionwithparameters n=7and p=1/4,andlet Ybe 
a random variable having the binomial distribution with 
parametersn=5and p=1/2.Whichofthesetworandom 
variablescanbe predictedwiththe smaller M.S.E.? 
12.Consideracoinforwhichtheprobabilityofobtaining 
aheadoneachgiventossis0.3.Supposethatthecoinisto 
betossed15times,andlet Xdenotethenumberofheads 
thatwill be obtained. 
a. Whatpredictionof Xhas thesmallestM.S.E.?
b. Whatpredictionof Xhas thesmallestM.A.E.?
13.Suppose that the distribution of Xis symmetric 
aroundapoint m.Prove that mis a median of X.
248Chapter4 Expectation 
14.FindthemedianoftheCauchydistributiondeﬁnedin 
Example4.1.8.
15.LetXbearandomvariablewithc.d.f. F.Supposethat 
a<b are numbers such that both aandbare medians of 
X.
a. ProvethatF(a)=1/2. 
b. Prove that there exist a smallest c≤aand a largest 
d≥bsuch that every number in the closed interval 
[c,d ]is a median of X.
c. If Xhas a discrete distribution, prove that F(d)> 
1/2. 16.LetXbearandomvariable.Supposethatthereexists 
anumber msuchthatPr (X<m)=Pr (X>m).Provethat 
mis a medianofthedistributionof X.
17.LetXbearandomvariable.Supposethatthereexists 
a number msuch that Pr (X<m)< 1/2 and Pr (X>m)< 
1/2.Provethat mistheuniquemedianofthedistribution 
of X.
18.Prove the following extension of Theorem 4.5.1. Let 
mbe the pquantile of the random variable X. (See Deﬁ- 
nition3.3.2.)If risastrictlyincreasingfunction,then r(m)
is the pquantileof r(X).
4.6 Covariance and Correlation 
Whenweareinterestedinthejointdistributionoftworandomvariables,itisuseful 
tohaveasummaryofhowmuchthetworandomvariablesdependoneachother.
Thecovarianceandcorrelationareattemptstomeasurethatdependence,butthey 
onlycaptureaparticulartypeofdependence,namelylineardependence.
Covariance 
Example 
4.6.1Test Scores . Whenapplyingforcollege,highschoolstudentsoftentakeanumberof 
standardized tests. Consider a particular student who will take both a verbal and a 
quantitative test. Let Xbe this student’s score on the verbal test, and let Ybe the 
same student’s score on the quantitative test. Although there are students who do 
much better on one test than the other, it might still be reasonable to expect that a 
student who does very well on one test to do at least a little better than average on 
the other. We would like to ﬁnd a numerical summary of the joint distribution of X
andYthatreﬂectsthedegreetowhichwebelieveahighorlowscoreononetestwill 
be accompaniedby a highor low score on the other test. ◭
Whenweconsiderthejointdistributionoftworandomvariables,themeans,the 
medians, and the variances of the variables provide useful information about their 
marginaldistributions.However,thesevaluesdonotprovideanyinformationabout 
the relationship between the two variables or about their tendency to vary together 
rather than independently. In this section and the next one, we shall introduce 
summariesofajointdistributionthatenableustomeasuretheassociationbetween 
two random variables, determine the variance of the sum of an arbitrary number of 
dependentrandomvariables,andpredictthevalueofonerandomvariablebyusing 
theobserved value ofsomeother related variable. 
Deﬁnition
4.6.1Covariance . Let XandYbe random variables having ﬁnite means. Let E(X)=µX
andE(Y)=µYThe covarianceof XandY,whichisdenotedbyCov (X,Y),isdeﬁned 
as 
Cov(X,Y)=E[(X −µX)(Y−µY)], (4.6.1)
ifthe expectation in Eq. (4.6.1) exists. 
4.6 Covariance andCorrelation 249
It can be shown (see Exercise 2 at the end of this section) that if both XandY
haveﬁnitevariance,thentheexpectationinEq.(4.6.1)willexistandCov (X,Y)will
be ﬁnite. However, the value ofCov (X,Y)can be positive, negative, or zero. 
Example 
4.6.2Test Scores . Let XandYbe the test scores in Example 4.6.1, and suppose that they 
have thejoint p.d.f. 
f(x,y)=/braceleftbigg2xy +0.5 for 0 ≤x≤1and 0 ≤y≤1, 
0 otherwise. 
We shall compute the covariance Cov (X,Y). First, we shall compute the means µX
andµYof XandY, respectively. The symmetry in the joint p.d.f. means that Xand
Yhavethesamemarginal distribution;hence, µX=µY. We see that 
µX=/integraldisplay1
0/integraldisplay1
0[2 x2y+0.5x]dydx 
=/integraldisplay1
0[x2+0.5x]dx =1
3+1
4=7
12 ,
so that µY=7/12 as well. The covariance can be computed using Theorem 4.1.2. 
Speciﬁcally, we mustevaluate the integral 
/integraldisplay1
0/integraldisplay1
0/parenleftbigg
x−7
12 /parenrightbigg/parenleftbigg 
y−7
12 /parenrightbigg
(2xy +0.5)dydx.
This integral is straightforward, albeit tedious, to compute, and the result is 
Cov(X,Y)=1/144. ◭
The following resultoften simpliﬁes the calculation ofa covariance. 
Theorem
4.6.1For all randomvariables XandYsuch thatσ2
X<∞andσ2
Y<∞,
Cov(X,Y)=E(XY)−E(X)E(Y). (4.6.2)
Proof Itfollows fromEq. (4.6.1) that 
Cov(X,Y)=E(XY −µXY−µYX+µXµY)
=E(XY)−µXE(Y)−µYE(X)+µXµY.
SinceE(X)=µXandE(Y)=µY, Eq. (4.6.2) is obtained. 
The covariance between XandYis intended to measure the degree to which 
XandYtend to be large at the same time or the degree to which one tends to be 
largewhiletheotherissmall.Someintutionaboutthisinterpretationcanbegathered 
from a careful look at Eq. (4.6.1). For example, suppose that Cov (X,Y)is positive. 
Then X>µ XandY>µ Ymustoccurtogetherand/or X<µ XandY<µ Ymustoccur
togethertoalargerextentthan X<µ XoccurswithY>µ YandX>µ Xoccurswith
Y<µ Y.Otherwise,themeanwouldbenegative.Similarly,ifCov (X,Y)isnegative, 
thenX>µ XandY<µ Ymustoccurtogetherand/or X<µ XandY>µ Ymustoccur
together to larger extent than the other two inequalities. If Cov (X,Y)=0, then the 
extent to which XandYare on the same sides of their respective means exactly 
balances theextent to which they are on opposite sides oftheir means. 
250Chapter4 Expectation 
Correlation 
AlthoughCov (X,Y)givesanumericalmeasureofthedegreetowhich XandYvary
together,themagnitudeofCov (X,Y)isalsoinﬂuencedbytheoverallmagnitudesof 
XandY.Forexample,inExercise5inthissection,youcanprovethatCov (2X,Y)=
2Cov (X,Y). In order to obtain a measure of association between XandYthat is 
notdrivenbyarbitrarychangesinthescales ofoneortheotherrandomvariable,we 
deﬁne a slightly differentquantity next. 
Deﬁnition
4.6.2Correlation . Let XandYbe random variables with ﬁnite variances σ2
Xandσ2
Y, re- 
spectively. Then the correlationof XandY, which is denoted by ρ(X,Y), is deﬁned 
as follows: 
ρ(X,Y)=Cov(X,Y)
σXσY. (4.6.3)
Inordertodeterminetherangeofpossiblevaluesofthecorrelation ρ(X,Y),we 
shallneed thefollowing result.
Theorem
4.6.2Schwarz Inequality . For all randomvariables UandVsuch thatE(UV)exists,
[E(UV)]2≤E(U 2)E(V 2). (4.6.4)
If, in addition, the right-hand side of Eq. (4.6.4) is ﬁnite, then the two sides of 
Eq. (4.6.4) equal the same value if and only if there are nonzero constants aand
bsuch thataU +bV =0 withprobability 1. 
Proof If E(U 2)=0,thenPr (U =0)=1.Therefore,itmustalsobetruethatPr (UV =
0)=1.Hence, E(UV)=0,andtherelation(4.6.4)issatisﬁed.Similarly,if E(V 2)=0, 
then the relation (4.6.4) will be satisﬁed. Moreover, if either E(U 2)or E(V 2)is 
inﬁnite, then the right side of the relation (4.6.4) will be inﬁnite. In this case, the 
relation(4.6.4) willsurely be satisﬁed. 
For the rest of the proof, assume that 0 <E(U 2)< ∞and 0<E(V 2)< ∞. For 
allnumbersaandb,
0≤E[(aU +bV)2]=a2E(U 2)+b2E(V 2)+2abE(UV) (4.6.5)
and
0≤E[(aU −bV)2]=a2E(U 2)+b2E(V 2)−2abE(UV). (4.6.6)
If we let a=[E(V 2)]1/2andb=[E(U 2)]1/2, then it follows from the relation (4.6.5) 
that
E(UV)≥− [E(U 2)E(V 2)]1/2. (4.6.7)
Italso follows fromthe relation(4.6.6) that 
E(UV)≤[E(U 2)E(V 2)]1/2. (4.6.8)
These two relations together imply thatthe relation (4.6.4) is satisﬁed. 
Finally, suppose that the right-hand side of Eq. (4.6.4) is ﬁnite. Both sides of 
(4.6.4)equalthesamevalueifandonlyifthesameistrueforeither(4.6.7)or(4.6.8). 
Both sides of (4.6.7) equal the same value if and only if the rightmost expression in 
(4.6.5)is0.This,inturn,istrueifandonlyif E[(aU +bV)2]=0,whichoccursifand 
only if aU +bV =0 with probability 1. The reader can easily check that both sides 
of(4.6.8) equalthe same value ifand only if aU −bV =0 withprobability 1. 
4.6 Covariance andCorrelation 251
A slightvarianton Theorem4.6.2 is the resultwe want. 
Theorem
4.6.3Cauchy-Schwarz Inequality . Let XandYbe random variables with ﬁnite variance. 
Then 
[Cov(X,Y)]2≤σ2
Xσ2
Y, (4.6.9)
and
−1≤ρ(X,Y)≤1. (4.6.10)
Furthermore, the inequality in Eq. (4.6.9) is an equality if and only if there are 
nonzeroconstants aandbandaconstant csuchthataX +bY =cwithprobability1. 
Proof LetU=X−µXandV=Y−µY.Eq.(4.6.9)nowfollowsdirectlyfromTheo- 
rem4.6.2.Inturn,itfollowsfromEq.(4.6.3)that[ ρ(X,Y)]2≤1or,equivalently,that 
Eq.(4.6.10)holds.Theﬁnalclaimfollowseasilyfromthesimilarclaimattheendof 
Theorem4.6.2. 
Deﬁnition
4.6.3Positively/Negatively Correlated/Uncorrelated . It is said that XandYarepositively
correlated if ρ(X,Y)> 0, that XandYarenegativelycorrelated if ρ(X,Y)< 0, and 
thatXandYareuncorrelated if ρ(X,Y)=0. 
It can be seen from Eq. (4.6.3) that Cov (X,Y)andρ(X,Y)must have the same
sign; thatis, bothare positive, or both are negative, or both are zero. 
Example 
4.6.3Test Scores . ForthetwotestscoresinExample4.6.2,wecancomputethecorrelation 
ρ(X,Y). The variances of XandYare both equal to 11 /144, so the correlation is 
ρ(X,Y)=1/11. ◭
Properties of Covariance and Correlation 
Weshallnowpresentfourtheoremspertainingtothebasicpropertiesofcovariance 
andcorrelation.
The ﬁrst theorem shows that independent random variables must be uncorre- 
lated.
Theorem
4.6.4If XandYareindependentrandomvariableswith0 <σ 2
X<∞and0<σ 2
Y<∞,then 
Cov(X,Y)=ρ(X,Y)=0.
Proof If XandYare independent, then E(XY)=E(X)E(Y) . Therefore, by Eq. 
(4.6.2),Cov(X,Y)=0. Also, itfollows that ρ(X,Y)=0. 
The converse of Theorem 4.6.4 is not true as a general rule. Two dependent 
randomvariablescanbeuncorrelated.Indeed,eventhough Yisanexplicitfunction 
of X, itis possible that ρ(X,Y)=0, as inthe following examples. 
Example 
4.6.4Dependent but Uncorrelated Random Variables . Suppose that the random variable X
cantakeonlythethreevalues −1,0,and1,andthateachofthesethreevalueshasthe 
sameprobability.Also,lettherandomvariable Ybedeﬁnedbytherelation Y=X2.
Weshall show that XandYare dependentbutuncorrelated.
252Chapter4 Expectation 
Figure 4.10 The shaded 
regioniswherethejointp.d.f. 
of (X,Y)is constant and 
nonzero in Example 4.6.5. 
Theverticallineindicatesthe 
values of Ythat are possible
whenX=0.5. 
/H110021.0/H110020.5
/H110020.50.51.0
1.00.5y
x
In this example, XandYare clearly dependent, since Yis not constant and the 
value of Yis completely determinedby the value of X. However, 
E(XY)=E(X 3)=E(X)=0,
becauseX3is the same random variable as X. Since E(XY)=0 and E(X)E(Y)=0, 
itfollowsfromTheorem4.6.1thatCov (X,Y)=0andthat XandYareuncorrelated.
◭
Example 
4.6.5Uniform Distribution Inside a Circle . Let (X,Y)have joint p.d.f. that is constant on 
the interior of the unit circle, the shaded region in Fig. 4.10. The constant value of 
the p.d.f. is one over the area of the circle, that is, 1 /(2π). It is clear that XandY
are dependent since the region where the joint p.d.f. is nonzero is not a rectangle. 
In particular, notice that the set of possible values for Yis the interval (−1,1), but 
whenX=0.5,thesetofpossiblevaluesfor Yisthesmallerinterval (−0.866,0.866).
Thesymmetryofthecirclemakesitclearthatboth XandYhavemean0.Also,itis 
not difﬁcult to see that E(XY)=/integraltext /integraltext 
xyf(x,y)dxdy =0. To see this, notice that the 
integralof xy overthetophalfofthecircleisexactlythenegativeoftheintegralof xy 
overthebottomhalf.Hence,Cov (X,Y)=0,buttherandomvariablesaredependent. 
◭
The next result shows that if Yis a linearfunction of X, then XandYmust be 
correlated and,in fact, |ρ(X,Y)|= 1. 
Theorem
4.6.5Supposethat Xisarandomvariablesuchthat0 <σ 2
X<∞,and Y=aX +bforsome
constantsaandb,where a→negationslash=0.If a> 0,then ρ(X,Y)=1.If a< 0,then ρ(X,Y)=− 1. 
Proof If Y=aX +b, then µY=aµ X+bandY−µY=a(X −µX). Therefore, by 
Eq. (4.6.1),
Cov(X,Y)=aE [(X −µX)2]=aσ 2
X.
SinceσY=|a|σX, the theoremfollows fromEq. (4.6.3). 
There is a converse to Theorem 4.6.5. That is, |ρ(X,Y)|= 1implies that Xand
Yarelinearlyrelated.(SeeExercise17.)Ingeneral,thevalueof ρ(X,Y)providesa
measureoftheextenttowhichtworandomvariables XandYarelinearlyrelated.If 
4.6 Covariance andCorrelation 253
the joint distribution of XandYis relatively concentrated around a straight line in 
thexy -planethathasapositiveslope,then ρ(X,Y)willtypicallybecloseto1.Ifthe 
jointdistributionisrelativelyconcentratedaroundastraightlinethathasanegative 
slope,then ρ(X,Y)willtypicallybecloseto −1.Weshallnotdiscusstheseconcepts 
furtherhere,butweshallconsiderthemagainwhenthebivariatenormaldistribution 
is introduced and studied inSec. 5.10. 
Note: Correlation Measures Only Linear Relationship. A large value of |ρ(X,Y)|
meansthatXandYareclosetobeinglinearlyrelatedandhencearecloselyrelated. 
But a small value of |ρ(X,Y)|does not mean that XandYare not close to being 
related. Indeed, Example 4.6.4 illustrates random variables that are functionally
related buthave0 correlation. 
We shall now determine the variance of the sum of random variables that are 
not necessarily independent.
Theorem
4.6.6If XandYarerandomvariables such thatVar (X)< ∞andVar (Y)< ∞, then 
Var (X +Y)=Var (X)+Var (Y)+2Cov (X,Y). (4.6.11)
Proof SinceE(X +Y)=µX+µY, then 
Var (X +Y)=E[(X +Y−µX−µY)2]
=E[(X −µX)2+(Y −µY)2+2(X −µX)(Y−µY)]
=Var (X)+Var (Y)+2Cov (X,Y).
For all constants aandb, it can be shown that Cov (aX,bY)=ab Cov(X,Y)
(see Exercise 5 at the end of this section). The following then follows easily from 
Theorem4.6.6. 
Corollary 
4.6.1Leta,b, and cbe constants. Under the conditions ofTheorem4.6.6, 
Var (aX +bY +c)=a2Var (X)+b2Var (Y)+2ab Cov(X,Y). (4.6.12)
A particularly useful special case ofCorollary 4.6.1 is 
Var (X −Y)=Var (X)+Var (Y)−2Cov (X,Y). (4.6.13)
Example 
4.6.6Investment Portfolio . Consider,onceagain,theinvestorinExample4.3.7onpage230 
tryingtochooseaportfoliowith$100,000toinvest.Weshallmakethesameassump- 
tions about the returns on the two stocks, except that now we will suppose that the 
correlationbetweenthetworeturns R1andR2is −0.3,reﬂectingabeliefthatthetwo 
stocks tend to react in opposite ways to common market forces. The variance of a 
portfolio of s1shares of the ﬁrst stock, s2shares of the second stock, and s3dollars
invested at3.6%is now 
Var (s 1R1+s2R2+0.036s3)=55 s2
1+28 s2
2−0.3/radicalbig
55 ×28 s1s2.
Wecontinuetoassumethat(4.3.2)holds.Figure4.11showstherelationshipbetween 
themeanandvarianceoftheefﬁcientportfoliosinthisexampleandExample4.3.7. 
Notice how the variances are smaller in this example than in Example 4.3.7. This is 
duetothefactthatthenegativecorrelationlowersthevarianceofalinearcombina- 
tion withpositive coefﬁcients. ◭
Theorem4.6.6canalsobeextendedeasilytothevarianceofthesumof nrandom
variables, as follows. 
254Chapter4 Expectation 
Figure 4.11 Mean and vari-
ance of efﬁcient investment 
portfolios.
Mean portfolio return400005000 6000 7000 8000 9000 10,000Variance of portfolio return 1.5/H1100310 8
5/H1100310 710 8Correlation /H11005 /H11002 0.3
Correlation /H11005 0 
Theorem
4.6.7If X1,...,X nare random variablessuch that Var (X i)< ∞fori=1,...,n , then 
Var /parenleftBiggn/summationdisplay
i=1Xi/parenrightBigg
=n/summationdisplay
i=1Var (X i)+2/summationdisplay/summationdisplay
i<j Cov(X i,X j). (4.6.14)
Proof For every random variable Y, Cov (Y,Y)=Var (Y). Therefore, by using the 
resultin Exercise 8 atthe endofthis section, we can obtain the following relation: 
Var /parenleftBiggn/summationdisplay
i=1Xi/parenrightBigg
=Cov
n/summationdisplay
i=1Xi,n/summationdisplay
j=1Xj
=n/summationdisplay
i=1n/summationdisplay
j=1Cov(X i,X j).
We shall separate the ﬁnal sum in this relation into two sums: (i) the sum of those 
termsforwhich i=jand(ii)thesumofthosetermsforwhich i→negationslash=j.Then,ifweuse 
the factthat Cov (X i,X j)=Cov(X j,X i), we obtainthe relation 
Var /parenleftBiggn/summationdisplay
i=1Xi/parenrightBigg
=n/summationdisplay
i=1Var (X i)+/summationdisplay/summationdisplay
i→negationslash=jCov(X i,X j)
=n/summationdisplay
i=1Var (X i)+2/summationdisplay/summationdisplay
i<j Cov(X i,X j).
The following is a simplecorrolary to Theorem4.6.7. 
Corollary 
4.6.2If X1,...,X nare uncorrelated random variables (that is, if XiandXjare uncorre-
latedwhenever i→negationslash=j), then 
Var /parenleftBiggn/summationdisplay
i=1Xi/parenrightBigg
=n/summationdisplay
i=1Var (X i). (4.6.15)
Corollary 4.6.2 extends Theorem 4.3.5 on page 230, which states that (4.6.15) holds 
if X1,...,X nareindependent randomvariables.
Note: In General, Variances Add Only for Uncorrelated Random Variables. The 
variance of a sum of random variables should be calculated using Theorem 4.6.7 in 
general. Corollary4.6.2applies onlyfor uncorrelated random variables.
4.6 Covariance andCorrelation 255
Summary
Thecovarianceof XandYisCov (X,Y)=E{[X−E(X)][ Y−E(Y)]}.Thecorrelation 
is ρ(X,Y)=Cov(X,Y)/ [Var (X)Var (Y)]1/2, and it measures the extent to which X
andYarelinearlyrelated.Indeed, XandYarepreciselylinearlyrelatedifandonly 
if |ρ(X,Y)|= 1. The variance of a sum of random variables can be expressed as the 
sum of the variances plus two times the sum of the covariances. The variance of a 
linearfunction is Var (aX +bY +c)=a2Var (X)+b2Var (Y)+2ab Cov(X,Y).
Exercises 
1. Supposethatthepair (X,Y)isuniformlydistributedon 
the interiorofa circle ofradius 1. Compute ρ(X,Y).
2. Prove that if Var (X)< ∞and Var (Y)< ∞, then 
Cov(X,Y)is ﬁnite. Hint:By considering the relation 
[(X −µX)±(Y −µY)]2≥0, show that 
|(X −µX)(Y−µY)|≤ 1
2[(X −µX)2+(Y −µY)2].
3. Suppose that Xhas the uniform distribution on the 
interval [−2,2] and Y=X6. Show that XandYare un-
correlated.
4. Supposethatthedistributionofarandomvariable Xis 
symmetricwithrespecttothepoint x=0,0 <E(X 4)< ∞,
andY=X2. Show that XandYareuncorrelated.
5. For all random variables XandYand all constants a,
b,c, and d, show that 
Cov(aX +b,cY +d)=ac Cov(X,Y).
6. LetXandYberandomvariablessuchthat0 <σ 2
X<∞
and0<σ 2
Y<∞.Supposethat U=aX +bandV=cY +
d,where a→negationslash=0 and c→negationslash=0.Showthat ρ(U,V)=ρ(X,Y)if 
ac> 0, and ρ(U,V)=− ρ(X,Y)if ac< 0. 
7. LetX,Y, and Zbe three random variables such that 
Cov(X,Z)and Cov(Y,Z)exist, and let a,b, and cbe 
arbitrary givenconstants. Show that 
Cov(aX +bY +c,Z)=aCov(X,Z)+bCov(Y,Z).
8. Suppose that X1,...,X mandY1,...,Y nare random
variablessuchthatCov (X i,Y j)existsfori=1,...,m and
j=1,...,n , and suppose that a1,...,a mandb1,...,b n
are constants. Show that 
Cov
m/summationdisplay
i=1aiXi,n/summationdisplay
j=1bjYj
=m/summationdisplay
i=1n/summationdisplay
j=1aibjCov(X i,Y j).
9. Supposethat XandYaretworandomvariables,which 
may be dependent, and Var (X)=Var (Y). Assuming that 
0<Var (X +Y)< ∞and0<Var (X −Y)< ∞,showthat 
therandomvariables X+YandX−Yareuncorrelated.10.Suppose that XandYare negatively correlated. Is 
Var (X +Y)largeror smaller than Var (X −Y)?
11.Showthattworandomvariables XandYcannotpos-
sibly have the following properties: E(X)=3, E(Y)=2, 
E(X 2)=10, E(Y 2)=29,and E(XY)=0. 
12.Suppose that XandYhave a continuous joint distri- 
butionfor which thejoint p.d.f. is as follows: 
f(x,y)=/braceleftBigg
1
3(x +y)for0≤x≤1and 0 ≤y≤2, 
0 otherwise. 
Determine thevalue ofVar (2X−3Y+8).
13.Supposethat XandYarerandomvariablessuchthat
Var (X)=9, Var (Y)=4, and ρ(X,Y)=− 1/6. Determine 
(a)Var (X +Y)and(b)Var (X −3Y+4).
14.Supposethat X,Y,and Zarethreerandomvariables
such that Var (X)=1, Var (Y)=4, Var (Z)=8, Cov (X,Y)
=1, Cov (X,Z)=− 1, and Cov (Y,Z)=2. Determine (a)
Var (X +Y+Z)and(b)Var (3X−Y−2Z+1).
15.Suppose that X1,...,X nare random variables such
that the variance of each variable is 1 and the correlation 
betweeneachpairofdifferentvariablesis1 /4.Determine 
Var (X 1+...+Xn).
16.Consider the investor in Example 4.2.3 on page 220. 
Suppose that the returns R1andR2on the two stocks 
have correlation −1. A portfolio will consist of s1shares
of the ﬁrst stock and s2shares of the second stock where 
s1,s 2≥0. Find a portfolio such that the total cost of the 
portfoliois$6000andthevarianceofthereturnis0.Why 
is this situationunrealistic? 
17.LetXandYberandomvariableswithﬁnitevariance. 
Provethat|ρ(X,Y)|= 1impliesthatthereexistconstants 
a,b,and csuchthataX +bY =cwithprobability1. Hint:
UseTheorem4.6.2 with U=X−µXandV=Y−µY.
18.LetXandYhaveacontinuousdistributionwithjoint 
p.d.f. 
f(x,y)=/braceleftbiggx+yfor 0≤x≤1and0 ≤y≤1, 
0 otherwise. 
ComputethecovarianceCov (X,Y).
256Chapter4 Expectation 
4.7 Conditional Expectation 
Sinceexpectations(includingvariancesandcovariances)arepropertiesofdistri- 
butions,therewillexistconditionalversionsofallsuchdistributionalsummaries 
as well as conditional versions of all theorems that we have proven or will later 
proveaboutexpectations.Inparticular, supposethatwewishtopredictoneran- 
dom variable Yusing a function d(X)of another random variable Xso as to 
minimizeE([Y−d(X)]2).Then d(X)shouldbetheconditionalmeanof Ygiven
X. There is also a very useful theorem that is an extension to expectations of the 
lawoftotalprobability.
Deﬁnition and Basic Properties 
Example 
4.7.1 Household Survey . Acollectionofhouseholdsweresurveyed,andeachhouseholdre- 
portedthenumberofmembersandthenumberofautomobilesowned.Thereported 
numbersarein Table 4.1. 
Supposethatweweretosampleahouseholdatrandomfromthosehouseholds 
in the survey and learn the number of members. What would then be the expected 
numberofautomobiles thatthey own? ◭
The question at the end of Example 4.7.1 is closely related to the conditional 
distributionofone randomvariable given the other, as deﬁnedin Sec. 3.6. 
Deﬁnition
4.7.1 Conditional Expectation/Mean . Let XandYberandomvariablessuchthatthemean 
of Yexistsandisﬁnite.The conditionalexpectation(orconditionalmean)of Ygiven
X=xis denoted by E(Y |x)and is deﬁned to be the expectation of the conditional 
distributionof YgivenX=x.
For example, if Yhas a continuous conditional distribution given X=xwith
conditionalp.d.f. g2(y |x), then 
E(Y |x)=/integraldisplay∞
−∞yg 2(y |x)dy. (4.7.1)
Similarly,if Yhasadiscreteconditionaldistributiongiven X=xwithconditionalp.f. 
g2(y |x), then 
E(Y |x)=/summationdisplay
Allyyg 2(y |x). (4.7.2)
Table 4.1 Reported numbers of household members and 
automobiles inExample4.7.1 
Number ofmembers Number of 
automobiles 1 2 3 4 5 6 7 8 
0 10 7 3 2 2 1 0 0 
1 12 21 25 30 25 15 5 1 
2 1 5 10 15 20 11 5 3 
3 0 2 3 5 5 3 2 1 
4.7 Conditional Expectation 257
Thevalueof E(Y |x)willnotbeuniquelydeﬁnedforthosevaluesof xsuchthat
the marginal p.f. or p.d.f. of Xsatisﬁesf1(x)=0. However, since these values of x
form a set of points whose probability is 0, the deﬁnition of E(Y |x)at such a point 
is irrelevant. (See Exercise 11 in Sec. 3.6.) It is also possible that there will be some 
values of xsuch that the mean of the conditional distribution of YgivenX=xis 
undeﬁned for those xvalues. When the mean of Yexists and is ﬁnite, the set of x
values for whichthe conditionalmeanis undeﬁned has probability0. 
TheexpressionsinEqs.(4.7.1)and(4.7.2)arefunctionsof x.Thesefunctionsof 
xcan be computed before Xis observed, and this idea leads to the following useful 
concept.
Deﬁnition
4.7.2 Conditional Means as Random Variables . Let h(x)stand for the function of xthat is 
denotedE(Y |x)ineither(4.7.1)or(4.7.2).Deﬁnethesymbol E(Y |X)tomean h(X)
and callitthe conditionalmeanof YgivenX.
In other words, E(Y |X)is a random variable (a function of X) whose value when 
X=xis E(Y |x). Obviously, we could deﬁne E(X |Y)andE(X |y)analogously.
Example 
4.7.2 Household Survey . Consider the household survey in Example 4.7.1. Let Xbe the 
numberofmembersinarandomlyselectedhouseholdfromthesurvey,andlet Ybe 
the number of cars owned by that household. The 250 surveyed households are all 
equally likely to be selected, so Pr (X =x,Y =y)is the number of households with 
xmembersand ycars,dividedby250.ThoseprobabilitiesarereportedinTable4.2. 
Suppose that the sampled household has X=4 members. The conditional p.f. of Y
givenX=4is g2(y |4)=f(4,y)/f 1(4),whichisthe x=4columnofTable4.2divided 
by f1(4)=0.208, namely, 
g2(0|4)=0.0385, g 2(1|4)=0.5769, g 2(2|4)=0.2885, g 2(3|4)=0.0962.
The conditional meanof YgivenX=4 is then 
E(Y |4)=0×0.0385+1×0.5769+2×0.2885+3×0.0962=1.442.
Similarly, we can compute E(Y |x)forall eight values of x. They are 
x 1 2 3 4 5 6 7 8 
E(Y |x)0.609 1.057 1.317 1.442 1.538 1.533 1.75 2
Table 4.2 Jointp.f.f(x,y)of XandYinExample4.7.2togetherwithmarginal 
p.f.’s f1(x)andf2(y)
x
y1 2 3 4 5 6 7 8 f2(y)
0 0.040 0.028 0.012 0.008 0.008 0.004 0 0 0.100 
1 0.048 0.084 0.100 0.120 0.100 0.060 0.020 0.004 0.536 
2 0.004 0.020 0.040 0.060 0.080 0.044 0.020 0.012 0.280 
3 0 0.008 0.012 0.020 0.020 0.012 0.008 0.004 0.084 
f1(x)0.092 0.140 0.164 0.208 0.208 0.120 0.048 0.020
258Chapter4 Expectation 
Therandomvariablethattakesthevalue0 .609whenthesampledhouseholdhasone
member, takes the value 1 .057 when the sampled household has two members, and 
so on, is the randomvariable E(Y |X). ◭
Example 
4.7.3 A Clinical Trial . Consideraclinicaltrialinwhichanumberofpatientswillbetreated 
and each patient will have one of two possible outcomes: success or failure. Let P
be the proportion of successes in a very large collection of patients, and let Xi=1
if the ith patient is a success and Xi=0 if not. Assume that the random variables 
X1,X 2,...are conditionally independent given P=pwith Pr (X i=1|P=p)=p.
LetX=X1+...+Xn, which is the number of patients out of the ﬁrst nwho are
successes. We now compute the conditional mean of XgivenP. The patients are 
independentandidenticallydistributedconditionalon P=p.Hence,theconditional 
distribution of XgivenP=pis the binomial distribution with parameters nandp.
As we saw in Sec. 4.2, the mean of this binomial distribution is np , so E(X |p)=np 
andE(X |P)=nP . Later, we will show how to compute the conditional mean of P
givenX. This canbe usedtopredict Pafterobserving X. ◭
Note:TheConditionalMeanof YGiven XIsaRandomVariable. BecauseE(Y |X)
is a function of the random variable X, it is itself a random variable with its own 
probability distribution, which can be derived from the distribution of X. On the 
other hand,h(x)=E(Y |x)is a function of xthat can be manipulated like any other 
function. The connection between the two is that when one substitutes the random 
variableXforxin h(x), the resultis h(X)=E(Y |X).
Weshallnowshowthatthemeanoftherandomvariable E(Y |X)mustbe E(Y).
A similar calculation shows thatthe mean of E(X |Y)must be E(X).
Theorem
4.7.1 Law of Total Probability for Expectations . Let XandYbe random variables such that 
Yhasﬁnitemean. Then 
E[E(Y |X)]=E(Y). (4.7.3)
Proof We shall assume, for convenience, that XandYhave a continuous joint 
distribution.Then 
E[E(Y |X)]=/integraldisplay∞
−∞E(Y |x)f 1(x)dx 
=/integraldisplay∞
−∞/integraldisplay∞
−∞yg 2(y |x)f 1(x)dydx.
Sinceg2(y |x)=f(x,y)/f 1(x), itfollows that 
E[E(Y |X)]=/integraldisplay∞
−∞/integraldisplay∞
−∞yf(x,y)dydx =E(Y).
Theproofforadiscretedistributionoramoregeneraltypeofdistributionissimilar. 
Example 
4.7.4 Household Survey . At the end of Example 4.7.2, we described the random variable 
E(Y |X).Itsdistributioncanbeconstructedfromthatdescription.Ithasadiscretedis- 
tributionthattakestheeightvaluesof E(Y |x)listedneartheendofthatexamplewith 
corresponding probabilities f1(x)forx=1,...,8. To be speciﬁc, let Z=E(Y |X),
then Pr[ Z=E(Y |x)]=f1(x)forx=1,...,8. The speciﬁc values are 
4.7 Conditional Expectation 259
z 0.609 1.057 1.317 1.442 1.538 1.533 1.75 2
Pr (Z =z)0.092 0.140 0.164 0.208 0.208 0.120 0.048 0.020
We can compute E(Z)=0.609×0.092+...+2×0.020=1.348. The reader can 
verifythatE(Y)=1.348by using the values of f2(y)in Table 4.2. ◭
Example 
4.7.5 A Clinical Trial . In Example 4.7.3, we let Xbe the number of patients out of the 
ﬁrstnwho are successes. The conditional mean of XgivenP=pwas computed as 
E(X |p)=np ,where Pistheproportionofsuccessesinalargepopulationofpatients. 
If the distribution of Pis uniform on the interval [0 ,1], then the marginal expected 
value of Xis E[E(X |P)]=E(nP)=n/ 2. We will see how to calculate E(P |X)in 
Example4.7.8. ◭
Example 
4.7.6 Choosing Points from Uniform Distributions . Suppose that a point Xis chosen in 
accordance with the uniform distribution on the interval [0 ,1]. Also, suppose that 
afterthevalue X=xhasbeenobserved (0<x< 1),apoint Yischoseninaccordance 
with a uniform distribution on the interval [ x,1]. We shall determine the value 
of E(Y).
For each given value of x (0<x< 1),E(Y |x)will be equal to the midpoint 
(1/2)(x+1)ofthe interval [ x,1]. Therefore, E(Y |X)=(1/2)(X+1)and
E(Y)=E[E(Y |X)]=1
2[E(X)+1] =1
2/parenleftbigg1
2+1/parenrightbigg
=3
4. ◭
Whenmanipulatingtheconditionaldistributiongiven X=x,itissafetoactasif 
Xistheconstant x.Thisfact,whichcansimplifythecalculationofcertainconditional 
means, is now stated withoutproof. 
Theorem
4.7.2 LetXandYbe random variables, and let Z=r(X,Y)for some function r. The 
conditional distribution of ZgivenX=xis the same as the conditional distribution 
of r(x,Y)givenX=x.
One consequence of Theorem 4.7.2 when XandYhave a continuous joint 
distribution is that 
E(Z |x)=E(r(x,Y)|x)=/integraldisplay∞
−∞r(x,y)g 2(y |x)dy.
Theorem4.7.1 also implies thatfor two arbitrary randomvariables XandY,
E{E[r(X,Y)|X]}= E[r(X,Y)], (4.7.4)
by letting Z=r(X,Y)and notingthat E{E(Z |X)}= E(Z).
Wecandeﬁne,inasimilarmanner,theconditionalexpectationof r(X,Y)given
Yand the conditional expectation of a function r(X 1,...,X n)of several random 
variables givenone or more ofthe variables X1,...,X n.
Example 
4.7.7 Linear Conditional Expectation . Suppose that E(Y |X)=aX +bfor some constants a
andb. We shall determine the value of E(XY)in terms of E(X)andE(X 2).
ByEq.(4.7.4), E(XY)=E[E(XY |X)].Furthermore,since Xisconsideredtobe 
given andﬁxed in theconditional expectation, 
E(XY |X)=XE(Y |X)=X(aX +b)=aX 2+bX.
260Chapter4 Expectation 
Therefore, 
E(XY)=E(aX 2+bX)=aE(X 2)+bE(X). ◭
The mean is not the only feature of a conditional distribution that is important 
enoughto getits own name. 
Deﬁnition
4.7.3 Conditional Variance . Foreverygivenvalue x,letVar (Y |x)denotethevarianceofthe 
conditionaldistribution of Ygiven thatX=x. Thatis, 
Var (Y |x)=E{[Y−E(Y |x)]2|x}. (4.7.5)
Wecall Var (Y |x)theconditionalvarianceof YgivenX=x.
The expression in Eq. (4.7.5) is once again a function v(x). We shall deﬁne 
Var (Y |X)to be v(X)and callitthe conditionalvarianceof YgivenX.
Note: Other Conditional Quantities. In much the same way as in Deﬁnitions 4.7.1 
and4.7.3,wecoulddeﬁneanyconditionalsummaryofadistributionthatwewish.For 
example,conditionalquantilesof YgivenX=xarethequantilesoftheconditional 
distributionof YgivenX=x.Theconditionalm.g.f.of YgivenX=xisthem.g.f.of 
the conditionaldistribution of YgivenX=x, etc. 
Prediction 
AttheendofExample4.7.3,weconsideredtheproblemofpredictingtheproportion 
Pof successes in a large population of patients given the observed number Xof 
succesesinasampleofsize n.Ingeneral,considertwoarbitraryrandomvariables X
andYthat have a speciﬁed joint distribution and suppose that after the value of X
has been observed, the value of Ymust be predicted. In other words, the predicted 
value of Ycan depend on the value of X. We shall assume that this predicted value 
d(X)mustbe chosenso as to minimize the mean squared error E{[Y−d(X)]2}.
Theorem
4.7.3 The prediction d(X)thatminimizes E{[Y−d(X)]2}is d(X)=E(Y |X).
Proof We shall prove the theorem in the case in which Xhas a continuous distri- 
bution, but the proof in the discrete case is virtually identical. Let d(X)=E(Y |X),
and letd∗(X)be an arbitrary predictor. We need only prove that E{[Y−d(X)]2}≤ 
E{[Y−d∗(X)]2}. Itfollows fromEq. (4.7.4) that 
E{[Y−d(X)]2}= E(E {[Y−d(X)]2|X}). (4.7.6)
A similar equation holds for d∗. Let Z=[Y−d(X)]2, and let h(x)=E(Z |x). Sim- 
ilarly, let Z∗=[Y−d∗(X)]2andh∗(x)=E(Z ∗|x). The right-hand side of (4.7.6) is /integraltext
h(x)f 1(x)dx ,andthecorrespondingexpressionusing d∗is /integraltext
h∗(x)f 1(x)dx .So,the 
proofwill be complete ifwe can prove that 
/integraldisplay
h(x)f 1(x)dx ≤/integraldisplay
h∗(x)f 1(x)dx. (4.7.7)
Clearly,Eq.(4.7.7)holdsifwecanshowthat h(x)≤h∗(x)forallx.Thatis,theproof 
is complete if we can show that E{[Y−d(X)]2|x}≤ E{[Y−d∗(X)]2|x}. When we 
conditionon X=x,weareallowedtotreat Xasifitweretheconstant x,soweneed 
toshowthat E{[Y−d(x)]2|x}≤ E{[Y−d∗(x)]2|x}.Theselastexpressionsarenothing 
more than the M.S.E.’s for two different predictions d(x)andd∗(x)of Ycalculated
4.7 Conditional Expectation 261
using the conditional distribution of YgivenX=x. As discussed in Sec. 4.5, the 
M.S.E.ofsuchapredictionissmallestifthepredictionisthemeanofthedistribution 
of Y. In this case, that mean is the mean of the conditional distribution of Ygiven
X=x.Since d(x)isthemeanoftheconditionaldistributionof YgivenX=x,itmust 
havesmallerM.S.E.thaneveryotherprediction d∗(x).Hence, h(x)≤h∗(x)forallx.
If the value X=xis observed and the value E(Y |x)is predicted for Y, then 
the M.S.E. of this predicted value will be Var (Y |x), from Deﬁnition 4.7.3. It follows 
from Eq. (4.7.6) that if the prediction is to be made by using the function d(X)=
E(Y |X), then the overall M.S.E., averaged over all the possible values of X, will be 
E[Var (Y |X)]. 
If the value of Ymust be predicted without any information about the value of 
X, then, as shown in Sec. 4.5, the best prediction is the mean E(Y)and the M.S.E.
is Var (Y). However, if Xcan be observed before the prediction is made, the best 
prediction is d(X)=E(Y |X)and the M.S.E. is E[Var (Y |X)]. Thus, the reduction in 
the M.S.E.that canbe achieved by using the observation Xis 
Var (Y)−E[Var (Y |X)]. (4.7.8)
Thisreductionprovidesameasureoftheusefulnessof Xinpredicting Y.Itisshown 
in Exercise 11 at the end of this section that this reduction can also be expressed as 
Var[ E(Y |X)]. 
It is important to distinguish carefully between the overall M.S.E., which is 
E[Var (Y |X)], and the M.S.E. of the particular prediction to be made when X=x,
which is Var (Y |x).Beforethe value of Xhas been observed, the appropriate value
for the M.S.E. of the complete process of observing Xand then predicting Yis 
E[Var (Y |X)]. Aftera particular value xof Xhas been observed and the prediction
E(Y |x)has been made, the appropriate measure of the M.S.E. of this prediction is 
Var (Y |x).Ausefulrelationshipbetweenthesevaluesisgiveninthefollowingresult, 
whose proofis lefttoExercise 11. 
Theorem
4.7.4 Law of Total Probability for Variances . If XandYare arbitrary random variables for
whichthenecessaryexpectationsandvariancesexist,thenVar (Y)=E[Var (Y |X)]+
Var[ E(Y |X)]. 
Example 
4.7.8 A Clinical Trial . In Example 4.7.3, let Xbe the number of patients out of the ﬁrst 
40 in a clinical trial who have success as their outcome. Let Pbe the probability 
that an individual patient is a success. Suppose that Phas the uniform distribution
on the interval [0 ,1] before the trial begins, and suppose that the outcomes of the 
patientsareconditionallyindependentgiven P=p.AswesawinExample4.7.3, X
hasthebinomialdistributionwithparameters40and pgivenP=p.Ifweneededto 
minimize M.S.E. in predicting Pbefore observing X, we would use the mean of P,
namely,1/2.TheM.S.E.wouldbeVar (P)=1/12.However,weshallsoonobservethe 
valueof Xandthenpredict P.Todothis,weshallneedtheconditionaldistribution 
of PgivenX=x. Bayes’ theorem for random variables (3.6.13) tells us that the 
conditionalp.d.f. of PgivenX=xis 
g2(p |x)=g1(x |p)f 2(p)
f1(x), (4.7.9)
whereg1(x |p)is the conditional p.f. of XgivenP=p, namely, the binomial p.f. 
g1(x |p)=/parenleftbig40 
x/parenrightbig
px(1−p)40 −xforx=0,...,40, f2(p)=1for0 <p< 1isthemarginal 
p.d.f.of P,and f1(x)isthemarginalp.f.of Xobtainedfromthelawoftotalprobability 
262Chapter4 Expectation 
Figure4.12 Theconditional 
p.d.f. of PgivenX=18 in 
Example 4.7.8. The marginal 
p.d.f.of P(priortoobserving 
X) is also shown. 
pDensity 
0.2 0.4 0.6 0.8 1.05
4
3
2
1
0Marginal
Conditional
for randomvariables (3.6.12):
f1(x) =/integraldisplay1
0/parenleftbigg40 
x/parenrightbigg
px(1−p) 40 −xdp. (4.7.10)
This last integral looks difﬁcult to compute. However, there is a simple formula for 
integralsofthis form, namely, 
/integraldisplay1
0pk(1−p) ℓdp =k!ℓ!
(k +ℓ+1)!. (4.7.11)
A proof of Eq. (4.7.11) is given in Sec. 5.8. Substituting (4.7.11) into (4.7.10) yields 
f1(x) =40!
x!(40 −x) !x!(40 −x) !
41!=1
41 ,
forx=0,..., 40. Substituting this into Eq. (4.7.9) yields 
g2(p |x) =41!
x!(40 −x) !px(1−p) 40 −x,for 0<p< 1. 
For example, with x=18, the observed number of successes in Table 2.1, a graph of 
g2(p |18 )is shown in Fig. 4.12. 
If we want to minimize the M.S.E. when predicting P, we should use E(P |x) ,
the conditional mean. We can compute E(P |x) using the conditional p.d.f. and 
Eq. (4.7.11):
E(P |x) =/integraldisplay1
0p41!
x!(40 −x) !px(1−p) 40 −xdp 
=41!
x!(40 −x) !(x +1)!(40 −x) !
42!=x+1
42 .(4.7.12)
So, after X=xis observed, we will predict Pto be (x +1)/ 42, which is very close to 
the proportion of the ﬁrst 40 patients who are successes. The M.S.E. after observing 
X=xis the conditional variance Var (P |x) . We can compute this using (4.7.12) and 
E(P 2|x) =/integraldisplay1
0p241!
x!(40 −x) !px(1−p) 40 −xdp 
=41!
x!(40 −x) !(x +2)!(40 −x) !
43!=(x +1)(x+2)
42 ×43 .
4.7 Conditional Expectation 263
Using the fact that Var (P |x) =E(P 2|x) −[E(P |x) ]2, we see that 
Var (P |x) =(x +1)( 41 −x) 
42 2×43 .
The overall M.S.E. of predicting PfromXis the mean of the conditional M.S.E. 
E[Var (P |X) ]=E/parenleftbigg(X +1)( 41 −X) 
42 2×43 /parenrightbigg
=1
75 ,852E( −X2+40 X+41 )
=1
75 ,852/parenleftBigg
−1
41 40 /summationdisplay
x=0x2+40 
41 40 /summationdisplay
x=0x+41 /parenrightBigg
=1
75 ,852/parenleftbigg
−1
41 40 ×41 ×81 
6+40 
41 40 ×41 
2+41 /parenrightbigg
=301
75 ,852=0.003968.
In this calculation, we used two popular formulas, 
n/summationdisplay
k=0k=n(n+1)
2, (4.7.13)
n/summationdisplay
k=0k2=n(n+1)( 2n+1)
6. (4.7.14)
The overall M.S.E. is quite a bit smaller than the value 1 /12 =0.08333, which we 
would have obtained before observing X. As an illustration, Fig. 4.12 shows how 
much more spread out the marginal distribution of Pis compared to the conditional 
distribution of Pafter observing X=18. ◭
It should be emphasized that for the conditions of Example 4.7.8, 0.003968 is the 
appropriate value of the overall M.S.E. when it is known that the value of Xwill be 
available for predicting Pbut before the explicit value of Xhas been determined.
After the value of X=xhas been determined, the appropriate value of the M.S.E. is 
Var (P |x) =(x +1)( 41 −x) 
75 ,852. Notice that the largest possible value of Var (P |x) is 0.005814 
whenx=20 and is still much less than 1/12. 
A result similar to Theorem 4.7.3 holds if we are trying to minimize the M.A.E. 
(mean absolute error) of our prediction rather than the M.S.E. In Exercise 16, you 
can prove that the predictor that minimizes M.A.E. is d(X) equal to the median of 
the conditional distribution of YgivenX.
Summary
The conditional mean E(Y |x) of YgivenX=xis the mean of the conditional 
distribution of YgivenX=x. This conditional distribution was deﬁned in Chapter 3. 
Likewise, the conditional variance Var (Y |x) of YgivenX=xis the variance of 
the conditional distribution. The law of total probability for expectations says that 
E[E(Y |X) ]=E(Y) . If we will observe Xand then need to predict Y, the predictor 
that leads to the smallest M.S.E. is the conditional mean E(Y |X) .
