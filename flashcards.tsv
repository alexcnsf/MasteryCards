Condition (c)	The probability that gambler A will win the initial fortune of gambler B before he loses his own initial fortune.
Conditional probability function (p.f.) from mixed distribution	The conditional probability function of a discrete random variable given a continuous random variable in a mixed joint distribution, defined by a specific equation.
Pr(A ∪ B)	The probability of event A or event B occurring, given that A and B are independent and Pr(B) is four times Pr(A).
Discrete Uniform Random Variable	A type of random variable that takes on a fixed set of discrete values, with each value having an equal probability of being selected.
Independence of random variables	The concept that two or more random variables are independent of each other if the joint probability density function (PDF) of the variables is equal to the product of their individual PDFs.
Conditional Probability Distribution (p.f./p.d.f)	A probability distribution that describes the probability of one event given that another event has occurred.
Discrete Distribution	A type of distribution that assigns positive probability to at most countably many different values.
Mixed Bivariate Distributions	A type of bivariate distribution where one random variable is discrete and the other is continuous, requiring a joint distribution that combines both discrete and continuous components.
Two sets of posterior probabilities	Posterior probabilities refer to the updated probabilities of a hypothesis or event after new evidence or information has been observed.
Uniform Distribution on Interval [a,b]	A probability distribution where every point in the interval [a,b] is equally likely to be selected, and the probability density function (p.d.f.) is constant over the interval.
Marginal Cumulative Distribution Function (c.d.f.)	The marginal cumulative distribution function (c.d.f.) of a random variable X is the cumulative distribution function that results from marginalizing a joint cumulative distribution function F(x, y) over the variable Y, yielding a single cumulative distribution function F1(x) that describes the probability of X alone.
Chain rule for differentiation	A formula in calculus that allows us to differentiate a composition of functions, by differentiating each function separately and multiplying the results.
Augmented experiment	An extension of an experiment to incorporate additional structure or partitions that can facilitate the calculation of probabilities.
One-to-one function	A function that maps unique elements of its domain to unique elements of its co-domain, and every element of the co-domain is mapped to by at most one element of the domain.
Mixed Distribution	A mixed distribution is a combination of a discrete distribution and a continuous distribution, often necessary in practical problems.
Uniform Distribution on an Interval	A random variable X is said to have a uniform distribution on the interval [a,b] if it is equally likely to fall within every subinterval of [a,b] and the probability that X will belong to a subinterval is proportional to the length of that subinterval.
Initial State	The starting point or initial condition of a Markov chain, often represented by a random variable.
Corollary 3.8.1	A theoretical result stating that if a random variable X has a uniform distribution on an interval and the quantile function G−1 is defined, then the c.d.f. of the random variable Y = G−1(X) will be G.
Bivariate joint c.d.f. of Y1 and Yn	The cumulative distribution function of the minimum and maximum values of a random sample of size n from a distribution.
Risk-Neutral Price	The price of an option that is equal to the present value of the expected value of the option, which is calculated assuming that the expected value of the stock price is equal to the current value of the stock price.
Joint Probability Function (p.f.)	The function that specifies the probability that a set of n random variables X1, ..., Xn all take on specific values x1, x2, ..., xn.
Joint probability function/density	The joint probability function or density of two random variables, denoted as f(x, p) = g1(x | p) * f2(p), given the conditional probability function g1(x | p) of X given P and the probability density function f2(p) of P.
Marginal CDF	The marginal cumulative distribution function (CDF) of a random variable, denoted by Fi(x), represents the probability that the random variable takes on a value less than or equal to the specified value, ignoring the values of other random variables.
Initial Probabilities	The probabilities of the initial state in a Markov chain, describing the likelihood of being in each state at the start.
Conditional probability	The probability of an event given the occurrence or non-occurrence of another event, typically denoted as P(A|C) = k/summationdisplayj=1P(Bj|C)P(A|Bj∩C).
Bivariate Probability Density Function	The bivariate probability density function is a specification of a bivariate distribution at each point in the two-dimensional space.
Marginal Probability Density Function	The marginal probability density function of a single random variable is specified at every value of the variable by integrating the joint probability density function over all possible values of the other variables.
Event	A set of outcomes or occurrences in an experiment or situation.
Sequence	An ordered list of any kind of objects, including numbers, that can be indexed and used to calculate various mathematical sums.
Probability mass function (p.f.) of X	A function that assigns a probability value to each distinct value of a discrete random variable X, summing to 1.
Absorbing States	States in a Markov chain where all probability is concentrated, meaning the chain remains in these states forever.
Multivariate Law of Total Probability	A generalization of the law of total probability to multiple random variables, stating that the marginal pdf of one random variable is the integral of the joint pdf over the other random variables.
Demand for Electricity	The demand for electricity is a random variable that takes values in the interval [1, 150].
Marginal Distribution	The marginal distribution of a random variable is its distribution derived from a joint distribution by summing or integrating out the other variables involved, yielding a distribution that describes the probability of the original variable alone.
Conditional p.d.f.	The probability density function of a random variable given that another random variable has taken on a specific value.
Probability of no box containing more than two balls	The probability that no box contains more than two balls is 5C5/(5^5) = 7/16, given that five balls are thrown at random into n boxes and all throws are independent.
Marginal Probability Density Function (p.d.f.)	The marginal probability density function (p.d.f.) of a random variable X is the probability density function that results from marginalizing a joint probability density function f(x, y) over the variable Y, yielding a single probability density function f1(x) that describes the probability of X alone.
Prior probability	The initial probability of a hypothesis or event before considering new and relevant data or evidence. It is used in Bayes' theorem as a starting point for calculating the posterior probability.
Probability of winning at least once in 50 independent games	The probability of winning at least once in 50 independent games is 1 - (49/50)^(50), given that the probability of winning is 1/50 in each game.
Probable State Distribution	The distribution of the states that a Markov chain will be in, calculated before the initial state or any transitions are observed, distinguishing from the probabilities of being in the various states after observing the initial state or after observing any of the intervening transitions.
Circle	A two-dimensional region defined by an equation of the form x^2 + y^2 = r^2, where r is the radius of the circle.
Binomial Distribution	A probability distribution that models the number of successes in a fixed number of independent trials, each with a constant probability of success (p) and failure (1-p).
Discrete Bivariate Distribution	A discrete bivariate distribution is a joint distribution of two random variables X and Y where each variable has a discrete distribution.
Bayes' Theorem (Conditional)	Bayes' theorem has a conditional version, which is used to calculate the conditional probability distribution of Z given Y=y and W=w, given the conditional probability distribution of Y given Z=z,w and the conditional probability distribution of Z given W=w.
Bivariate Distributions	A set of two random variables, one discrete and the other continuous, that are related and have a joint probability distribution.
Marginal Probability Distribution Function (p.d.f.)	A marginal probability distribution function (p.d.f.) is a function that describes the probability distribution of a random variable, disregarding the values of other random variables. It is derived from a joint p.d.f. by integrating or summing over the other variables.
Discrete random variable	A discrete random variable is a random variable that can only take on a countable set of distinct values, with each value having a corresponding probability.
Truncated Voltage	Truncated voltage is a distorted distribution of a voltage value recorded by a voltmeter that records the actual value of Xif X≤3 and records the value 3 if X> 3.
Theorem 3.3.1	A mathematical statement that establishes the probability of a random variable X exceeding a value x is equal to 1 minus the cumulative distribution function at that value.
Marginal Probability Function (Marginal p.f.)	The marginal p.f. of a random variable X is a function that specifies the probability of X falling within a given range of values, derived from the joint p.f. of X and another random variable Y.
Identity Matrix	A square matrix where all elements on the main diagonal are 1, and all other elements are 0, used to represent the additive identity in matrix operations.
Quantile function	A function that maps a probability to the corresponding quantile or percentile of a random variable.
p.d.f.	Probability density function, it represents the probability distribution of a continuous random variable.
Expected Return	The average profit or loss that an investment is expected to generate, often measured by the mean of the distribution of possible returns.
What type of relationship exists between two variables, as described by their joint probability density function?	<img src="page_91_figure.png">
Positively/Negatively Correlated/Uncorrelated	If X and Y are random variables, it is said that they are positively correlated if ρ(X,Y) > 0, negatively correlated if ρ(X,Y) < 0, and uncorrelated if ρ(X,Y) = 0.
Risk-Free Portfolio	A risk-free portfolio is a combination of assets that produces no change in net worth, regardless of the outcome of the investments.
Bayes' theorem	A mathematical formula used to update the probability of a hypothesis (or event) based on new and relevant data or evidence. It is named after Thomas Bayes and is a fundamental concept in Bayesian statistics, which uses probability theory to model uncertain events.
Mean Squared Error (M.S.E.)	The expected value of the square of the difference between a predicted value and the actual value of a random variable, used to measure the quality of a prediction.
Bolt selection	An experiment where a box of bolts contains long and short bolts, and employees provide different estimates of the composition, leading to a conditional probability calculation.
Joint Cumulative Distribution Function (c.d.f.)	The joint c.d.f. of two random variables X and Y is a function that describes the probability that both X ≤ x and Y ≤ y, for any given values x and y. It is calculated as the integral of the joint probability density function (p.d.f.) over the set of points in the xy-plane where both X ≤ x and Y ≤ y.
Series Connection	A system in which all components must function properly for the system to function properly.
Absorbing State	In a Markov chain, if pii =1 for some state i, then that state is called an absorbing state.
Conditioning on a Random Variable	Conditioning on a random variable means finding the conditional probability distribution of another random variable given the value of the first random variable.
Pg	A matrix used to determine the probability that at least one line will be in use at the next observation time, given that no lines are in use at the current time.
Conditional Mean	The expected value of a random variable Y given a specific value of random variable X, denoted as E(Y|x). It can be computed using the conditional probability density function (p.d.f.) or probability mass function (p.m.f.) of Y given X=x.
Mean return	The average return on investment of a portfolio, calculated by multiplying the number of shares of each stock, the price of each stock, and the expected return on each stock, and summing these values.
Probability Density Function (PDF)	A probability density function is a function that describes the probability of outcomes in a random experiment, where the probability of each outcome is given by the height of the function at that outcome.
What is the probability that a continuous uniform random variable falls within a specified interval?	<img src="page_60_figure.png">
Joint Distribution (Multivariate c.d.f.)	The function that specifies the probability distribution of a set of n random variables X1, ..., Xn.
Support	The closure of the set {x: f(x) > 0} is called the support of the distribution of X, where f is the probability function.
Waiting time	The amount of time a person has to wait for service in a queue.
What determines the amount of resources (water and electricity) needed in a new office complex?	<img src="page_40_figure.png">
Conditioning on a function	In probability theory, conditioning on a function of a random variable means finding the conditional probability distribution of the random variable given the value of the function.
Convex Function	A function g(x) that satisfies the condition that for any two points x and y, and any α in the interval (0,1), g(αx + (1-α)y) ≥ αg(x) + (1-α)g(y), which means that the function is curved upwards and cannot be concave (hole-shaped).
Probability of first three rolls yielding number 6	The probability that the numbers 6 appear exactly three times in 10 rolls of a balanced die.
Countable Collection of Points	A set of points in the xy-plane that can be put into a one-to-one correspondence with the natural numbers, implying that the probability of an uncountable set of points is 0.
Clinical trial	A study designed to evaluate the effectiveness of a treatment or intervention, often involving patients with different outcomes, such as relapse or success.
Joint Probability Density Function (Joint p.d.f.)	The non-negative function f(x,y) that represents the continuous joint distribution of two random variables X and Y, such that the probability of a subset C is equal to the integral of f(x,y) over the surface of C, if the integral exists.
Probability Update	The process of updating prior probabilities to posterior probabilities using Bayes' theorem, based on new information or data.
Functions of Discrete Random Variables	A method of generating new random variables by defining transformations of existing discrete random variables.
Law of Total Probability for Random Variables	A mathematical formula that calculates the marginal probability distribution of a random variable from its conditional probability distributions with respect to another random variable.
Joint Probability Mass Function (p.f.)	The joint probability mass function f(x) of random variables X1, ..., Xn specifies the probability Pr(X = x) at every point x ∈ Rn.
Indicator Random Variable	An indicator random variable is a variable that takes values of 0 or 1, indicating whether a certain event occurs or not.
Interquartile Range (IQR)	A measure of the spread of a probability distribution, defined as the difference between the 75th percentile and the 25th percentile of the distribution.
Probability of specific numbers on bus tickets	The probability that each of the four numbers U, V, W, and X on bus tickets is equally likely to be any of the 10 digits 0, 1, ..., 9 is 1/10, given that each bus ticket contains four numbers.
Service rate	The rate at which a server works in a queue, denoted as Y.
Unconscious Statistician	A metaphorical term referring to individuals who unknowingly or incorrectly assume that the definition of the mean of a transformed random variable is defined by its probability distribution function.
Partition	A set of disjoint and exhaustive events that cover the entire sample space, denoted as B1, B2, ..., Bk, such that B1 ∪ B2 ∪ ... ∪ Bk = S, where S is the sample space of an experiment.
Linear Function of a Continuous Random Variable	A transformation of a continuous random variable X by a linear function Y = aX + b (where a ≠ 0), resulting in a new random variable Y with a probability density function (p.d.f.) given by g(y) = 1/|a|f((y – b)/a) for -∞ < y < ∞.
Probability of Independent Events	The probability of the intersection of multiple independent events is the product of the individual probabilities.
Convolution of Probability Density Functions	The process of combining the probability density functions (p.d.f.'s) of two or more independent random variables to form a new p.d.f.
What is the probability distribution function of a random variable?	<img src="page_95_figure.png">
Balanced Dice	Dice that have an equal probability of landing on each of their sides, typically from 1 to 6.
What is a measure of spread or dispersion that can be used to describe the extent to which values in a distribution are dispersed from the mean?	<img src="page_172_figure.png">
Bayes' Theorem	A mathematical formula used to update the probability of an event A after learning that event B has occurred, given by Pr(A|B) = Pr(A∩B) / Pr(B).
Joint p.f., p.d.f., or p.f./p.d.f.	The probability density function (p.d.f.) of a joint distribution of multiple random variables, describing the probability of each possible combination of outcomes.
What happens when the probability distribution of a continuous random variable is spread over all possible values?	<img src="page_48_figure.png">
Discrete Time Parameter	A type of stochastic process where the process is observed at discrete or separated points in time, rather than continuously in time.
Expectation of a Random Variable	The expected value of a random variable is the sum of the product of each possible value and its probability, which represents the long-run average outcome of an experiment.
Smaller than Var (X −Y)	To have a smaller variance than the variance of the difference between two random variables X and Y.
What is the relationship between a polynomial function and its coefficients?	<img src="page_42_figure.png">
What is a joint probability distribution density function?	<img src="page_67_figure.png">
Hypothetical Observation	The potential or hypothetical observation of additional information beyond what is currently known.
Mixed joint distribution	A joint distribution that combines discrete and continuous random variables, where the conditional probability density functions are defined separately for each type of variable.
Unfavorable Game	A game in which the probability of winning is smaller than the probability of losing.
Conditional Probability Density Function (p.d.f.)	The conditional p.d.f. g(x | y) is a function that defines the probability density of a random variable X given that another random variable Y has taken on a specific value y. It is used to update the probability distribution of X after learning the value of Y.
Variance of a Linear Function	The variance of a linear function of random variables X and Y is Var (aX +bY +c)=a2Var (X)+b2Var (Y)+2ab Cov(X,Y).
Rate	A rate is a measure of intensity or frequency per unit of time, often used to describe the behavior of random variables.
Bernoulli Distribution	A type of binomial distribution with parameters n = 1 and p, where p is the probability of success in a single trial.
Independence of disjoint events A and B	A and B are independent if they are disjoint and each has a positive probability.
Variance of a Sum	The variance of a sum of random variables is equal to the sum of their individual variances plus two times the sum of their covariances.
Marginal Probability Distribution (p.f./p.d.f)	A probability distribution that describes the behavior of a random variable without considering its relationship with another random variable.
Convolution	The combination of two or more independent continuous random variables to form a new random variable, where the probability density function (p.d.f.) of the new variable is the integral of the product of the p.d.f.'s of the original variables.
Genotype	The specific combination of alleles that an individual possesses for a particular gene.
What is the rule governing the relationship between a random variable's quantile function and probability values for a general random variable?	<img src="page_63_figure.png">
What is the set of values in a specific example and mathematical proof?	<img src="page_123_figure.png">
Instrumental Variable	A variable that is used as a control or a treatment in a statistical analysis to determine the effect of another variable on an outcome variable.
Marginal Probability Function (p.f.)	The marginal probability function (p.f.) of a random variable X is the probability function that results from marginalizing a joint probability function f(x, y) over the variable Y, yielding a single probability function f1(x) that describes the probability of X alone.
Random Walk	A random walk is a mathematical process where a particle moves along a line in jumps of one unit, where the probability of moving one unit to the left is p (0 ≤ p ≤ 1) and the probability of moving one unit to the right is 1 − p.
Conditional probability function/density	A function or density that gives the probability of an event given that another event has occurred, denoted as g1(x | y) and g2(y | x).
What is the name given to the cumulative distribution function (c.d.f.) of one random variable when derived from a joint distribution with another variable?	<img src="page_77_figure.png">
Joint CDF	The joint cumulative distribution function (CDF) of two or more random variables, denoted by F(x1, x2, ..., xn), represents the probability that each of the random variables takes on a value less than or equal to the specified value.
Independence of Events	Events are independent if the occurrence or non-occurrence of one event does not affect the probability of another event, regardless of whether both events depend on the same or different random variables.
Service Rate	The rate at which a server serves customers in a queue.
Mean	Another name for the expected value of a random variable, representing the average value or the long-term average behavior of the random variable.
Conditional Continuous Distribution	The probability density function (p.d.f.) of a continuous random variable Y given a specific value of random variable X, denoted as g2(y|x).
Uniform distribution	A distribution that assigns equal probability to all values within a given interval, in this case, 0 ≤ p ≤ 1.
Mean of a Function	The mean of a function of a random variable X, denoted by E(r(X)), is calculated by summing the product of the function values and the probabilities of each outcome in the random variable's distribution.
Median annual income	The middle value of a dataset representing the annual income of a set of families, where at least half of the families have an income greater than or equal to the median and at least half have an income less than or equal to the median.
Moment Generating Function (m.g.f.)	A function used to calculate the expected value of a random variable raised to a power, providing a simple and efficient way to derive the distribution of the variable.
Theorem 4.3.4	A theorem that states that if Y = aX + b, then the variance of Y is equal to the square of a times the variance of X, and the standard deviation of Y is the absolute value of a times the standard deviation of X.
Posterior probability	The updated probability of a hypothesis or event after considering new and relevant data or evidence. It is calculated using Bayes' theorem and takes into account the prior probability of the event, the likelihood of the data given the event, and the prior probability of the event given the data.
Random Sample	A set of items selected from a larger population in such a way that each item has an equal chance of being selected.
Countable Sample Space	A sample space that has a finite or countably infinite number of outcomes, each with a probability assigned to it.
Support of a Joint Distribution	The set of points (x,y) in the xy-plane where the joint probability density function (p.d.f.) is non-zero, representing the region where it is possible for the random variables X and Y to take on values.
Inverse Probability	The probability of an event given that another event has not occurred. It is calculated as the probability of the event divided by the probability of the non-occurrence of the conditioning event.
Continuous Distribution/Random Variable	A continuous distribution or random variable is a random variable X that has a non-negative function f, defined on the real line, such that for every interval of real numbers (bounded or unbounded), the probability that X takes a value in the interval is the integral of f over the interval.
Joint Distribution	The joint distribution of two random variables is a function that describes the probability distribution of the pair of variables. It can be defined for discrete random variables (joint probability mass function) or continuous random variables (joint probability density function).
Prior Probability	A prior probability is the probability of an event or hypothesis before any additional evidence or data is considered. It represents the initial belief or confidence in the event or hypothesis.
What is the probability distribution of a random variable that is the square of a uniformly distributed variable confined to a certain interval?	<img src="page_115_figure.png">
Unique Stationary Distribution	A property of a Markov chain where there is only one probability distribution that the chain's state will converge to over time, and which does not change over time.
Minimum Mean Squared Error (M.S.E.)	The minimum achievable value of the mean squared error between predicted and actual values of a random variable.
Conditional Independence	A set of events A1, ..., Ak are independent if and only if learning that some of the events occur does not change the probability that any combination of the other events occurs.
Probability of each box containing one red and one white ball	The probability that each box contains one red ball and one white ball is (3/6)^3 = 1/8, given that three red and three white balls are thrown at random into three boxes and all throws are independent.
Bivariate Cumulative Distribution Functions	A generalization of the calculation of a cumulative distribution function to a bivariate distribution.
Reliability	The probability that a system or component will function properly.
What is the constraint on the variables x and y in a subset of a region?	<img src="page_84_figure.png">
Law of Total Probability	A theorem that states that the probability of an event A can be calculated as the sum of the probabilities of each member of a partition of the sample space, weighted by the conditional probability of A given each event in the partition.
Probability Function	If a random variable X has a discrete distribution, the probability function (p.f.) of X is a function f such that for every real number x, f(x) = Pr(X = x).
Probability of Randomly Selected Couple	The likelihood that a randomly selected couple from a population possesses certain characteristics.
Random Variable	A real-valued function that is defined on a sample space and assigns a value to each outcome in the sample space.
Conditional Joint Probability Density Function (p.d.f.)	The probability density function that describes the joint distribution of two or more variables given a specific value of another variable.
Symmetric Distribution	A probability distribution is symmetric with respect to a point x0 if the probability density function or probability mass function is equal at x0 + δ and x0 - δ for all values of δ.
What is the probability density function of a random variable that is restricted to the interval [0, 1]?	<img src="page_79_figure.png">
Sufficient Condition for Independence	A sufficient condition for two continuous random variables X and Y to be independent is that their joint support R, where f(x,y) > 0, be a rectangular region with sides parallel to the coordinate axes, and that the joint p.d.f. can be factored into separate functions of X and Y in R.
Matrix Inverse	The matrix that, when multiplied by a given matrix, results in the identity matrix, used to solve systems of linear equations.
How to determine the joint probability density function of two random variables using geometric methods.	<img src="page_69_figure.png">
Characteristic Function	A complex-valued function that generates the moments of a probability distribution; it is defined as the expected value of e^(itX), where t is a real number.
Recessive Allele	An allele that is masked by a dominant allele. In the example, allele 'a' is recessive, and only individuals with the genotype 'aa' will exhibit the recessive trait.
Convolution of Distributions	The process of combining the probability distributions of two or more independent random variables to form a new distribution.
Random Vector	A set of n random variables X1, ..., Xn treated as a single multidimensional random variable X = (X1, ..., Xn).
Uniform random variables	Random variables with a uniform distribution over a set, where all elements in the set have an equal probability of being selected.
Infinite Mean	A random variable X with an infinite mean E(X) when both the sums of positive and negative values of x times their probabilities are infinite, and E(X) does not exist in this case.
Central Moment	A central moment is the expected value of a random variable minus its mean, raised to a power, which is used to describe the distribution of the variable around its mean.
Order statistics	The ordered set of values obtained by arranging a set of random variables X1, ..., Xn in ascending or descending order.
Independent Random Variables	Two or more random variables that are statistically independent, meaning their joint probability distribution is the product of their individual probability distributions.
Distribution of a function of a random variable	The distribution of a function of a random variable is a way to extend the construction of the distribution to the case of several functions of several random variables.
Probability function (p.f.)	A probability function is a function that assigns a probability value to each possible value of a discrete random variable, where the sum of the probabilities equals 1.
Variate Distributions	A statistical concept that deals with the joint probability density function (p.d.f.) or probability mass function (p.f.) of two or more random variables.
Markov chain	A stochastic process in which the outcome of the next state depends only on the current state, and not on any previous states.
Cumulative Distribution Function (c.d.f.)	The function F(x) = Pr(X ≤ x) that represents the probability of a random variable X being less than or equal to a specified value x.
Conditional Bayes' Theorem	Conditional Bayes' theorem is a version of Bayes' theorem that is applied when there is an additional condition or event C. It states that the probability of a hypothesis B given an event A and condition C is equal to the product of the probability of B given C and the probability of A given B and C, divided by the sum of the probabilities of all possible events B given C.
Corollary 4.6.1	States that if a, b, and c are constants under the conditions of Theorem 4.6.6, then the variance of aX + bY + c is equal to a² times the variance of X, b² times the variance of Y, and 2ab times the covariance of X and Y.
Percentile	An alternative term for quantile, representing a point on a cumulative distribution function (c.d.f.) where a specified percentage of the distribution is at or below that point.
Joint p.d.f. (g)	The joint probability density function of two or more random variables.
Stationary Distribution	A probability vector v that satisfies vP = v, representing a distribution that stays the same at all times when the Markov chain starts in this distribution.
Frequency Interpretation of Probability	A view of probability as the long-run relative frequency of an event in a sequence of repeated trials or experiments.
Pr(Xi > 3)	The probability that a customer will have to wait longer than three time units, calculated by integrating the univariate marginal p.d.f. of Xi from 3 to infinity.
Brand A, Brand B	Two different brands of toothpaste, used in an example of a random process.
Inverse of a Function	A function s(y) that is the inverse of a one-to-one function r(x), meaning that s(y) = x if and only if r(x) = y.
Probability of seven games being necessary to determine the winner of the World Series	The probability that it will be necessary to play seven games in order to determine the winner of the World Series is 1 - (p + p^2 + ... + p^6), given that the probability of team A winning any particular game is p.
Discrete Random Variable	A random variable with a discrete distribution.
Random Variables	Variables whose values are determined by chance, often represented by a probability distribution or a random sampling process.
Randomly Selected Couple	A perspective or scenario in which a particular couple is chosen from a larger population without any biases or preferences.
Symmetric distribution	A distribution that is unchanged by replacing each value with its opposite value, often referred to as a "two-sided" distribution.
Conditional k-dimensional p.f., p.d.f., or p.f./p.d.f.	The conditional probability distribution function (p.d.f.) of a k-dimensional random vector given another random vector, defined as the ratio of the joint p.d.f. to the marginal p.d.f. of the conditioning vector.
Unipolar Depression	A type of depression characterized by having no manic disorder, where the symptoms of depression are present without the co-occurrence of manic episodes.
Pr(A ∪ Bc|B)	The probability of event A occurring when event B has occurred, given that event B has a positive probability.
Standard Deviation	The standard deviation of a random variable X is the non-negative square root of its variance, denoted by σ, if the variance exists. It is a measure of the spread or dispersion of a distribution.
Probability of exactly one student attending class on a particular day	The probability that exactly one of three students A, B, and C attends class on a particular day is 0.3, given that they attend class independently and A attends 30% of the time, B attends 50% of the time, and C attends 80% of the time.
Pairwise Independence	Pairwise independence exists when two or more events are independent, but it does not necessarily imply that all events are independent. It is possible for events to be pairwise independent but not mutually independent.
Minimum value of a random sample	The smallest value in a random sample of size n from a distribution.
Conditional Probability Density Function	A function that describes the probability density of a random variable given the value of another random variable.
Random sample	A random sample is a set of independent and identically distributed (i.i.d.) observations taken from a population.
Portfolio	A collection of stocks, bonds, or other investment assets, combined to achieve a specific investment goal or risk-return profile.
Stationary transition distributions	A Markov chain has stationary transition distributions if the transition distribution is the same for every time n (n = 1, 2, . . .), that is, Pr (X n+1=j|Xn=i) = pij for all n, i, j.
Binomial Random Variable	A random variable with a binomial distribution, characterized by a fixed number of trials (n) and a fixed probability of success (p).
Bayes' Theorem for Random Variables	A mathematical formula that calculates the conditional probability distribution of one random variable given another random variable, using the joint probability distribution and the marginal probability distribution of the random variables.
Probability of a Machine Producing a Defective Item	The probability that a machine, M2, produced a defective item, given that the selected item is defective.
Multiplication Rule for Distributions	A theorem that states that the joint probability function or density of two random variables X and Y is the product of the marginal probability function or density of X and the conditional probability function or density of Y given X.
Linear Function of Two Random Variables	A function of the form Y = a1X1 + a2X2 + b, where X1 and X2 are random variables and a1, a2, and b are constants.
Elementary Algebra	A branch of mathematics that deals with the manipulation and solution of algebraic equations, often using basic arithmetic operations and mathematical identities.
Coin Toss	The act of flipping a coin and observing its outcome, which can be either heads or tails, with each outcome having a 50% chance of occurring.
Product of Random Variables	The product of the expectations of multiple random variables, where independence is a necessary condition for equality to hold.
One-to-one transformation	A one-to-one transformation is a transformation where every point in the domain has a unique image in the range.
Geometric Distribution	The distribution of the number of trials until the first success in a sequence of independent and identically distributed Bernoulli trials.
Limit definition of conditional probability density function	The conditional probability density function of a continuous random variable given a specific value of another random variable is defined as a limit, where the probability density of the conditioning event approaches zero.
Joint Probability Density Function (p.d.f.)	A function f(x,y) that specifies the probability density of the joint distribution of two random variables X and Y, giving the probability of finding a point (x,y) in the xy-plane.
Joint Probability Distribution Function (p.f.)	The joint probability distribution function (p.f.) of two random variables is a function that gives the probability of the joint occurrence of the two variables, typically denoted as f(x, y).
Independent and identically distributed (i.i.d.)	A set of random variables that are both independent and have the same probability distribution, often written as X1, X2, ..., Xn∼F(), where F() is a common distribution.
Marginal Probability Density Function (p.f.)	A function f2(y) that specifies the probability of a particular value y of random variable Y.
Joint Probability Function/Probability Density Function (JPFF/JPDF)	A function that describes the probability distribution of two random variables, X and Y, where X is discrete and Y is continuous. The JPFF/JPDF, denoted by f(x, y), is non-negative for all x and y, and satisfies the condition:/integraldisplay∞ −∞∞/summationdisplay i=1 f(x i, y) dy = 1.
Fair Game	A game where the probability of winning is equal for both players, typically represented by a probability value of 0.5 or 1/2.
Probability Space	A set of possible outcomes or events, often used to represent the sample space of an experiment or situation.
Infinite Sequence	A sequence of events that continues indefinitely, often used in probability theory to analyze the behavior of a system.
Marginal probability density function (PDF)	The probability density function (PDF) of a single random variable, obtained by marginalizing over the other variables in a joint PDF.
Median Absolute Deviation (MAD)	A measure of the spread or dispersion of a distribution, calculated as the median of the absolute deviations of individual values from the median.
p(ij)	The conditional probability of being in state j at time n+2 given being in state i at time n, in a Markov chain.
Probability Mass Function (PMF)	A function that describes the probability distribution of a discrete random variable, often denoted as f(x) and giving the probability mass at each point in the sample space.
Schwarz Inequality	A mathematical result stating that for all random variables U and V, the square of the expected product of U and V is less than or equal to the product of the expected squares of U and V, i.e., [E(UV)]^2 ≤ E(U^2) * E(V^2).
Mean Existence	The mean of a distribution exists if the integral of the absolute value of the distribution's functions converges at the boundaries of the domain.
Conditional Distribution	The distribution of a random variable X given that another random variable Y has taken on a particular value y. It is defined as the conditional probability density function (p.f.) g1(x |y) = f(x,y) / f2(y), where f is the joint p.f. of X and Y, and f2 is the marginal p.f. of Y.
What happens to the value of Y when the value of X is known?	<img src="page_198_figure.png">
Probability of being on time on the fourth day of class	The likelihood that someone will arrive on the fourth day of class after having been assessed on their punctuality on previous days.
Disk	A disk is a region in two-dimensional space defined by an equation of the form (x-a)² + (y-b)² ≤ r², where (a, b) is the center of the disk and r is its radius.
Random Variable with a Discrete Distribution	A random variable with a discrete distribution is a variable that takes on distinct, isolated values with different probabilities.
Multiplication Rule	A theorem stating that the probability of a sequence of events occurring is equal to the product of the probabilities of each event given the previous events, denoted as P(A1 ∩ A2 ∩ ... ∩ An) = P(A1)P(A2|A1)P(A3|A1 ∩ A2) ... P(An|A1 ∩ A2 ∩ ... ∩ An-1).
Discrete distribution	A distribution of distinct values or outcomes that can be referred to by a single value or outcome, with probability mass concentrated on specific values.
Conditional Version	The conditional version of a result or theorem is obtained by adding "conditional on W=w" to every probabilistic statement in the original result, including probabilities, c.d.f.'s, quantiles, names of distributions, p.d.f.'s, and p.f.'s.
Mean of a Transformed Random Variable	The expected value of a function g(X) of a random variable X, which can be calculated using the probability density function or its cumulative distribution function.
De Finetti-Halmos Theorem	A theorem stating that if two random variables X and Y have a continuous joint distribution, then each has a continuous distribution when considered separately.
Moment inequality	A moment inequality is a statement that shows a relationship between the moments of a random variable, such as E(X^2) ≥ [E(X)]^2 or E[(X - µ)^4] ≥ σ^4, where µ is the mean and σ is the variance.
Probability Integral Transformation	A transformation that maps a continuous random variable to a uniform random variable on the interval [0, 1] by taking the cumulative distribution function (c.d.f.) of the original random variable.
Coin	A piece of currency with a head on one side and a tail on the other, used in an example of a random process.
Mean of Bounded Discrete Random Variable	The expectation of a bounded discrete random variable X, denoted by E(X), is a number defined as the sum of the product of each possible value of X and its probability function f(x).
Random Variable E(Y |X)	The random variable that takes the value 0.609 when the sampled household has one member, 1.057 when the sampled household has two members, and so on. It represents the expected value of Y given X.
Independent Random Variables Condition	Two random variables are independent if the rows of the table specifying their joint probability distribution function (p.f.) are proportional to one another, or equivalently, if the columns of the table are proportional to one another.
Bernoulli probability function	The Bernoulli probability function g1(x|p) is the probability mass function of a binomial distribution with parameters 40 and p, evaluated at x.
Marginal Probability Density Function (Marginal p.d.f.)	The marginal p.d.f. of a random variable X is a function that specifies the probability density of X falling within a given range of values, derived from the joint p.d.f. of X and another random variable Y.
What is the probability distribution of the minimum value in a sample of uniform random variables?	<img src="page_127_figure.png">
Continuous Distribution	A probability distribution where the random variable can take any value within a given range or interval, rather than being limited to a specific set of discrete values.
Marginal Cumulative Distribution Function (Marginal c.d.f.)	The marginal c.d.f. of a random variable X is a function that specifies the probability that X falls within a given range of values, derived from the joint c.d.f. of X and another random variable Y.
Factorization	A joint probability density function (p.d.f.) f(x,y) of two independent random variables X and Y can be represented in the form f(x,y) = h1(x)h2(y), where h1(x) is a non-negative function of x alone and h2(y) is a non-negative function of y alone.
Upper Quartile	The 3/4 quantile or 75th percentile of a distribution, which represents the value above which 75% of the data lies.
What is the distribution of changes in portfolio value?	<img src="page_59_figure.png">
Law of Total Probability (Conditional)	The law of total probability has a conditional version given another random vector W=w, which is used to calculate the conditional probability distribution of Y given W=w.
Center of Gravity	The center of gravity of a probability distribution is its mean, where the weights are the probabilities and the locations are the values of the variable.
What is the concept that a small change in probability assigned to a large value can significantly impact?	<img src="page_157_figure.png">
Stochastic Matrix	A square matrix where all elements are nonnegative and the sum of the elements in each row is 1.
Defective parts problem	A probabilistic problem involving the production of parts by a machine, where the proportion of defective parts is unknown, and the number of defectives among a sample of parts follows a binomial distribution.
Uniform distribution on integers	A uniform distribution on integers is a probability distribution that assigns equal probability to each integer in a given range, such as 0 to 999 in the case of the state lottery game.
Conditional Probability Density Function (PDF)	A conditional probability density function is a function that describes the probability of an outcome of one random variable, given the value of another random variable.
Mean Absolute Error (MAE)	A measure of the average absolute difference between the values of a random variable and a prediction or estimate.
Unbounded Random Variables	Unbounded random variables are continuous random variables that can take on any real value, without any upper or lower bound.
Pseudo-random number generator	A method to generate random numbers that mimic the properties of truly random numbers, but are generated algorithmically.
Value at Risk (VaR)	A quantity that represents the potential loss of a portfolio over a fixed time horizon, calculated as the 1 - α quantile of the distribution of the portfolio's returns, where α is the confidence level. It represents the maximum loss that an investor can expect to incur with a specified level of confidence.
Conditional mean (expectation)	The conditional mean of a random variable P given a value x, denoted as E(P|x), is the expected value of P after observing X=x.
Conditional probability function (c.p.f.)	A function that specifies the probability of a particular outcome given a specific set of conditions or events. In the context of Markov chains, the c.p.f. is used to describe the probability of transitioning from one state to another.
Change of Variable	A transformation of a function or an integral by replacing the variable or variables with a new function or variables, often used to simplify or solve problems.
Bivariate Distribution	The collection of joint probabilities of a bivariate distribution.
Cauchy Distribution	A specific type of continuous distribution with a p.d.f. given by f(x) = 1/π(1+x^2) for -∞ < x < ∞, which is known to have an infinite expectation.
Unbounded Random Variable	A continuous random variable that can take on values outside of a well-defined range, often resulting in an infinite expectation.
Matrix Squaring	The operation of taking the square of a matrix, used in Markov chains to compute the conditional probability of being in a state at a future time given the current state.
Theorem 4.3.2	A theorem that states that the variance of a random variable X is always non-negative, and that if X is bounded, the variance must exist and be finite.
Uncorrelated random variables	Random variables X and Y are uncorrelated if their covariance is zero, meaning that there is no linear relationship between them.
Problem Restatement	Recasting a problem in a simpler or more manageable form, often involving Bayes' theorem or the multiplication rule.
Probability of certain sequence in three rolls of a balanced die	The probability that the numbers appear in the sequence X1 > X2 > X3 in three rolls of a balanced die is 1/24.
Jacobian	The Jacobian J is a matrix of partial derivatives that describes the transformation specified by the equations connecting the variables Y 1, ..., Y n. It is used to calculate the joint probability density function of Y 1, ..., Y n from the joint probability density function of X 1, ..., X n.
How is the variance of a portfolio's return affected by the variances and correlations of the individual assets?	<img src="page_200_figure.png">
Strictly increasing or strictly decreasing function	A function r(x) of a random variable X is strictly increasing or strictly decreasing if its derivative (or inverse) is continuous and either always positive or always negative.
How to calculate the mean of a function of a random variable?	<img src="page_158_figure.png">
Exponential distribution	A continuous probability distribution where the probability density function (p.d.f.) is proportional to the exponential of minus the sum of the variables, exp(-Σxi), where xi is a positive value.
Constant Value of p.d.f.	The constant value of the probability density function (p.d.f.) throughout the interval [a,b], which is equal to 1/(b-a).
Direct Derivation of the p.d.f.	A method to compute the probability density function (p.d.f.) of a random variable Y by applying the chain rule for differentiation to the cumulative distribution function (c.d.f.) G of the original random variable X, when the function r(x) is one-to-one and differentiable.
Median	The 1/2 quantile or 50th percentile of a distribution, which divides the distribution in half as closely as possible.
Initial Probability Vector (v)	A distribution of the state at time 1, used to calculate all probabilities associated with the Markov chain.
Cumulative distribution function (c.d.f.)	The function that describes the probability that a real-valued random variable X will take on a value less than or equal to a given real number x.
Joint Discrete Distribution	A distribution where the random vector (X1, ..., Xn) can take on only a finite or an infinite sequence of different possible values (x1, ..., xn) in Rn.
Joint distribution of Y1 and Yn	The probability distribution of the minimum and maximum values of a random sample of size n from a distribution.
Continuous Random Variable	A random variable whose cumulative distribution function is continuous and has no jumps.
Independent	Having a probability distribution such that knowledge of one random variable does not affect the probability distribution of the other.
Joint Probability	The probability of an event {(X,Y)∈C} for a bivariate distribution.
Single-Server Queue	A system in which customers arrive at a single server, which serves them one by one.
Close to each other	Two posterior probabilities are close to each other if they have similar values, indicating that the new evidence or information has not significantly changed the initial probability.
Cauchy distribution	A Cauchy distribution is a probability distribution with probability density function f(x) = 1 / (π(1 + (x - c)^2)), where c is a constant, and is characterized by the lack of a finite mean or variance.
Histogram	A histogram is a graphical display of a collection of numbers. It is particularly useful for displaying the observed values of a collection of random variables that have been modeled as conditionally i.i.d.
Individual Cumulative Distribution Functions (c.d.f.s)	Individual cumulative distribution functions (c.d.f.s) F1(x) and F2(y) are derived from the joint c.d.f. (F(x, y)) by taking limits as y→∞ and x→∞, respectively. F1(x) is the probability that a random variable X takes a value less than or equal to x, while F2(y) is the probability that a random variable Y takes a value less than or equal to y.
Joint Probability Function	The joint probability function of two random variables X and Y is a function f such that for every point (x, y) in the xy-plane, f(x, y) = Pr(X = x and Y = y).
Conditional Distributions	Conditional distributions are the probabilities of events determined by random variables, conditional on events determined by other random variables. They are used to adjust the probabilities associated with the random variables that have not yet been observed, after observing some of the random variables of interest.
Vector Space	A set of vectors over a scalar field, such as a set of real or complex numbers, that satisfies certain axioms including closure, addition, and scalar multiplication.
Continuous distribution	A probability distribution where the random variable can take on any value within a certain range or interval, with a probability density function (PDF) that describes the distribution.
Probability	A measure of the likelihood of an event occurring, ranging between 0 (impossible) and 1 (certain).
Independent Events	Two events are considered independent if the occurrence of one event does not affect the probability of the other event. In other words, the probability of one event remains unchanged regardless of whether the other event occurs or not.
Probability (1/2 each)	The chance of an individual getting one allele or the other from one of its parents.
Variance	The variance of a random variable is the expected value of the squared difference between the variable's value and its mean, which represents the spread or dispersion of the variable's values.
What is the typical value or average location of a distribution?	<img src="page_157_figure.png">
Uniform Distribution	A type of distribution that assigns equal probability to each of a finite set of integers.
Theorem 3.8.1 (Function of a Discrete Random Variable)	This theorem states that if X is a discrete random variable with p.f. f and Y = r(X) is a function of X, then the p.f. of Y is g(y) = Pr(Y = y) = ∑x: r(x) = y f(x).
What is the possible range of values for a continuous random variable Y?	<img src="page_79_figure.png">
Joint Probability Distribution (p.f./p.d.f)	A probability distribution that describes the joint behavior of two or more random variables.
Marginal probability density function	The marginal p.d.f. of a random variable P, denoted as f2(p), is the probability density of P before observing X, and is equal to 1 for 0<p<1.
Covariance	A measure of the relationship between two random variables, calculated as the expected value of the product of their deviations from their means.
Binomial distribution	A discrete probability distribution that models the number of successes in a fixed number of independent trials, where each trial has a constant probability of success.
Probability of at least one student attending class on a particular day	The probability that at least one of three students A, B, and C attends class on a particular day is 0.9, given that they attend class independently and A attends 30% of the time, B attends 50% of the time, and C attends 80% of the time.
Augmented Experiment	An experiment can be augmented to include potential or hypothetical observation of any additional information that would be useful to help calculate any desired probabilities.
Joint Probability Function (Joint p.f.)	A function that gives the probability of a specific combination of values for two or more random variables.
Joint Probability Function (Discrete Case)	In the case of a discrete joint distribution, the joint probability function f(x, y) = Pr(X = x and Y = y) for every point (x, y) in the xy-plane, and there are at most countably many pairs (x, y) that account for all of the probability.
Unfair Game	A game where the probability of winning is not equal for both players, typically represented by a probability value other than 0.5 or 1/2.
Matching Problem	A problem involving assigning elements (such as letters or objects) to corresponding positions (such as envelopes or slots).
Probability of a Rectangle	The probability of a rectangle in the xy-plane, calculated using the joint cumulative distribution function (joint c.d.f.) F.
Initial Fortune	The amount of money a player starts with in a game or sequence of games.
Conditional Variance	For a given value of X, Conditional Variance (Var(Y|x)) denotes the variance of the conditional distribution of Y given X=x. It is defined as the expectation of the square of the difference between Y and its conditional expectation given X=x.
Probability Density Function (p.d.f.)	A function associated with a continuous random variable that gives the probability density at each point in the interval, such that the integral of the function over the entire interval equals 1.
Transformation	A transformation is a set of equations that describes the relationship between the variables Y 1, ..., Y n and the variables X 1, ..., X n. The Jacobian of the transformation is used to calculate the joint probability density function of Y 1, ..., Y n from the joint probability density function of X 1, ..., X n.
Service Time	The time it takes for a server to serve a customer in a queue.
Function of a Discrete Random Variable	The function of a discrete random variable is a new random variable that is defined as a transformation of the original random variable.
Brute-Force Distribution of a Function	A method to find the distribution of a function of several random variables by transforming the joint probability density function of the random variables.
Density Is Not Probability	A reminder that the probability density function (p.d.f.) itself does not equal the probability, but rather it is used to calculate the probability by integrating the p.d.f. over the desired region.
Parallel Connection	A system in which at least one component must function properly for the system to function properly.
Genetic allele	A variant of a gene that is present at a specific location on a chromosome.
Continuous Distributions	Continuous distributions are distributions of continuous random variables, which can take on any value within a given interval or range.
Multivariate Distribution	A distribution that describes the behavior of multiple random variables and their relationships with each other.
Bounded Continuous Random Variable	A continuous random variable with a well-defined upper and lower bound, as opposed to an unbounded random variable which can take on values outside of this range.
Efficient portfolio	A portfolio that has the smallest variance for a fixed mean return, or the largest mean return for a fixed variance, among all possible portfolios.
Boys A, B, and C	Three boys throwing a ball, used in an example of a Markov chain.
Interpolation (X1 + X2)	The sum of two independent and identically distributed uniform random variables, representing the probability of a continuous endpoint (e.g., [0, 1]).
Marginal Expected Value	The marginal expected value of a random variable X, denoted as E(X), is the average value of X when all other variables are integrated out or marginalized.
Uniform pseudo-random number generator	A method to generate random numbers with a uniform distribution on a specified interval, which can be used to simulate random variables with other specified distributions.
Expectation of a Combination of Random Variables	The expectation of a product of independent random variables is not always equal to the product of their individual expectations.
Inverse Matrix	The inverse of a matrix is a matrix that, when multiplied by the original matrix, results in the identity matrix.
What is the support of a random point selected from inside a circle?	<img src="page_69_figure.png">
What is the probability that a continuous random variable takes on a value less than or equal to a specified value x4?	<img src="page_55_figure.png">
Nonsingular Matrix	A nonsingular matrix is a square matrix that has no zero row or column, and can be inverted to obtain its inverse.
Moment generating function (m.g.f.)	A moment generating function is a function that generates the moments of a random variable, defined as E(etX) for some real number t, and used to determine the probability distribution of the random variable.
Bayes' theorem for random variables	Bayes' theorem for random variables states that the conditional p.d.f. of P given X=x is the product of the conditional p.f. of X given P, the marginal p.d.f. of P, and the marginal p.f. of X, denoted as f2(p), g1(x|p), and f1(x), respectively.
Expectation of a Function of Two Variables	The mean of the sum of the squares of two random variables X and Y, which can be calculated using the joint probability density function (p.d.f.) of X and Y.
Linear Function	A function that satisfies the property that f(aX) = af(X) for all real numbers a and all values of X.
Expectation of Random Variables	A mathematical expectation of a random variable, denoted by E(X), which is the probability distribution's long-run average value of X.
Arrival Rate	The rate at which customers arrive at a queue.
Condition a)	The probability that gambler A will win the initial fortune of gambler B before he loses his own initial fortune.
Conditional Probability Density Function (CPDF)	A function that describes the probability distribution of a random variable given the value of another random variable, used to calculate conditional probabilities and expected values.
Initial Distribution	The probability distribution of the initial state of a Markov chain, which specifies the probability of being in each state at a given time.
Approximation	An estimate or approximation of a quantity, often obtained using a simplified method or approach.
What is the minimum possible number of heads that can occur in 10 independent coin tosses?	<img src="page_41_figure.png">
Craps	A popular gambling game played by rolling two dice, where the sum of the numbers determines the outcome of the game.
Value of Pr(A) in independent events A, B, and C	The value of Pr(A) is 1/5 given that A, B, and C are independent, and Pr(A ∪ B ∪ C) = 5Pr(A).
Rectangle	A two-dimensional region bounded by four lines, described by a set of coordinates (x, y) that satisfy a certain condition.
Derivative	A measure of how a function changes as its input changes.
Monotone Increasing	The property of a function F(x,y) that it is increasing in x for each fixed y, and increasing in y for each fixed x.
Expected Value	The average value of a random variable, calculated by multiplying each possible outcome by its probability and summing the results.
Iterative Solution	A method of solving a recursive equation by iteratively applying the equation to find the value of the variable.
Expectation of a Combination of Random Variables (Example)	The expectation of a product of random variables can be calculated by multiplying the expectations of each variable, as long as the variables are independent.
Stochastic process	A sequence of random variables indexed by time, which can be used to model and analyze phenomena that exhibit randomness and uncertainty.
Joint p.d.f.	A function that represents the probability distribution of multiple random variables, both continuous and discrete, where the probability is calculated by summing over discrete variables and integrating over continuous variables.
High School	A high school is a type of educational institution that provides secondary education to students, typically between the ages of 14 and 18.
What is the expected rate of failure for appliances manufactured by a particular company?	<img src="page_158_figure.png">
Joint Distribution Function	The same as the joint cumulative distribution function (joint c.d.f.).
Continuous distribution function (c.d.f.)	A function that describes the probability distribution of a continuous random variable, giving the probability that the variable takes on a value less than or equal to a given point.
Marginal joint p.d.f.	The marginal joint probability density function of a subset of random variables X1, ..., Xn, denoted by f0(x1, ..., xk), is a function that describes the probability distribution of the subset variables, ignoring the remaining variables.
Mutually Independent Events	Events are mutually independent if the probability of their intersection is the product of their individual probabilities, regardless of whether a subset of these events is considered. This is expressed mathematically as Pr (A i1∩...∩A ij)=Pr (A i1)...Pr (A ij) for every subset Ai1, ..., A ij of these events.
Probability of Winning	The chance of a player winning a game, game, or a sequence of games, given the probability of winning each game and the player's initial fortune.
Ned	A game where the player needs to obtain a random value before the original value is obtained a second time to win. The probability of winning this game is skipped as it cannot be determined.
Event Inclusion	The probability that an event A is included in an event B, given certain conditions.
Independence of Random Variables	Random variables that do not affect each other's outcomes or probabilities, such that the probability distribution of one variable does not depend on the value of the other variables.
Conditional Expectation	The expected value of a random variable Y given the value of another random variable X, denoted by E(Y|X), which is the function that minimizes the expected squared difference between Y and d(X).
Third central moment of X	The third central moment of a random variable X is a measure of the asymmetry of its distribution, defined as E[(X - E(X))^3], where E(X) is the expected value of X.
What is the probability density function (p.d.f.) of X in a given example?	<img src="page_79_figure.png">
Jensen's Inequality	A theorem that states that if g is a convex function and X is a random vector with finite mean, then E[g(X)] ≥ g(E(X)), which provides a lower bound on the expected value of a function of a random variable.
Specified distribution	A specific probability distribution, characterized by a probability density function or cumulative distribution function, that defines the behavior of a random variable.
Conditional distribution	A probability distribution of a subset of random variables given that some other variables have specific values, often denoted by f(x|y).
Distribution of a Random Variable	The distribution of a random variable is the collection of all probabilities of the form Pr(X ∈ C) for all sets C of real numbers such that {X ∈ C} is an event.
Range	The distance between the minimum and maximum values of a random sample.
Genotype and Phenotype	In population genetics, a gene has different forms, known as alleles (A and a). An individual's genotype refers to their genetic makeup, while their phenotype refers to the visible traits they exhibit, such as hair color or blood type. The dominant allele (A) can mask the effects of the recessive allele (a).
Uncorrelated Random Variables	Random variables whose covariance is zero, meaning that the systematic relationship between the variables is absent.
Joint Distribution Function (c.d.f.)	The function that specifies the probability that a set of n random variables X1, ..., Xn all take on values less than or equal to specific values x1, x2, ..., xn. It is formulated by the relation F(x1, ..., xn) = Pr(X1 ≤ x1, X2 ≤ x2, ..., Xn ≤ xn).
Uniform Pseudo-Random Number Generator	A deterministic algorithm that generates numbers that appear to have the uniform distribution on the interval [0,1], often used in statistical analyses to create pseudo-random samples.
Law of Total Probability for Expectations	A mathematical theorem stating that the mean of the conditional mean of a random variable Y given X is equal to the mean of Y, i.e., E[E(Y |X)] = E(Y).
Theorem 4.3.3	A theorem that states that the variance of a random variable X is zero if and only if the entire probability distribution of X is concentrated at a single point.
Expectation of a Transformed Random Variable	The expected value of a function g(X) of a random variable X, which can be calculated using the probability density function of X or its cumulative distribution function.
Maximum value of a random sample	The largest value in a random sample of size n from a distribution.
Conditional Probability Table (CPT)	A table that summarizes the probability of each possible outcome given a set of variables or conditions.
Experiment	A process or situation where the outcome is uncertain and can be measured, such as rolling dice or selecting balls from a box.
Corollary	If uncorrelated random variables X1,...,Xn are added, the variance of the sum is equal to the sum of their individual variances.
What type of continuous distribution is characterized by a constant probability density over a specified interval?	<img src="page_50_figure.png">
What is the relationship between a set A and a sequence of events B1-B5?	<img src="page_6_figure.png">
Mutually independent conditions for Ac, Bc, and Cc	The events Ac, Bc, and Cc are mutually independent if A, B, and C are mutually independent and Pr(D|A∩B) = 0.
Second Largest	The largest value in a set of observations, excluding the largest value.
Quantile Function	The quantile function of a cumulative distribution function F(x) is the inverse function of F(x), denoted as x = Q(p), which gives the value of x for a given probability p.
Stochastic Process	A sequence of random variables with a probability model that assigns a joint distribution to the variables, making it possible to predict the state of the process at a future time given the past states and the present state.
Success	An outcome of an event that meets a specific criteria, often used in conjunction with failure to describe the behavior of a system.
Time-Dependent Markov Chain	A Markov chain where the probabilities of transitioning from one state to another vary over time.
What is the probability that a randomly selected household owns at least two of both cars and televisions?	<img src="page_66_figure.png">
Correlation	A measure of the linear relationship between two random variables, calculated as the covariance of the standardized variables.
Direct Calculation	In probability theory, direct calculation refers to the process of finding the probability distribution of a function of a random variable by evaluating the integral or sum directly.
Probability Quantile	A quantile that has a specific probability of being exceeded, such as the 0.01 quantile or the 0.99 quantile.
Continuous Joint Distribution	A probability distribution over two random variables X and Y, where there exists a non-negative function f(x,y) such that for every subset C of the plane, the probability is equal to the integral of f(x,y) over the surface of C, if the integral exists.
Multiplication Rule for Conditional Probabilities	The theorem stating that the probability of two events occurring is the product of the probability of the first event and the conditional probability of the second event occurring given the first event, denoted as P(A ∩ B) = P(B) \* P(A|B) when P(B) > 0, and vice versa.
Random variable	A mathematical representation of a set of possible outcomes of a random experiment, where each outcome is assigned a unique value. In the context of the job candidate scenario, random variables are used to model the quality of each candidate and the outcome of the hiring process.
Mean Absolute Error (M.A.E.)	The mean value of the absolute difference between predicted and actual values of a random variable.
Conditionally Independent Random Variables	A set of random variables are conditionally independent given a random vector Z if the conditional pdf/f of the joint distribution factorizes into the product of the conditional pdf/f of each variable given Z.
Conditionally Independent	When a collection of events become independent given that another event has occurred.
Cauchy-Schwarz Inequality	A theorem stating that if X and Y are random variables with finite variance, then [Cov(X,Y)]^2 ≤ σ_x^2 * σ_y^2, and -1 ≤ ρ(X,Y) ≤ 1, where ρ(X,Y) is the correlation coefficient.
Probability Measure	A set function that assigns a non-negative real number to each subset of the xy-plane, satisfying certain axioms and used to quantify the probability of events in the joint distribution of random variables X and Y.
What is a common practice in plant breeding to produce new varieties with desired traits?	<img src="page_138_figure.png">
Transition distributions	The conditional distributions of the state at time n+1 given the state at time n, that is, Pr (X n+1=j|Xn=i) for i, j =1, . . . , k and n = 1, 2, . . . .
Pin	A matrix used to determine the probability that exactly four lines will be in use at the next observation time.
Conditional Probability with Bayesian Update	Updating the probability of an event based on new information or evidence using Bayes' theorem.
Independence	The relationship between two events where the occurrence of one does not affect the probability of the other, Pr(A∩B) = Pr(A) × Pr(B).
Correlation Coefficient	A measure of the degree to which two random variables, X and Y, vary together, defined as Cov(X, Y) / (σX * σY), where Cov(X, Y) is the covariance and σX and σY are the standard deviations of X and Y, respectively.
What is the relationship between a prior probability density and the probability density given new information?	<img src="page_208_figure.png">
Expected Value of Waiting Time	The expected value of the waiting time in a queue, which is inversely proportional to the rate at which customers are served (Z), expressed as Y = 1/Z.
Independence testing	A statistical method to determine if a sequence of observations appears to be independent and identically distributed according to a specified distribution.
Cumulative distribution function	A mathematical function that describes the probability that a random variable will take on a value less than or equal to a given value. In the context of the job candidate scenario, the cumulative distribution function is used to model the probability of hiring the best candidate and to optimize the choice of r.
Uniform Distribution on Integers	A probability distribution where all integers within a specified range (a, ..., b) are equally likely to be chosen, with a probability of 1/(b-a+1) for each integer.
Multivariate Transformation	A generalization of a one-to-one transformation to the case of several random variables, used to transform the joint distribution of multiple random variables.
Marginal Univariate Distribution	The distribution of a single random variable Xi, obtained by integrating or summing over all other variables Xj (j≠i) in the joint distribution.
Deriving P.D.F. of a New Random Variable	A method for finding the probability density function (p.d.f.) of a new random variable by differentiating the cumulative distribution function (c.d.f.) of the new random variable with respect to the new variable.
Arbitrage opportunity	A situation where an investor can profit by taking opposing positions in two or more assets or markets, without risking any capital, by exploiting a discrepancy in prices or values.
Normalizing Constant	A normalizing constant is a value that is needed to ensure that the integral of a probability density function is equal to 1. It is often denoted as c and is used to normalize the function.
Moment Generating Function	The moment generating function is a related tool that aids in deriving distributions of sums of independent random variables and limiting properties of distributions.
Tossing Coins	An experiment where a coin is tossed a certain number of times, resulting in a random number of heads or tails.
Integral	A mathematical operation that calculates the area under a curve or the accumulation of a function over a specified interval.
What happens to the variance of a random variable when it is multiplied by a constant and shifted by another constant?	<img src="page_175_figure.png">
What is the range of values for a certain random sample?	<img src="page_126_figure.png">
Markov Chain	A sequence of random variables, where the future state of the chain depends only on the current state, and not on any of the previous states.
Jump	A sudden increase in the cumulative distribution function at a specific point x, indicating a non-zero probability of the random variable taking on that exact value.
Mean Squared Error (MSE)	A measure of the average squared difference between the values of a random variable and a prediction or estimate.
Borel Paradox	A phenomenon where the conditional distribution of a random variable given a condition can be different from the conditional distribution given a similar condition, often due to the concept of conditioning on events with probability 0.
Conditional Probability Distribution	A conditional probability distribution is a function g(x | y) that defines the probability distribution of a random variable X given that another random variable Y has taken on a specific value y. It is used to update the probability distribution of X after learning the value of Y.
Gambler's Ruin Problem	A probability problem that involves two gamblers, A and B, who play a game where each play has a probability p for A to win 1 dollar from B and 1-p for A to lose 1 dollar to B. The game continues until one of the gamblers' fortunes reaches 0 dollars or a predetermined target k dollars. The problem is to determine the probability that A's fortune will reach k dollars before reaching 0 dollars.
Integrated density function (IDF)	A probability distribution that describes the range of a sample of random variables.
Expectation	The expected value of a random variable, calculated as a weighted average of its possible values, where the weights are given by the probability distribution of the variable.
Probability density function (p.d.f.)	A function that describes the probability distribution of a continuous random variable, giving the probability density at a given point.
Limit	A concept in probability theory that describes the behavior of a system as the number of observations increases, often used to describe the long-run proportion of successes.
Gambler's Fortune	The amount of money or other resources that a gambler has at a given point in a game or series of games.
Subjective Interpretation of Probability	A view of probability as a degree of belief or confidence in the occurrence of an event.
Non-uniqueness of a p.d.f.	A property of continuous distributions where the values of a p.d.f. can be changed arbitrarily at a finite number of points without affecting the probability distribution of a random variable.
How do the numbers of observed service times tend to decrease as the center of the subinterval increases?	<img src="page_111_figure.png">
Probability of sum of two loaded dice being 7	The probability that the sum of the numbers appearing on two loaded dice is 7, given that the probability of each number appearing is 0.1 for k=1,2,5, or 6 and 0.3 for k=3 or 4 when either die is rolled.
Discrete Joint Distribution	A discrete joint distribution of two random variables X and Y is a distribution where there are only finitely or at most countably many different possible values (x, y) for the pair (X, Y).
Limits at ±∞	The cumulative distribution function F(x) has limits at ±∞, meaning that lim x→−∞F(x) = 0 and lim x→∞F(x) = 1, indicating that the probability of X being less than or equal to a value x approaches 0 as x approaches negative infinity and approaches 1 as x approaches positive infinity.
Recursive Equation	An equation that defines the relationship between the value of a particular variable and its previous values.
Inherited Characteristics	Traits or features that are passed down from one generation to the next through genetics, such as eye color.
Conditioning on a continuous random variable	In probability theory, conditioning on a continuous random variable involves treating the variable as a constant in the calculation of conditional probability density functions, despite the fact that the condition has zero probability.
Conditioning on Random Variables in Sequence	Conditioning on random variables in sequence involves calculating the conditional probability distribution of a random variable given a sequence of previous random variables, as shown in Example 3.7.18.
Inverse function	A function that undoes the action of another function, such that the composition of the two functions is the identity function.
Marginal p.f., p.d.f., or p.f./p.d.f.	The probability density function (p.d.f.) of a marginal distribution of a subset of random variables, describing the probability of each possible value of those variables in isolation.
Marginal Bivariate Probability Density Function (p.d.f.)	The probability density function that combines the probability density functions of two or more variables.
Likelihood	The probability of observing a set of data given a particular hypothesis or event. It is used in Bayes' theorem as a measure of how well the data supports the hypothesis.
Marginal p.d.f.	The probability density function of a single random variable, obtained by integrating the joint p.d.f. over the other variables.
Theorem 3.4.3	A theorem stating that a joint p.d.f. must satisfy two conditions: f(x,y) ≥ 0 for all x and y, and the integral of f(x,y) over the entire xy-plane is equal to 1. Any function that satisfies these conditions is the joint p.d.f. for some probability distribution.
Transition Matrix	A square matrix in a finite Markov chain with stationary transition distributions, where elements pij represent the probability of transitioning from state i to state j.
Service Times in a Queue	A scenario where n=5 service times in a queue are discussed, and the marginal bivariate p.d.f. of (X1, X4) is integrated over x2, x3, and x5.
Probability of Simultaneous Events	The probability that two or more events will occur simultaneously.
CDF (Cumulative Distribution Function)	The cumulative distribution function (CDF) is a function that describes the probability that a random variable takes on a value less than or equal to a given value. It is defined as the probability that the random variable X takes on a value less than or equal to x, denoted as F(x) = P(X ≤ x).
Joint Probability Density Function (JPDF)	A function that describes the probability distribution of two or more random variables, often depicted as f(x,y)dxdy, where x and y are the random variables.
Probability Function (p.f.)	A function that specifies the probability that a random variable takes each of the different possible values.
Collector's Problem	A problem where a collector aims to obtain a complete set of r different items, and each item is contained in a package with probability 1/r, and the packages are filled independently of each other.
Joint Probability Distribution Function (p.d.f.)	A joint probability distribution function (p.d.f.) is a function that describes the probability distribution of two or more random variables, typically represented as a function of multiple variables.
Joint Probability Mass Function (jp.m.f.)	Not provided in the text.
What is the definition of probability for a pair of random variables?	<img src="page_72_figure.png">
Joint Probability Density Function (p.f.)	A function f(x,y) that specifies the probability of a particular combination of values x and y of random variables X and Y.
Support of the Distribution	The interval where the probability density function (p.d.f.) is greater than zero, which is the entire interval [a,b] in the case of a uniform distribution.
Theorem 3.3.2	A mathematical statement that establishes the probability of a random variable X falling within a given interval [x1, x2] is equal to the difference between the cumulative distribution function at x2 and the cumulative distribution function at x1.
Posterior Probability	A posterior probability is the probability of an event or hypothesis after considering additional evidence or data. It represents the updated belief or confidence in the event or hypothesis based on the new information.
Transition Matrix (P)	A matrix giving the probabilities of transition from the state indexing the row to the state indexing the column.
Marginal Probability Mass Function (p.m.f.)	A marginal probability mass function (p.m.f.) is a function that describes the probability distribution of a discrete random variable, disregarding the values of other random variables. It is derived from a joint p.m.f. by summing over the other variables.
Conditional Discrete Distribution	The probability distribution of a discrete random variable Y given a specific value of random variable X, denoted as g2(y|x).
Binomial probability function	The conditional probability function of X given P = p, which is g1(x | p) = x/parenrightbiggpx(1−p)^n−x for x=0,...,n.
Bivariate joint p.d.f. of Y1 and Yn	The probability density function of the minimum and maximum values of a random sample of size n from a distribution.
Bernoulli distribution	A Bernoulli distribution is a probability distribution that takes on only two values, 0 and 1, with a parameter p that determines the probability of each value, where Pr(Z = 1) = p.
Demand for Water	The demand for water is a random variable that takes values in the interval [4, 200].
Random Variables with a Discrete Joint Distribution	A concept in probability theory that deals with the distribution of a function of two or more random variables when the joint distribution of the random variables is discrete.
Event Tree	A graphical representation of all possible outcomes of a random experiment or set of experiments.
Lower Quartile	The 1/4 quantile or 25th percentile of a distribution, which represents the value below which 25% of the data lies.
Conditional p.f., p.d.f., or p.f./p.d.f.	The conditional probability distribution function (p.d.f.) of a random vector given another random vector, defined as the ratio of the joint p.d.f. to the marginal p.d.f. of the conditioning vector.
Beta-binomial distribution	The distribution described by the integral in equation (4.7.10), which represents the marginal p.f. of X, can be expressed in a simple formula using the Beta function, as shown in equation (4.7.11).
k-Equations	A set of k linear equations that can be used to solve for the values of a variable or variables in a recursive equation.
Joint Probability Density Function (PDF)	A joint probability density function is a function that describes the probability of different outcomes in a joint random experiment, where the outcomes are represented by multiple random variables.
Conditional Probability	The updated probability of an event A after learning that event B has occurred, denoted as Pr(A|B), is the probability of both A and B occurring, Pr(A∩B), divided by the probability of event B, Pr(B).
Probability Mass Function (p.f.)	A function that describes the probability distribution of a discrete random variable, often represented as f(x).
Random Points	Random points are points in space that are chosen at random according to a specific probability distribution.
State 1, State 2, State 3, State 4	Different stages or conditions in a Markov chain, represented by the numbers 1, 2, 3, and 4.
What is the relationship between the mean of a portfolio's return and its possible variances?	<img src="page_177_figure.png">
Sample Space	The set of all possible outcomes or results of an experiment.
Prisoner A’s reasoning	A thought process employed by Prisoner A (A) and other individuals to make decisions based on probability and uncertainty. In this scenario, A's reasoning is concerned with the probability of winning an initial fortune and the best strategy to employ in a game against another player (B).
Moment	A moment is the expected value of a random variable raised to a power, which is a way to describe the distribution of the variable.
Condition (b)	The probability that gambler A will win the initial fortune of gambler B before he loses his own initial fortune.
Continuous p.d.f.	A p.d.f. that is continuous over the entire real line, meaning it has no gaps or discontinuities.
Mutually Exclusive Events	Mutually exclusive events are events that cannot occur simultaneously. The occurrence of one event precludes the occurrence of the other event.
Median of a Distribution	A value of a random variable that divides the probability distribution into two equal parts, with 50% of the data points below and 50% above.
Linear Transformation	A linear transformation is a transformation of random variables that can be expressed as a linear combination of the original variables, and is often used to simplify or change the distribution of a random variable.
Joint Cumulative Distribution Function (Joint c.d.f.)	The joint distribution function or joint cumulative distribution function (joint c.d.f.) of two random variables X and Y, defined as the function F such that F(x,y) = Pr(X ≤ x and Y ≤ y) for all values of x and y (−∞ < x < ∞ and −∞ < y < ∞).
Inverse	In a matrix, the inverse is a matrix that, when multiplied by the original matrix, yields the identity matrix.
What is the probability of an event occurring when given prior knowledge of another event's occurrence?	<img src="page_2_figure.png">
Nondecreasing	The cumulative distribution function F(x) is nondecreasing as x increases, meaning that if x1 < x2, then F(x1) ≤ F(x2).
Dependent Random Variables	Dependent random variables are random variables X and Y that do not maintain a fixed probability distribution independently. Instead, the probability distribution of one variable is influenced by the value of the other variable.
Distribution	A collection of all probabilities Pr(X ∈ C) for all subsets C of the real numbers such that {X ∈ C} is an event.
Riemann integral	A concept used in calculus to define the integral of a function, which is used to calculate probabilities in probability theory.
Mixed Distributions	Distributions that combine both discrete and continuous random variables.
Chapman-Kolmogorov Equations	The equations that describe the evolution of the probability distribution of a Markov chain over time, and can be used to compute the conditional probability of a state at a future time given the current state.
i.i.d.	A set of random variables are independent and identically distributed (i.i.d.) if they are independent and have the same probability density function (PDF) or cumulative distribution function (CDF).
Quantile	The smallest value x such that F(x) ≥ p, where F is the cumulative distribution function (c.d.f.) of a random variable X and p is a probability between 0 and 1, also known as the p-quantile or 100p percentile of X.
Binomial Probability	The probability of exactly k successes in n Bernoulli trials, where the probability of success is p.
Mean of General Discrete Random Variable	The mean, expectation, or expected value of a discrete random variable X with probability function f exists and is defined as E(X) = sum over all possible values x of f(x), provided at least one of the sums of positive or negative values of x times their probabilities is finite.
Correlation Coefficient (ρ)	A statistical measure that assesses the extent to which two random variables are linearly related, ranging from -1 (perfect negative correlation) to 1 (perfect positive correlation) to 0 (no correlation).
Independent and Identically Distributed (i.i.d.)	A sequence of random variables that are independent and have the same probability distribution.
Deriving C.D.F. of a New Random Variable	A method for finding the cumulative distribution function (c.d.f.) of a new random variable by integrating the probability density function (p.d.f.) of the original random variable within a specific range, given a transformation r(X).
Distributions	A description of the probability distribution of a random variable, describing the likelihood of different values.
Conditional probability density function (p.d.f.)	The conditional probability density function of a continuous random variable given a specific value of another continuous or mixed random variable.
Linearly Related	Two random variables are precisely linearly related if their correlation coefficient is equal to 1.
Toothpaste	A type of personal care product used for cleaning teeth.
Independent random variables	Random variables that satisfy the condition that their joint PDF can be factored as the product of their individual PDFs.
Value-at-Risk (VaR)	A measure of the potential loss in a portfolio over a specific time horizon with a given probability, usually denoted as 1 - confidence level, where the confidence level is typically expressed as a decimal from 0 to 1.
Present Value of E(Y)	The value of money that an investor would have after a certain period without buying an option, which is equal to the expected value of the option.
Conditionally Independent Variables	Variables that are independent of each other given the value of one or more other variables.
Joint Marginal Distribution	A distribution that combines the information from n random variables X1, …, Xn, allowing the calculation of marginal distributions for each variable.
Law of the Unconscious Statistician	A theorem in probability theory that states that the expectation of a function of a random variable can be calculated directly as the integral or sum of the product of each value of the function and its corresponding probability density or probability mass.
Joint probability density function (PDF)	A function that describes the probability distribution of two or more random variables, representing the probability of different combinations of values for the variables.
Joint p.f./p.d.f.	A function that represents the probability distribution of multiple random variables, some of which have continuous distributions and some of which have discrete distributions.
Joint Probability Density Function (JPD)	The joint probability density function (JPD) g(y 1, ..., y n) of n random variables Y 1, ..., Y n is a function that describes the probability density of the random vector (Y 1, ..., Y n) over the domain T. It is defined as the absolute value of the determinant J, where J is the Jacobian of the transformation specified by the equations connecting the variables Y 1, ..., Y n, multiplied by the joint probability density function f(x 1, ..., x n) of the original variables X 1, ..., X n.
Conditional Probability Density Function (p.f.)	A function g1(x |y) that specifies the probability of a particular value x of random variable X given that random variable Y has taken on a particular value y.
Meaning of Independence	Independent events mean that the occurrence or non-occurrence of one event does not affect the probability of the occurrence or non-occurrence of another event.
Differentiable One-To-One Functions	A function r(x) that is differentiable and one-to-one on an open interval (a, b), which means it is either strictly increasing or strictly decreasing, and its inverse exists on the image interval (α, β).
How does the variance of a sample proportion change as the sample size increases, while keeping the population proportion constant?	<img src="page_178_figure.png">
Term: Joint Cumulative Distribution Function (Joint c.d.f.)	A joint cumulative distribution function (joint c.d.f.) F(x, y) is a function that describes the probability that a pair of random variables (X, Y) lies within a given region of the xy-plane. It is defined as the probability that X≤x and Y≤y, and it can be used to derive the cumulative distribution functions (c.d.f.s) of the individual random variables, F1(x) and F2(y).
Stationary Transition Distributions	In a Markov chain, the distribution of states at a given time, with all time steps being treated equally.
Bernoulli Trials	A sequence of independent and identically distributed trials, where each trial has only two outcomes, such as success or failure, heads or tails, or defective or non-defective.
One-to-One Function	A function that maps each element in the input set to a unique element in the output set, with each output element corresponding to exactly one input element.
Stationary Transition Probabilities	The probabilities of transitioning from one state to another in a Markov chain remain constant over time.
Determinant	A determinant is a scalar value that can be calculated from the elements of a square matrix.
Theorem	A mathematical statement that has been proven to be true, often used to describe the properties of a given concept or relationship.
Marginal Joint Distribution	The distribution of all possible combinations of values for multiple random variables, without conditioning on any specific values.
Singular Matrix	A square matrix whose determinant is zero, meaning that it does not have an inverse, leading to an undetermined solution for systems of linear equations.
