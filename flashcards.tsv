What is the concept that refers to the events that are part of both event A and event B?	Image placeholder
What are the intersections of set A with events B1 to B5 in a partition of a proof?	Image placeholder
Conditional Probability	The updated probability of event A after learning that event B has occurred, calculated as the probability of the intersection of events A and B relative to the sample space of B, which represents the set of outcomes in B that also result in the occurrence of A.
Conditional Probability	A probability measure that represents the likelihood of an event A occurring given that another event B has occurred, denoted by Pr(A|B). It is calculated as the proportion of the total probability Pr(B) that is represented by the intersection of events A and B, Pr(A∩B).
Conditional Probability	The probability of an event occurring given that another event has occurred, written as P(A|B) = P(A∩B) / P(B).
Conditional Probability	The probability of an event A given that another event B has occurred, denoted as Pr(A|B), is the chance of A happening when B has already occurred.
Multiplication Rule for Conditional Probabilities	A rule that allows the calculation of the probability of two events occurring together by multiplying the probability of one event by the conditional probability of the other event given the first event has occurred.
Conditional Probability	The probability of an event A occurring given that another event B has occurred, written as Pr(A|B), which is equal to the product of conditional probabilities Pr(A|B)Pr(A|B)Pr(A|B)Pr(A|B) up to Pr(A|B), where each probability is conditional on the previous event.
Conditional Probability	Conditional probability is the probability of an event A given that another event B has occurred, denoted by Pr(A|B). It can be calculated using the law of total probability, which states that Pr(A) = Σ Pr(Bj) Pr(A|Bj) for a partition of the sample space S, where B1, ..., Bk are disjoint and non-overlapping events.
Law of Total Probability	The law of total probability is a theorem that states that the probability of an event A can be calculated by summing the product of the probability of each event B in a partition of the sample space and the conditional probability of event A given event B. It is expressed mathematically as Pr(A) = Σ Pr(Bj) Pr(A|Bj), where B1, ..., Bk are disjoint and non-overlapping events.
Conditional Probability	The probability of an event occurring given that another event has occurred, denoted as Pr(A | B) and calculated as the probability of both events occurring divided by the probability of the given event.
Random Composition	The unknown distribution of properties or outcomes in a population or collection, which can be represented by a set of possible compositions or proportions.
Bayes' Theorem	A statistical theorem that calculates the probability of an event based on prior knowledge or assumptions, in this case, the proportion of successes among patients.
Augmented Experiment	An experiment can be augmented to include potential or hypothetical observations of additional information to help calculate probabilities, allowing for the inclusion of outcomes that determine desired quantities.
Conditional Probability	(not present in this chunk, as it is mentioned in the context of previous examples)
Conditional Probability	The probability of an event occurring given that another event has occurred.
Conditional Probability	The revised probability of an event A after learning that event B (with Pr(B) > 0) has occurred, denoted by Pr(A | B), computed as Pr(A ∩ B) / Pr(B), which represents the probability of A given B.
Conditional Probability	The probability of an event occurring given that another event has occurred, which is calculated by dividing the probability of the intersection of the two events by the probability of the other event.
Conditional Probability	The probability of an event occurring given that another event has occurred. It is calculated as the probability of the intersection of the two events (A ∩ B) divided by the probability of the conditioning event (D).
Uniform Coin	A coin where the occurrence of heads or tails is equally likely, with a probability of 0.5 for each outcome. In the given problem, the presence of two fair coins and three coins with a head on each side introduces a non-uniform probability distribution for a randomly selected coin. However, the probability of getting a head is still higher than the probability of getting a tail.
What is the event that occurs in both event A and event B?	Image placeholder
What is the representation of the intersection of a set and various events in a proof of a mathematical theorem?	Image placeholder
Conditional Probability	The updated probability of an event A after observing that event B has occurred, which is calculated by considering the set of outcomes that are included in both events A and B.
Conditional Probability	The probability of an event A given that another event B has occurred, denoted as Pr(A|B). It is computed as the proportion of the total probability of B that is represented by the probability of A and B occurring together, Pr(A∩B).
Conditional Probability	The probability of occurrence of an event A, given that event B has already occurred.
Multiplication Rule for Conditional Probabilities	A rule that states that the probability of two events occurring is the product of the probability of one event and the conditional probability of the second event given the first event.
Rolling of Dice	A simple experiment in which two dice are rolled until the sum reaches either 7 or 8.
Event	A set of outcomes of an experiment that can be identified and measured.
Sample Space	The set of all possible outcomes of an experiment.
Probability	A measure of the likelihood of an event occurring, usually represented as a number between 0 and 1.
Conditional Probability	The probability of an event occurring given that another event has occurred, denoted as P(A|B).
Partition	A collection of events B1, ..., Bk in a sample space S such that the events are disjoint and their union covers the entire sample space, i.e., k i=1 Bi = S.
Law of Total Probability	A theorem stating that for every event A in a sample space S, if the events B1, ..., Bk form a partition of S and Pr(Bj) > 0 for j = 1, ..., k, then Pr(A) = Σ j=1 k Pr(Bj)Pr(A|Bj).
Conditional Probability	The probability of an event A occurring given that another event B has occurred, denoted as Pr(A|B).
Sample Space	The set of all possible outcomes or results of an experiment or random phenomenon.
Event	A set of outcomes or results of an experiment or random phenomenon that satisfy a certain condition or property.
Bayesian inference	The process of using Bayes' theorem to update the probability of a hypothesis or a parameter based on new data or evidence.
Prior distribution	The initial probability distribution assigned to a parameter or hypothesis before observing any data.
Sample space	The set of all possible outcomes or events in a probability experiment or study.
Event	A set of outcomes or a happening in a probability experiment or study.
Conditional probability	The probability of an event occurring given that another event has occurred.
Bayes' theorem	A mathematical formula used to update the probability of a hypothesis or a parameter based on new data or evidence.
Mixture distribution	A probability distribution that represents the combination of two or more underlying distributions or populations.
Hypothesis	A statement or assumption made about a population or phenomenon that is tested or verified through experimentation or observation.
Parameter	A numerical value or set of values that characterizes a probability distribution or a model.
Augmented Experiment	An experiment that includes the potential or hypothetical observation of additional information to calculate probabilities, even if it is not actually observed.
Conditional Probability	The probability of an event given that another event has occurred. In this context, it is assumed that the experiment includes outcomes that determine the values of quantities such as p.
Augmented Experiment	An experiment that includes a portion of the sample space that would be observed if the experiment were to continue indefinitely.
Conditional Probability	The revised probability of an event A after learning that event B (with Pr(B) > 0) has occurred, denoted by Pr(A|B) and computed as Pr(A ∩ B) / Pr(B).
Multiplication Rule for Conditional Probabilities	A formula for computing Pr(A ∩ B) as Pr(B) × Pr(A | B), often used when it is easy to assess the conditional probability Pr(A | B) directly.
Partition	A collection of disjoint events whose union is the entire sample space, chosen to reduce uncertainty by learning which one of the partition events occurs.
Law of Total Probability	A rule for combining conditional probabilities of an event A given each event in a partition to get the overall probability Pr(A).
Independent Events	Events A and B are independent if the probability of A occurring is not affected by whether or not B occurs, and vice versa.
Conditional Independence	Events A and B are conditionally independent given event C if Pr(A | C) = Pr(A) and Pr(B | C) = Pr(B) whenever Pr(C) > 0.
Probability of the fourth ball being blue (Exercise 5)	The probability that the first three balls will be red and the fourth ball will be blue in the scenario where a box contains r red balls and b blue balls, one ball is selected at random, returned to the box, and k additional balls of the same color are put in, and the process is repeated until four balls are selected.
Conditional Probability	The probability of an event given that another event has occurred.
Independent Events	Events that occur independently of each other, where the occurrence of one event does not affect the probability of the other event.
Random Sampling	Selecting an item or an event from a population or sample space, where each item or event has an equal chance of being selected.
What are the outcomes that both belong to an event A and event B, also known as the intersection of A and B?	Image placeholder
What are the key events in a partition that intersect with the set A in a given proof?	Image placeholder
Conditional Probability	The updated probability of an event A after learning that event B has occurred, often denoted as P(A|B), where B is the event that has been observed.
Definition of Conditional Probability	The conditional probability of event A given event B is the probability of A occurring when event B has already occurred.
Bayes' Theorem	(Note: This term does not have a clear definition in this context, but it is likely a reference to Bayes' theorem relating conditional probability to probability updating.)
Gambler's Ruin Problem	(Note: This term does not have a clear definition in this context, but it is likely a reference to a problem in probability theory related to a gambler's financial ruin.)
Probability of the event given that another event has occurred (Pr(A|B))	The probability of event A occurring given that event B has already occurred, calculated as the proportion of total probability Pr(B) represented by the probability of both events occurring together, Pr(A ∩ B).
Placebo	A treatment that is supposed to be neither helpful nor harmful; given to patients to serve as a control group in clinical trials.
Event	A set of outcomes or occurrences that can happen in a probability space.
Random Variable	A variable whose possible values are the outcomes of a random experiment or process.
Probability Space	A set of outcomes or events, and a set of probabilities assigned to each event, which together define a random experiment or process.
Multiplication Rule for Conditional Probabilities	A mathematical rule that states that the probability of a conjunction of two events (A and B) is equal to the probability of event B times the conditional probability of event A given that event B has occurred.
Bayes' Formula	A formula that expresses the probability of an event occurring given that another event has occurred, using the concept of conditional probability.
Total Probability Formula	A formula that expresses the probability of an event occurring, using the concept of conditional probability and the product of probabilities of successive conditional events.
Multiplication Rule	A rule that states that the probability of a sequence of events occurring is equal to the product of the probabilities of each event occurring, given that the previous events have occurred.
Conditional Version of the Multiplication Rule	A version of the multiplication rule that is applicable when the probability of an event is conditioned on the occurrence of another event.
Partition	A set of pairwise disjoint events B1, ..., Bk in a sample space S such that union of B1, ..., Bk is equal to S.
Law of Total Probability	A theorem stating that the probability of an event A in a sample space S can be calculated by summing the products of the probabilities of each event Bi in a partition of S and the conditionals of A given Bi, provided that Pr(Bj) > 0 for all j.
Bayesian estimation	The process of estimating the proportion of successes among all patients who might receive a treatment, using prior probabilities and updating them based on the observed data.
Augmented Experiment	An experiment that is extended to include the potential or hypothetical observation of additional information to help calculate probabilities.
Augmented Experiment (Explicit Application)	The explicit assumption that there exists an infinite sequence of patients who could be treated with imipramine, allowing the sample space to consist of infinite sequences of symbols, where each coordinate represents a patient's outcome (success or failure).
Augmentation	The expansion or modification of an experiment to include additional information or possibilities, even if it is not explicitly mentioned or observable.
Experiment	A procedure or process that involves uncertainty and produces a set of outcomes, often used to measure or determine probability.
Independent Events	Events that do not influence each other's probability of occurrence, meaning that the probability of one event does not change based on the occurrence of another event.
Probability	A measure of the likelihood or chance of an event occurring, often expressed as a number between 0 (impossible) and 1 (certain).
Sampling	The process of selecting a subset of outcomes from a larger set of possible outcomes, often used to describe the selection of elements from a probability space.
Bayes' Law	A formula used to compute the conditional probability of an event A given another event B, by rearranging the definition of conditional probability.
Probability of Complement	The probability of an event not occurring, represented as P(A') or P(~A), is the complement of the probability of the event occurring.
What is the concept that describes the likelihood of an outcome occurring given that another specific event has occurred?	Image placeholder
What does the intersection of set A with events B1 to B5 represent in a proof of Theorem 2.1.4?	Image placeholder
Conditional Probability	The updated probability of an event A after learning that event B has occurred, considering the event B has already occurred.
Conditional Probability	The probability of an event A occurring given that event B has already occurred, denoted as Pr(A|B). It is calculated as the ratio of the probability of both events A and B occurring to the probability of event B occurring: Pr(A|B) = Pr(A ∩ B) / Pr(B).
Conditional Probability	The probability of an event occurring given that another event has occurred, denoted by Pr(A|B), which is calculated as the probability of the intersection of the two events divided by the probability of the second event.
Event	A set of outcomes of a random experiment, such as the event that the sum of two dice is 3, 5, or 7.
Intersect	The intersection of two events, denoted by A∩B, is the set of outcomes that are common to both events.
Probability	A measure of the likelihood of an event occurring, typically represented as a number between 0 and 1.
Random Experiment	An action or process that is subject to chance and has a set of possible outcomes, such as rolling two dice.
Conditional Probability	The probability of the intersection of two events, given that one of the events has occurred.
Multiplication Rule for Conditional Probabilities	A theorem stating that the probability of the intersection of two events is equal to the probability of one event multiplied by the conditional probability of the other event, given the first event has occurred.
Events	Sets of outcomes or results of an experiment.
Conditional Events	Events that occur given that another event has occurred.
Sample Space	The set of all possible outcomes of an experiment.
Independent Events	Events that occur independently of each other, without affecting the probability of the other event.
Conditional Probability	The probability of an event occurring given that another event has occurred, denoted as P(A|B) and read as "the probability of A given B".
Conditional Probability	The conditional probability of an event A given an event B, denoted as Pr(A|B), is defined as the probability of A occurring given that B has occurred, and is calculated as the ratio of the probability of the intersection of A and B to the probability of B.
Conditional Probability	The probability of an event occurring given that another event has occurred, denoted by Pr(A|B) and defined as the ratio of the probability of the intersection of events A and B to the probability of event B.
Law of Total Probability	A fundamental property of probability that states the total probability of an event is the sum of the probabilities of the event occurring under different conditions, denoted by Pr(A) and defined as the sum of Pr(Bi) \* Pr(A|Bi) over all possible Bi.
Augmented Experiment	A concept that suggests that an experiment can be modified or expanded to facilitate the calculation of probabilities by introducing additional structure or assuming a partition of the experiment.
Example 2.1.10	A problem that involves a game where a person plays until they get a score Y that is greater than or equal to their initial score X, and calculates the probability that the score Y is 50.
Example 2.1.11	A problem that involves selecting bolts from a box that may have one of two possible compositions, and calculates the probability of selecting a long bolt given the different compositions.
Composition Uncertainty	The uncertainty about the proportion of successes or failures in a collection of patients or entities, which affects the probability of success for each individual in the sample.
Sample Space Partitioning	The process of dividing the sample space into smaller parts or events, in this case, to account for the different possible compositions of the collection of patients.
Conditional Probability	The probability of an event occurring given that another event has occurred, in this case, Pr(E1|Bj) is the probability of the first patient in the imipramine group having a success, given that the sample was drawn from a collection with a proportion of (j-1)/10 successes.
Prior Beliefs	The initial beliefs or assumptions about the probability of each possible composition of the collection of patients, in this case, Pr(Bj) = 1/11 for each j.
Augmented Experiment	An experiment that can be extended to include the potential or hypothetical observation of any additional information that would be useful to help calculate desired probabilities.
Sample Space	The set of all possible outcomes or combinations of outcomes in an experiment.
Conditional Probability	Not explicitly defined in this text, but implies the probability of an event given the occurrence or non-occurrence of another event.
Experiment	A process or procedure that generates outcomes or data, which can be used to calculate probabilities.
Proportion of Successes	The ratio of number of successes (e.g., patients responding positively) to the total number of observations (e.g., patients) in an experiment.
Augmented Experiment	An experiment that is not explicitly designed to determine certain quantities, but can still be assumed to determine those quantities, implying that the experiment is extended or augmented in some way.
Conditional Probability	The probability of an event occurring given that another event has occurred, often denoted as P(A|B) and read as "the probability of A given B".
Event	A set of outcomes or a set of occurrences that define a specific situation or result, often denoted as E or A.
Probability	A measure of the likelihood or chance of an event occurring, typically denoted as P(A) and ranging from 0 to 1.
Samplespace	The set of all possible outcomes or results in an experiment, often denoted as S.
Theorem	A mathematical statement that has been proven to be true, often used to describe a mathematical principle or formula.
Conditional Probability	The probability of an event A occurring given that event B has occurred, denoted by Pr(A|B) and computed as Pr(A∩B)/Pr(B).
Multiplication Rule for Conditional Probabilities	A formula used to compute Pr(A∩B) = Pr(B) Pr(A|B) when assessing a conditional probability directly.
Partition	A collection of disjoint events whose union is the sample space. A partition is chosen to reduce important uncertainty if we learn which one of the partition events occurs.
Law of Total Probability	A rule that combines conditional probabilities to obtain Pr(A), given that conditional probabilities are available for each event in a partition.
Independent Events	Events A and B are independent if Pr(A|B) = Pr(A) and Pr(B|A) = Pr(B).
Independent Events	Two events are said to be independent if the occurrence of one event does not affect the probability of the other event.
Conditional Probability	The conditional probability of an event A given another event B is the probability of A occurring given that B has already occurred.
Events	An event is a set of outcomes of an experiment or situation, and is typically denoted by a variable or variable expression.
Random Variable	A random variable is a variable that takes on different values depending on the outcome of an experiment or situation, and is often denoted by a letter such as X or Y.
Random Sample	A random sample is a subset of data selected from a larger population, with each individual or item having an equal chance of being selected.
Probability	The probability of an event is a measure of the likelihood of the event occurring, expressed as a value between 0 and 1.
Coin Flip	A coin flip is a simple statistical experiment in which a coin is tossed and the outcome is either heads or tails.
Binomial Distribution	The binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent trials, each with a constant probability of success.
Random Experiment	A random experiment is an action or situation that results in a range of possible outcomes, where the outcome(s) are uncertain until the experiment is conducted.
Conditional Probability (Pr(A ∩ B|D))	The probability of the occurrence of an event A under the condition that event B has occurred and event D is true. In this case, Pr(A ∩ B|D) is the probability of obtaining a head given that a coin is selected from a box containing three coins with heads on both sides, four coins with tails on both sides, and two fair coins.
What is the event that occurs when the conditions of event A are met and also satisfies the criteria of event B?	Image placeholder
What is the significance of the intersections between set A and events B1 to B5 in a proof?	Image placeholder
What is shown in this figure?	Image placeholder
What type of partition elements exhibit the highest probability after observing A?	Image placeholder
What are the posterior probabilities of partition elements in a binary tree, according to the Bayes' theorem, after considering a certain number of patients?	Image placeholder
What is the significance of Example 3.1.3 in the context of utility demands?	Image placeholder
What is the event that occurs when water demand is between 50 and 175?	Image placeholder
What is a polynomial function representing the pattern of x values and the corresponding f(x) values?	Image placeholder
What happens when trying to apply the concept of all possible values to the probability density function (p.d.f.) of a continuous random variable?	Image placeholder
What is a probability distribution that is uniform over a specific interval?	Image placeholder
What is the purpose of the graph showing the cumulative distribution function (c.d.f.)?	Image placeholder
What does this p.d.f. graph represent?	Image placeholder
What is the common method used to determine a quantile from a uniform distribution?	Image placeholder
What relationship exists between a quantile and the cumulative distribution function (c.d.f.) of a random variable X?	Image placeholder
What is the cumulative distribution function for a continuous uniform random variable between 0 and 1?	Image placeholder
What is the joint probability function of two random variables represented by a table showing their probabilities?	Image placeholder
What is an example of a joint probability density function (p.d.f.) used to represent the relationship between two random variables?	Image placeholder
What is a support set in a graph?	Image placeholder
What is the concept of a subset of a support set where an inequality is fulfilled?	Image placeholder
What is the concept of probability in relation to geometric shapes?	Image placeholder
What are marginal cumulative distribution functions, probability functions, or probability densities, and how do they relate to joint probability functions?	Image placeholder
What is the set S on the graph that represents the points where the function f(x, y) is greater than zero?	Image placeholder
What is the marginal probability density function of a random variable?	Image placeholder
What is the marginal probability density function of X?	Image placeholder
What is the region represented by the boundary of a specific equation in a visual representation of a problem scenario?	Image placeholder
What is the relationship between the conditional probability density function and the joint probability density function in a particular scenario?	Image placeholder
What is the probability density function of a random variable in a specific example?	Image placeholder
What is the shape of the distribution of service times in a system with 100 observed service times?	Image placeholder
What type of statistical distribution might a histogram of service times represent, given its shape and parameters?	Image placeholder
What is the probability distribution function (p.d.f.) of the squared value of a random variable X that has a uniform distribution?	Image placeholder
What is the set Ay in mathematical contexts?	Image placeholder
What region is shaded in a specific boundary problem, requiring a specific integration?	Image placeholder
What region does the function in Eq. (3.9.7) depict?	Image placeholder
What is the probability distribution of the minimum and maximum values in a random sample of size n from a Uniform(0, 1) distribution?	Image placeholder
What type of probability distribution is illustrated, and what is the key characteristic of the distribution shown?	Image placeholder
What is the purpose of finding the set T in a set S, given the information about the Cartesian product (x1, x2) ∈ S?	Image placeholder
What process involves selecting and crossing plant species to create new, beneficial crop varieties?	Image placeholder
What are the possible offspring that can result from a combination of different alleles being inherited?	Image placeholder
What is the middle value that represents the average of a set of discrete or continuous data points?	Image placeholder
What concept in statistics represents the balance point of a continuous distribution?	Image placeholder
What is the probability density function (p.d.f.) of the Cauchy distribution?	Image placeholder
What is the probability distribution represented in a graph showing the relationship between values of x and f(x)?	Image placeholder
What are two illustrations of uniform distributions with the same mean but different spreads?	Image placeholder
What is the relationship between the probability density function (p.d.f.) of a random variable X and its transformed versions?	Image placeholder
What happens to the probability distribution of a random variable when it is shifted?	Image placeholder
What happens to the probability distribution function of a random variable when the variable is reflected, and how does this impact the distribution's properties?	Image placeholder
What is represented by the graph showing means and variances of investment portfolios?	Image placeholder
What is depicted in this plot of mean and variance, and what are the possible combinations being shown?	Image placeholder
What is the effect of varying variances on binomial distributions with the same mean, as illustrated by two distributions with different levels of variance?	Image placeholder
What are the key similarities and differences between two binomial distributions with the same mean but different variances?	Image placeholder
When should a fire engine wait along a road to minimize the expected impact of its response time, considering the uncertainty of the situation?	Image placeholder
What is the region where the joint probability density function (p.d.f.) of (X, Y) is constant and nonzero?	Image placeholder
What is the relationship between the mean and variance of efficient portfolios?	Image placeholder
What are the key metrics used to describe efficient investment portfolios?	Image placeholder
What is the conditional probability distribution of P given X = 18, and how does it relate to the prior probability distribution of P?	Image placeholder
Conditional Probability	The updated probability of event A after learning that event B has occurred, considering the set of outcomes in B that also result in the occurrence of A.
Conditional Probability	The probability of an event A given that event B has occurred, denoted as Pr(A | B). If Pr(B) > 0, it is computed as the proportion of the total probability Pr(B) that is represented by Pr(A ∩ B), which intuitively represents the proportion of B that is also part of A.
Conditional Probability	The probability of an event occurring given that another event has occurred.
Event	A set of outcomes of an experiment, typically denoted by uppercase letters (e.g., A, B, C).
Random Variable	A variable whose possible values are determined by the outcomes of an experiment, often represented by uppercase letters (e.g., T).
Probability	A measure of the likelihood of an event occurring, usually represented by a number between 0 and 1.
Placebo	A treatment that is supposed to be neither helpful nor harmful, often used in clinical trials to compare the effects of different treatments.
Recurrence	The re-occurrence of a disease or condition, such as depression.
Clinical Trial	A controlled experiment designed to evaluate the safety and effectiveness of a treatment, often involving human subjects.
Probability Density Function	A function that describes the probability distribution of a random variable, often represented as Pr(X).
Conditional Probability	The probability of an event occurring given that another event has occurred, which is denoted as Pr(A|B) and calculated as Pr(A∩B) / Pr(B).
Multiplication Rule	A rule in probability theory that states the probability of the intersection of multiple events can be calculated as the product of the probabilities of each event given the events that precede it.
Unconditional Probability	The probability of an event occurring without any conditions or given events, which is denoted as Pr(A).
Partition	A set of disjoint events that together cover the entire sample space, where each event is a subset of the sample space.
Law of Total Probability	A theorem stating that the probability of an event can be calculated as the sum of the probabilities of the event occurring given that each of a partition of events occurs, weighted by the probability of each event in the partition.
Conditional Probability	The probability of an event given that another event has occurred.
Conditional Probability	The probability of an event occurring given that another event has occurred, denoted as P(A|B). It is calculated as the probability of the intersection of events A and B divided by the probability of event B.
Law of Total Probability	The law of total probability states that the probability of an event A can be calculated as the sum of the probabilities of A given each possible outcome of a partition, times the probability of each outcome, denoted as ∑[Pr(A|B_i)Pr(B_i)].
Augmented Experiment	An augmented experiment is a hypothetical extension of an original experiment that allows for the calculation of probabilities by introducing a partition or additional structure to the experiment.
Simple Random Sample	A random sample selected from a population in which every individual in the population has an equal chance of being included in the sample, and where the probability of selection is independent of any variable of interest.
Augmented Experiment	An experiment that can be extended to include potential or hypothetical observations of additional information that would be useful in calculating probabilities.
Conditional Probability	No definition provided in this text snippet. (Please note that the term "Conditional Probability" is mentioned in the context of Example 2.1.11 and Example 2.1.12, but a specific definition is not given.)
Event	A specific occurrence or outcome of a random experiment, such as the roll of a die or the drawing of a card.
Probability	A measure of the likelihood or chance of an event occurring, typically represented as a numerical value between 0 and 1.
Conditional Probability	The probability of an event occurring given that another event has occurred, denoted as Pr(A|B), where A is the target event and B is the conditioning event.
Augmented Experiment	An experiment that is expanded or modified to include additional information or variables, often used to determine previously unknown quantities.
Game of Craps	A popular gambling game involving the roll of dice, where a player can win or lose depending on the sum of the numbers rolled.
Sample Space	The set of all possible outcomes or results of a random experiment, often denoted as S.
Event Space	A set of events or outcomes of a random experiment, often denoted as E or F.
Partition	The division of a set or space into non-overlapping subsets or partitions, often used in probability theory and statistics.
Conditional Probability	The probability of an event A after learning that event B (with Pr(B) > 0) has occurred, denoted by Pr(A | B), and computed as Pr(A ∩ B) / Pr(B).
Bayes' Theorem	A mathematical formula used to update the probability of a hypothesis based on new information or evidence, given the probability of that evidence given the hypothesis.
Independent Events	Events A and B are independent if learning that B has occurred does not change the probability of A. Similarly, A and B are conditionally independent given C if A and B are independent after conditioning on C.
Conditional Probability	The probability of an event A occurring given that event B has occurred, denoted as Pr(A|B), is defined as the probability of the intersection of A and B divided by the probability of B. It represents the revised probability of A after learning that B has occurred.
Independence	Two events A and B are independent if the occurrence of one event has no impact on the probability of the other event. This is represented by the equation Pr(A|B) = Pr(A) and Pr(B|A) = Pr(B), which states that the conditional probability of A given B is equal to the unconditional probability of A, and vice versa.
Independence of two events	Two events A and B are independent if the probability of their intersection is equal to the product of their individual probabilities, i.e., Pr(A ∩ B) = Pr(A)Pr(B).
Independence	The state of two events being unrelated in such a way that the occurrence or non-occurrence of one event does not affect the probability of the other event.
Event	A set of outcomes of an experiment, often represented by a variable or a Boolean expression.
Independent Events	Two events that satisfy the definition of independence, where Pr(A ∪B) = Pr(A) + Pr(B) − Pr(A ∩B).
Complementary Events	Two events where the occurrence of one event makes the other event impossible, and vice versa.
Theorem	A mathematical statement proved to be true by a group of steps.
Sample Space	The set of all possible outcomes or arrangements in a probability experiment or situation.
Defective Item	An item in a sample that has a defect or flaw.
Independent Events	A set of events in which the outcome of one event does not affect the outcome of another event.
Conditional Probability	The probability of an event occurring given that another event has occurred.
Defective Item Probability	The probability that a particular item will be defective, denoted by p.
Non-Defective Item	An item in a sample that does not have a defect or flaw.
Multiple Defects	The occurrence of more than one defective item in a sample of items.
Head or Tail	The outcomes of a coin toss, with "head" referring to the side with the iconic head symbol and "tail" referring to the side with the iconic tail symbol.
Inspecting Items One at a Time	The process of selecting and inspecting items produced by a machine one at a time until a specified number of defectives are obtained, in order to calculate the probability of obtaining exactly n items to reach that number.
Conditional Probability	The probability of an event occurring given that another event has occurred, calculated as the probability of the intersection of the two events divided by the probability of the event that has occurred.
Independence	The state of two events being unrelated, meaning that the probability of one event occurring does not affect the probability of the other event occurring.
People v. Collins	A criminal case in which a mathematician calculated the probability that a randomly selected couple would possess certain characteristics, leading to a conviction and eventual overturning by the Supreme Court of California.
Prosecutor's Fallacy	The mistake of interpreting the probability of a random event as evidence of guilt, rather than considering the probability of the event given the circumstances.
Independent Events	Two events A and B with positive probability are independent if and only if Pr(A | B) = Pr(A).
Conditional Probability	The probability of an event B given that event A has occurred, denoted by Pr(B | A), is equal to the probability of the intersection of events B and A (Pr(B ∩ A)) divided by the probability of event A (Pr(A)).
Mutually Independent	A set of events are mutually independent if the occurrence or non-occurrence of one event does not affect the probability of any other event in the set.
Disjoint Events	Two or more events are disjoint if they cannot occur at the same time.
Union of Events	The union of two or more events A, B, and C is the event that occurs if any of the events A, B, or C occurs.
Independent Events	Events are considered independent if learning that one event occurred does not change the probability of the other event(s).
Conditional Independence	A concept that states that events A1, ..., Ak are independent given an event B, if for every subcollection Ai1, ..., Aij of j events (j = 2, 3, ..., k), the probability of the intersection of these events given B is the product of the conditional probabilities of each event given B.
Probability	The chance or likelihood of an event occurring, typically ranging from 0 (impossible) to 1 (certain).
Independent Events	A collection of events is said to be independent if and only if learning that some of them occur does not change the probabilities that any combination of the rest of them occurs, or, equivalently, if the probability of the intersection of every sub-collection is the product of the individual probabilities.
Three Numbers the Same	The probability that exactly the same number of tosses (or trials) will be required for a fair coin to obtain a head for the first time in three consecutive performances of the experiment.
Independent Feature Inheritance	The characteristic in which different children in a family inherit a feature (in this case, blue eyes) independently of one another, without any influence or correlation between their inheritances.
At Least Three Children with Blue Eyes	The probability that at least three children in a family of five have blue eyes, given that it is known that at least one child has blue eyes.
Conditional Probability with Family Structure	The probability of an event (e.g., at least three children with blue eyes) conditioned on the knowledge of a specific occurrence (e.g., at least one child having blue eyes) within a family structure.
Independent Events	A set of events (A, B, C) where each event is separate and independent of the others, and the probability of each event occurring does not depend on the occurrence or non-occurrence of the other events.
Probability of None or Exactly One Event	The probability that none of the independent events (A, B, C) will occur, or that exactly one of the events will occur, given their respective probabilities.
Permeability of Radioactive Particles	The probability that a given particle emitted by a radioactive material will penetrate a certain shield, and the probability that at least one particle will penetrate the shield.
At Least One Particle Penetration	The probability that at least one particle emitted by a radioactive material will penetrate a certain shield, given the known permeability and number of particles emitted.
Required Particles for Probability Threshold	The minimum number of particles that must be emitted by a radioactive material in order for the probability of at least one particle penetrating the shield to reach or exceed a given threshold (0.8).
World Series Winner	The probability that team A will win the World Series, given the probability of team A winning a particular game against team B (1/3).
Target Hitting Probability	The probability that a target will be hit for the first time on a specific throw (in this case, the third throw of boy A), given the individual probabilities of each boy hitting the target.
Conditional independence	The concept that the probability distribution of a random variable is unchanged when conditioned on two disjoint sets of events.
Bayes' Theorem	A mathematical formula used to update the probability of a hypothesis based on new evidence, given prior knowledge of conditional probabilities.
Probability of at least one color missing	The probability that at least one color (red, white, or blue) will be missing from a random selection of 10 balls from a box containing 20 red, 30 white, and 50 blue balls, with replacement.
Sequence of kind-dependent events	A sequence of events where each event depends on the previous event, but not on any other event in the sequence.
Programmer's expectation of successful compilation	The probability that a programmer expects a program to compile successfully, based on the difficulty of the programming task, with 80% expected for easy tasks and 40% for difficult tasks.
Bayes' Theorem	Bayes' Theorem is a mathematical formula used to update the probability of an event (or hypothesis) given new information or evidence. It is a way to revise the probabilities of events when new data or observations are available, taking into account the prior probabilities and the likelihood of the event given the new information.
Bayes' Theorem	A mathematical formula used to update the probability of an event given new information or evidence. It is used to calculate the conditional probability of an event given another event, and is commonly represented as P(A|B) = P(B|A) \* P(A) / P(B).
Conditional Probability	The probability of an event occurring given that another event has occurred. It is denoted by P(A|B) and is used to calculate the probability of event A occurring given that event B has occurred.
Partition	A set of events that are mutually exclusive and exhaustive, meaning that one and only one event can occur.
Conditional Probability	The probability of an event occurring given that it has already occurred, calculated using Bayes' theorem as the ratio of the likelihood of the event under each possible hypothesis and the prior probability of each hypothesis.
Bayes' Theorem	A mathematical formula that relates prior probabilities to posterior probabilities, used to update our knowledge about a set of events based on new information, and calculate conditional probabilities.
Dominant Allele	An allele (a variant of a gene) that is expressed in an individual even when only one copy of the allele is present, as opposed to a recessive allele which is only expressed when an individual has two copies of the allele.
Recessive Allele	An allele (a variant of a gene) that is only expressed in an individual when they have two copies of the allele, and is overshadowed by a dominant allele when present in combination with it.
Phenotype	The physical or biological characteristic of an organism resulting from the interaction of its genes and the environment, such as hair color, blood type, or eye color.
Genotype	The genetic makeup of an organism, including the specific alleles and their combination at a particular gene or set of genes.
Population Genetics	The study of the dynamics of gene frequencies in populations of living organisms, including the interactions between genes, environment, and selection pressure.
Bayes' Theorem	A mathematical formula used to update the probability of a hypothesis (B) given new data or evidence (A), by calculating the ratio of the prior probability of B to the probability of A given B, divided by the prior probability of A given B summed over all possible values of B.
Bayes' Theorem	A mathematical formula used to calculate the probability of an event conditioned on another event, given the prior probability of the event and the probability of the conditioning event given the event.
Conditionally Independent Events	Events are conditionally independent if the probability of an event occurring given certain conditions is the same regardless of whether earlier events in the sequence have occurred.
Conditional Independence	The concept that the occurrence of one event does not affect the probability of another event, conditional on a third event occurring.
Bayes' Theorem	A mathematical formula that expresses the probability of a hypothesis (event) given some data or evidence, as equal to the posterior probability times the likelihood of the data.
Prior Probability	The probability of an event occurring before any data or evidence is considered.
Posterior Probability	The probability of an event occurring after considering the data or evidence.
Likelihood	The probability of observing a particular set of data given a particular hypothesis (event).
Partition Events	Events that partition a sample space, meaning that they are exhaustive and mutually exclusive.
Bayes' Theorem	A mathematical formula that describes how to update the probability of a hypothesis (Bj) given new evidence (A), by comparing the likelihood of the evidence (Pr(A| Bj)) with the prior probability of the hypothesis (Pr(Bj)).
Conditional Independence	The property of two events (Ei) being independent given another event (Bj), meaning that the probability of Ei given Bj is the same as the probability of Ei.
Law of Total Probability	A mathematical formula that expresses the probability of an event (E) as a sum of conditional probabilities, given different conditions (Bj).
Partition Elements	A set of events (Bj) that partition a larger space, meaning that each event Bj is mutually exclusive and exhaustive, and the union of all Bj covers the entire space.
Prior Probabilities	The set of probabilities assigned to events before the occurrence of an observed event, which can influence the calculation of posterior probabilities.
Posterior Probability	The probability of an event occurring given that new information has been obtained, which updates the prior probability.
Bayes' Theorem	A mathematical formula used to update the probability of an event based on new evidence or information.
Conditional Probability	The probability of an event occurring given that another event has occurred or is known to have occurred.
Machine Learning	The use of algorithms and statistical models to enable machines to learn from data, make decisions, and perform tasks without being explicitly programmed.
Random Sampling	The process of selecting a subset of data or individuals from a larger population in a way that is independent and unbiased, with each member having an equal chance of being selected.
Cancer Detection	The use of medical tests and procedures to diagnose and detect cancer, including the development of new diagnostic tests and the evaluation of their accuracy.
Statistics	The study and application of statistical methods, techniques, and theories to extract insights and knowledge from data and make informed decisions.
Probability Theory	The branch of mathematics that deals with the study of chance events, randomness, and uncertainty, and the use of probability distributions to model and analyze data.
Conditional Probability	The probability of an event occurring given that another event has occurred or is known to have occurred.
Discrete Distribution	A probability distribution that describes the probability of each possible value in a discrete random variable.
Prior probability	The probability of an event before new information or data is observed, which is used as the basis for updating the probability through Bayes' theorem.
Posterior probability	The probability of an event after new information or data is observed, which is updated from the prior probability using Bayes' theorem.
Conditional probability	The probability of an event occurring given that another event has occurred, often represented as P(A|B), where A and B are events.
Conjugate prior	A prior distribution that is updated to a posterior distribution using Bayes' theorem, without requiring re-estimation of model parameters.
Bayes' theorem	A mathematical formula that updates the prior probability of an event to the posterior probability using the observed data and the law of total probability.
Law of total probability	A mathematical formula that expresses the probability of an event as the sum of the probabilities of the event given different possible states or outcomes.
Independent events	Events that occur independently, meaning that the occurrence or non-occurrence of one event does not affect the probability of the other event.
Conditional independence	The property that two events are independent given the occurrence of a third event or set of events.
Posterior probability	The probability of an event given the occurrence of another event or new information, calculated using Bayes' theorem.
Gambler's Ruin Problem	The problem of determining the probability that a gambler's fortune will reach a specified amount (k dollars) before it reaches zero, assuming the gambler continues to play until they win the specified amount or go broke, whichever comes first.
Gambler's Ruin Problem	A problem in probability theory that involves a gambler who starts with an initial amount of money and repeatedly plays a fair or unfair game against another gambler until one of them runs out of money.
Gambler's Ruin Problem	A mathematical problem where two gamblers play a series of games against each other, with each gambler having a finite amount of money, until one of them runs out of money.
Conditional Probability	The probability of an event occurring given that another event has occurred.
Initial Fortune	The amount of money or resources a person starts with in a situation where they aim to win or achieve a goal before losing their initial resources.
Probability of Winning	The likelihood of a person or entity winning a game, lottery, or other situation where outcomes are uncertain.
Goal Achievement Probability	The likelihood of a person or entity achieving their desired outcome or goal in a situation where outcomes are uncertain.
Random Events	Independent and unpredictable events that can occur in a situation, such as the outcome of a coin toss or a game of chance.
Coin Toss	A random event where a coin is flipped and the outcome is either heads or tails.
Box Selection	The act of choosing one of two or more boxes or containers randomly, often with an unknown or uncertain outcome.
Event Inequality	A situation where the probability of one event occurring is greater than or equal to the probability of another event occurring.
Event Comparison	The act of comparing the probabilities of two or more events to determine their relative likelihood of occurrence.
Supplementary Exercises	Additional problems or exercises that challenge and test understanding of concepts, often used as homework assignments or practice materials.
Sample Space	The set of all possible outcomes or results of a probabilistic experiment.
Probability that exactly three tosses will be required	The probability that it will take exactly three coin tosses for both a head and a tail to appear at least once.
Pr (A c∪Bc)	The probability of the event that neither A nor B occurs, given that Pr (A) = 1/3, Pr (B) = 1/5, and Pr (A |B) + Pr (B |A) = 2/3.
Pr (A ∪Bc|B)	The conditional probability of the event that A does not occur and B does not occur, given that B occurs, with Pr (A) = 1/3 and Pr (B) > 0.
Probability that the first three rolls each yield the number 6	The probability that the same number (6) appears each of the first three rolls of a balanced die, given that the number 6 appeared exactly three times in 10 rolls.
Conditional Probability	The probability of an event occurring given that another event has occurred.
Independent Events	Events are said to be independent if the occurrence of one event does not affect the probability of another event.
Bus Rider's Lucky Condition	A bus rider is said to be lucky if U + V = W + X, where U, V, W, and X are four numbers chosen independently and uniformly at random from the set {0, 1, ..., 9}.
Committee Member Selection	In a committee with eight members, three members are selected at random and independently in January, four members are selected at random and independently in February, and five members are selected at random and independently in March.
Coin Rolling Game	Two players, A and B, take turns rolling a pair of balanced dice, and the winner is the first player to obtain a sum of 7 on a given roll.
Coin Toss Game	Three players, A, B, and C, take turns tossing a fair coin, and the cycle is repeated until someone wins by being the first player to obtain a head.
Balanced Die	A die with six equally likely outcomes: 1, 2, 3, 4, 5, and 6.
Probability of Meeting a Shy Person	The probability of meeting a shy person at a gathering of statisticians and economists, given that 80% of statisticians are shy and 15% of economists are shy.
Dreamboat Car Production	Dreamboat cars are produced at three factories, A, B, and C, with factory A producing 20% of the total output, B producing 50%, and C producing 30%.
Conditional Probability	The probability of an event occurring given that another event has occurred.
Bayes' Formula	A formula for updating conditional probabilities based on new information.
Independent Events	Events that do not affect the probability of each other.
Independent Trials	Events that occur independently of each other.
Conditional Probability	Conditional probability is the probability of an event occurring given that another event has occurred. It is calculated as the probability of the event occurring and the event that has already occurred, divided by the probability of the event that has already occurred.
Discrete Distribution	A discrete distribution assigns positive probability to a countable number of different values. It can be characterized by its probability function (p.f.), which specifies the probability that the random variable takes each of the different possible values.
Random Variable	A real-valued function that is defined on the sample space of an experiment.
Random Variable (Example 3.1.1)	The number X of heads in the 10 tosses is a random variable. Another random variable in this example is Y=10-X, the number of tails.
Random Variable (Example 3.1.2)	A person's height in inches measured from a sample space of persons is a random variable.
Demand for Water	A random variable expressed as X(s)=x when s=(x,y) in the sample space of demands for water and electricity.
Demand for Electricity	A random variable expressed as Y(s)=y when s=(x,y) in the sample space of demands for water and electricity.
Indicator of High Demand	A random variable Z(s)=/braceleftbigg1 if s∈A∪B, 0 if s∈/negationslashA∪B, where A and B are events described in Example 1.5.4.
Random Variable	A variable that takes on different values depending on the outcome of an experiment or observation.
Probability of a Random Variable	The probability that the value of a random variable will belong to a given subset of real numbers, denoted as Pr (X ∈C).
Distribution of a Random Variable	The collection of all probabilities of the form Pr (X ∈C) for all sets C of real numbers such that {X∈C} is an event, which is a probability measure on the set of real numbers.
Event	A set of outcomes of an experiment or observation that has a definite probability.
Probability Measure	A measure that assigns a probability value to each subset of a set, such that the probability of the union of two or more disjoint sets is the sum of their individual probabilities.
Uniform Distribution	A distribution where each outcome in a set of outcomes has an equal probability, as illustrated in the example of tossing a fair coin.
Discrete Distribution	A distribution where the random variable can only take on a finite or countable set of distinct values, as illustrated in the example of the number of heads in a coin toss.
Continuous Distribution	A distribution where the random variable can take on any value within a given range or interval, as illustrated in the example of water demand in Example 3.1.5.
Discrete Distribution	A random variable that can take only a finite number of different values, at most an infinite sequence of different values.
Probability Function (p.f.)	A function that gives the probability of a random variable taking a specific value, abbreviated as p.f. The probability function is defined as the function such that for every real number x, f(x) = Pr(X = x).
Support of a Distribution	The closure of the set of values for which the probability function is greater than 0.
Theorem 3.1.1	A theorem stating that if X is a discrete random variable with probability function f, then if x is not one of the possible values of X, then f(x) = 0. Also, if the sequence x1, x2, ... includes all the possible values of X, then ∑∞ i=1 f(xi) = 1.
Probability Function (p.f.)	The p.f. is a function f(x) that corresponds to a possible value x, where the sum of the heights of the vertical segments must be 1.
Discrete Distribution	A distribution of a discrete random variable that can be characterized by its probability function (p.f.).
Bernoulli Distribution/Random Variable	A random variable Z that takes only two values 0 and 1 with Pr(Z = 1) = p has the Bernoulli distribution with parameter p.
Uniform Distribution on Integers	A uniform distribution on the integers a, ..., b, where the value of a random variable X is equally likely to be each of the integers a, ..., b.
Uniform Distribution	A probability distribution where each of the possible values is equally likely to occur, represented by the probability mass function f(x) = 1/(b-a+1) for x=a,...,b, and 0 otherwise.
Uniform Distribution on Integers	A special case of the uniform distribution where the possible values are a set of consecutive integers a,...,b.
Random Variables	Mathematical constructs used to describe outcomes of experiments or events, often denoted by capital letters X and Y, which can take on different values with assigned probabilities.
Binomial Distribution	A probability distribution used to model the number of successes in a fixed number of independent trials, each with a constant probability of success.
Binomial Distribution	A binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent trials, each with a constant probability of success.
Bernoulli Distribution	The Bernoulli distribution is a special case of the binomial distribution with a single trial, having parameters n=1 and p, where n is a positive integer and 0<p<1.
Uniform Distribution	A probability distribution in which all outcomes in a given interval are equally likely.
Probability Function (p.f.)	A function that describes the probability distribution of a random variable, giving the probability of each possible outcome.
Constant of Discrete Distribution	A value c that appears in the probability function f(x) = c*x for x=1,2,...,5.
Binomial Distribution	A discrete probability distribution that models the number of successes in n independent trials, each with a constant probability p of success.
Probability Density Function (p.d.f.)	A function that describes the probability distribution of a continuous random variable, giving the probability density at each point in the interval.
Continuous Distribution	A random variable X has a continuous distribution or is a continuous random variable if there exists a nonnegative function f, defined on the real line, such that for every interval of real numbers (bounded or unbounded), the probability that X takes a value in the interval is the integral of f over the interval.
Probability Density Function (p.d.f.)	If X has a continuous distribution, the function f described in Definition 3.2.1 is called the probability density function (abbreviated p.d.f.) of X.
Support	The closure of the set {x: f(x) > 0} is called the support of (the distribution of) X.
Probability Density Function Requirements	A typical p.d.f. must satisfy the following two requirements: f(x) ≥ 0, for all x, and ∫∞ -∞ f(x) dx = 1.
Continuous Distribution	A random variable X has a continuous distribution if it can take any value within a given interval, and the probability of X being equal to any specific value is zero.
Probability Density Function (p.d.f.)	A probability density function is a function that describes the distribution of a continuous random variable, where the area under the curve represents the probability that the random variable falls within a given interval.
Nonuniqueness of p.d.f.	The probability density function of a random variable with a continuous distribution is not unique, as small changes to the function at individual points do not affect the probability distribution, allowing for multiple possible versions of the p.d.f.
Uniform Distribution on an Interval	A probability distribution where a random variable X is equally likely to fall within a given interval [a, b], with the probability of X belonging to a subinterval within [a, b] being proportional to the length of that subinterval.
Uniform Distribution	A probability distribution where a random variable X takes a value within a specific interval [a, b] with equal likelihood, meaning any particular part of the interval is just as likely to be chosen as any other part of the same length.
Probability Density Function (PDF)	A function that describes the probability distribution of a random variable at a given point, often denoted as f(x), and represents the probability density at that point.
Support of the Distribution	The set of values for which the probability density function is non-zero, in this case, the interval [a, b].
Density Is Not Probability	A note that the probability density function does not equal the probability that the random variable is near a certain value, but rather the integral of the PDF over a range gives the probability of the variable being in that range.
Uniform Distribution PDF	The probability density function of a uniform distribution, which is given by f(x) = 1/(b-a) for a ≤ x ≤ b, and 0 otherwise.
Uniform Distribution on an Interval [a, b]	A random variable X that represents the outcome of an experiment where a point is chosen at random from the interval [a, b], meaning any part of the interval is just as likely to be chosen as any other part of the same length.
Uniform distribution	A continuous distribution where every possible value within a given interval or range has an equal probability or likelihood of being selected.
Probability density function (p.d.f.)	A function that describes the relative likelihood of an event occurring, and is used to calculate probabilities of events within a given interval or range.
Normalizing constant	A constant value that is used to ensure that the area under a probability density function is equal to 1, thereby ensuring that the function integrates to 1.
Incompletely specified p.d.f.	A probability density function where the value of a constant factor or parameter is not explicitly given, but can be determined using the fact that the integral of the p.d.f. must be 1.
Unbounded Random Variables	Random variables with continuous distributions that can be represented by a probability density function (p.d.f.) that is positive over an unbounded interval of the real line.
Probability Density Function (p.d.f.)	A function that describes the probability distribution of a continuous random variable, with values representing probability densities rather than probabilities.
Unbounded p.d.f.'s	Probability density functions that have values larger than 1, which can be unbounded in certain neighborhoods.
Mixed Distributions	Distributions that are a combination of discrete and continuous distributions.
Truncated Voltage	A random variable that represents the measured voltage in an electrical system, with a voltmeter recording the actual value of the voltage if it is within a certain range, and simply recording a fixed value if the voltage exceeds that range.
Cumulative Distribution Function (CDF)	A function, often denoted as F(x), that gives the probability of a continuous random variable X being less than or equal to a given value x.
Probability Density Function (p.d.f.)	A non-negative function, often denoted as f(x), that describes the probability density of a continuous random variable X. The probability that X lies in a particular interval [a, b] is given by the integral of the p.d.f. over that interval.
Uniform Distribution	A continuous distribution with a probability density function that is constant over a specified interval [a, b] and is zero elsewhere.
Random Variable	A mathematical representation of a variable whose possible values are not fixed, but are determined by chance or by a random process.
Cumulative Distribution Function (CDF)	The cumulative distribution function (CDF) F(x) of a random variable X is the function that maps each real number x to the probability that X is less than or equal to x, i.e., F(x) = Pr(X ≤ x) for -∞ < x < ∞.
Cumulative Distribution Function (c.d.f.)	The cumulative distribution function (c.d.f.) of a random variable X is a function F(x) that gives the probability that X is less than or equal to x, denoted as Pr(X ≤ x). It is a non-decreasing function that satisfies the properties of being non-decreasing, having limits at ±∞, and characterizes the distribution of a random variable.
F(x)	The cumulative distribution function of a random variable X, representing the probability that X takes a value less than or equal to x.
F(x −)	The left limit of the cumulative distribution function F(x), denoting the limit of the values of F(y) as y approaches x from the left (y < x).
F(x +)	The right limit of the cumulative distribution function F(x), denoting the limit of the values of F(y) as y approaches x from the right (y > x).
Continuity from the Right	A property of cumulative distribution functions stating that F(x) = F(x +) for every point x, meaning that a cdf is always continuous from the right.
Property 3.3.3	The property stating that a cumulative distribution function is always continuous from the right, i.e., F(x) = F(x +) for every point x.
Theorem 3.3.1	A theorem stating that for every value x, the probability Pr(X > x) is equal to 1 - F(x).
Theorem 3.3.2	A theorem stating that for all values x1 and x2 such that x1 < x2, the probability Pr(x1 < X ≤ x2) is equal to F(x2) - F(x1).
Cumulative Distribution Function (c.d.f.)	The cumulative distribution function (c.d.f.) of a random variable X, denoted by F(x), is a function that gives the probability that the variable takes on a value less than or equal to x.
Discrete Distribution	A distribution of a discrete random variable X, where the probability of each possible value of X is greater than zero, and there are a countable number of possible values.
Probability Mass Function (p.f.)	A function that assigns a probability mass to each possible value of a discrete random variable X.
Jump	A discontinuity in the cumulative distribution function (c.d.f.) F(x) at a point x, where the values of Pr(X<x) and Pr(X≤x) are different due to a jump in F(x) at x.
Cumulative Distribution Function (c.d.f.)	The cumulative distribution function (c.d.f.) of a continuous distribution, denoted by F(x), represents the probability that a random variable X takes on a value less than or equal to x.
Probability Density Function (p.d.f.)	The probability density function (p.d.f.) of a continuous distribution, denoted by f(x), represents the probability that a random variable X takes on a particular value x, divided by the probability density of the random variable in the neighborhood of x.
Fundamental Theorem of Calculus	The fundamental theorem of calculus relates the derivative of a function to its integral, which is used to establish the relationship between the probability density function and the cumulative distribution function.
Quantile Function	The quantile function of a continuous distribution, denoted by F^-1(x), represents the value that X takes on with probability x.
Fair Bet	A fair bet is a wager where the probability of winning and losing are equal, ensuring that the bet is neither favorable nor unfavorable to either party.
Quantile	A quantile is a value of a random variable below which a certain percentage of the distribution lies, such as the 50th percentile or the 0.5 quantile.
Percentile	A percentile is a quantile expressed as a percentage, such as the 83rd percentile, which is the value below which 83% of the distribution lies.
Inverse Cumulative Distribution Function (F−1)	The inverse cumulative distribution function, denoted by F−1, assigns each probability level p to the smallest value x such that the cumulative distribution function F(x) is greater than or equal to p.
Quantile Function	The quantile function is a function that assigns each probability level p to the corresponding quantile or percentile, which is the smallest value x such that the cumulative distribution function F(x) is greater than or equal to p.
Value at Risk (VaR)	Value at Risk (VaR) is a measure of the potential loss that an investment portfolio might incur over a fixed time horizon, such as one month, at a specified probability level, such as 0.99.
VaR	VaR (Value-at-Risk) is a measure of the risk of loss of a portfolio that is equal to or less than a given probability level (in this case, 0.99).
Quantile	A quantile is a point on the distribution of a random variable where a given proportion of the data lies below it. In other words, it is a value such that a specified percentage of the data is less than or equal to that value.
0.01 Quantile	A 0.01 quantile is a point on the distribution of a random variable where 1% of the data lies below it.
0.99 Quantile	A 0.99 quantile is a point on the distribution of a random variable where 99% of the data lies below it.
Uniform Distribution	A uniform distribution is a type of continuous distribution where every possible value within a given interval has an equal probability of occurrence.
Cumulative Distribution Function (c.d.f.)	The cumulative distribution function (c.d.f.) of a random variable is a function that gives the probability that the random variable takes on a value less than or equal to a given value.
Quantile Function	The quantile function of a random variable is the inverse of its cumulative distribution function, and it gives the value of the random variable for which a given proportion of the data lies below it.
Quantile	A quantile is a value of a random variable below which a specified fraction of the data points fall. In other words, it is the smallest number x such that F(x) ≥ p, where F is the cumulative distribution function and p is the specified fraction.
Quantile Function	The quantile function is a function that maps a given probability p to the corresponding value x of the random variable. It is often denoted by F −1(p) and represents the inverse of the cumulative distribution function F.
Median	The median of a distribution is the 1/2 quantile or 50th percentile of the random variable. It is the smallest number x such that F(x) ≥ 1/2, and it divides the distribution in half as closely as is possible.
Lower Quartile	The lower quartile (also known as the 25th percentile) is the 1/4 quantile of a distribution, which is the smallest number x such that F(x) ≥ 1/4.
Upper Quartile	The upper quartile (also known as the 75th percentile) is the 3/4 quantile of a distribution, which is the smallest number x such that F(x) ≥ 3/4.
Continuous Distribution	A continuous distribution has a continuous cumulative distribution function (c.d.f.) and a probability density function (p.d.f.) that is differentiable for all x at which the c.d.f. is differentiable.
Discrete Distribution	A discrete distribution has a c.d.f. that is constant between the possible values and jumps by the probability mass function (p.m.f.) at each possible value x.
Quantile Function	The quantile function F−1(p) is equal to the smallest x such that F(x) ≥ p for 0<p< 1, where F(x) is the cumulative distribution function of a random variable X.
Cumulative Distribution Function (c.d.f.)	The c.d.f. F(x) of a random variable X is F(x) = Pr(X ≤ x) for all real x, and is continuous from the right. The c.d.f. F(x) − F(x −) = Pr(X = x) for all x.
Probability Density Function (p.d.f.)	The p.d.f. f(x) of a continuous distribution is equal to F′(x) = f(x), where F(x) is the c.d.f. of the distribution.
Pr (X< 0)	The probability that the random variable X is less than 0.
Pr (X ≤0)	The probability that the random variable X is less than or equal to 0.
Pr (X =1)	The probability that the random variable X is equal to 1.
Pr (0<X ≤3)	The probability that the random variable X is greater than 0 and less than or equal to 3.
Pr (0<X< 3)	The probability that the random variable X is greater than 0 and less than 3.
Pr (0≤X≤3)	The probability that the random variable X is greater than or equal to 0 and less than or equal to 3.
Pr (1<X ≤2)	The probability that the random variable X is greater than 1 and less than or equal to 2.
Pr (1≤X≤2)	The probability that the random variable X is greater than or equal to 1 and less than or equal to 2.
Pr (X> 5)	The probability that the random variable X is greater than 5.
Pr (X ≥5)	The probability that the random variable X is greater than or equal to 5.
Pr (3≤X≤4)	The probability that the random variable X is greater than or equal to 3 and less than or equal to 4.
Random Variable	A random variable is a function that assigns a numerical value to each possible outcome of a random experiment.
Quantile Function	The quantile function F−1 of a distribution function F is the function that assigns to each real number p in (0,1) the value F−1(p) such that F(F−1(p)) = p.
Joint Distribution	The joint distribution of two random variables X and Y is the collection of all probabilities of the form Pr[(X,Y) ∈ C] for all sets C of pairs of real numbers such that {(X,Y) ∈ C} is an event.
Discrete Joint Distribution	The joint distribution of two random variables X and Y is said to be discrete if there are only finitely or at most countably many different possible values for the pair (X,Y).
Discrete Joint Distribution	A joint distribution where the joint probability function f(x,y) is defined for a discrete random variable X and a discrete random variable Y, and the probability of each value of X and Y is zero if the pair (x,y) is not one of the possible values of the pair (X,Y).
Theorem	A mathematical statement that provides a logical approach to establish a fact or a relationship in statistics, specifically dealing with discrete joint distributions.
Probability Function	A function f(x,y) that specifies the joint probability of each pair of values (x,y) of the random variables X and Y.
Summation	A mathematical operation used to compute the sum of a series of values, typically denoted by the symbol ∑.
Joint Probability Function	A function f(x,y) that defines the joint probability of each pair of values (x,y) of the random variables X and Y.
Example	An illustration or demonstration of a concept, often used in statistics to clarify complex ideas or to provide practical applications.
Continuous Joint Distributions	A type of joint distribution where the joint probability function f(x,y) is a function of continuous variables X and Y, and the probability of each value of X and Y is defined as the area of the region bounded by the curves of the probability density function.
Continuous Joint Distribution	A continuous joint distribution is a nonnegative function f defined over the entire xy-plane such that for every subset C of the plane, Pr[(X,Y)∈C] = ∫∫C f(x,y)dxdy, if the integral exists. The function f is called the joint probability density function (abbreviated joint p.d.f.) of X and Y.
Continuous Joint Distribution	A joint distribution of two random variables X and Y is said to be continuous if its probability density function (p.d.f.) f(x,y) can be written as a continuous function over a region in the xy-plane, and the probability of any region is given by the integral of f(x,y) over that region, if the integral exists.
Support of a Continuous Joint Distribution	The support S of a continuous joint distribution is the region in the xy-plane where the density function f(x,y) is not zero.
Normalizing Constant	In the context of a continuous joint distribution, the normalizing constant c is a constant that makes the integral of the density function f(x,y) over the support S equal to 1.
Joint p.d.f.	A function that characterizes a joint distribution of two random variables, X and Y, where X is discrete and Y is continuous, and is defined as the integral of the product of the probability mass function and the probability density function over the subsets of the real numbers.
Mixed Bivariate Distribution	A joint distribution of two random variables, X and Y, where one variable is discrete and the other is continuous.
Joint p.f./p.d.f.	A function that characterizes a joint distribution of two random variables, X and Y, where X is discrete and Y is continuous, and is defined as the integral of the product of the probability mass function and the probability density function over the subsets of the real numbers.
Joint Probability Distribution Function (p.f./p.d.f.)	A function that describes the joint probability distribution of two random variables, X and Y, such that each value of X and Y corresponds to a specific probability value.
Joint Probability Density Function (p.d.f.)	A function that describes the probability density of the joint distribution of two random variables, X and Y, such that each value of X and Y corresponds to a specific probability density value.
Probability of a General Set	A measure of the probability of the set of all possible pairs of values that fall within a given set C, computed using the joint probability distribution function or density function of X and Y.
Marginalization	The process of computing the marginal probability distribution of a single random variable from the joint probability distribution of two or more random variables.
Joint (Cumulative) Distribution Function (Joint c.d.f.)	The joint distribution function or joint cumulative distribution function (joint c.d.f.) of two random variables X and Y is a function F(x, y) defined as the probability that X is less than or equal to x and Y is less than or equal to y, for all values of x and y. It is a generalization of the calculation of a cumulative distribution function (c.d.f.) to a bivariate distribution.
c.d.f.	A cumulative distribution function (c.d.f.) is a function that describes the probability distribution of a real-valued random variable. It gives the probability that the variable takes on a value less than or equal to a given value.
Joint c.d.f.	A joint cumulative distribution function (joint c.d.f.) is a function that describes the probability distribution of two or more real-valued random variables. It gives the probability that all the variables take on values less than or equal to given values.
Bivariate distribution	A bivariate distribution is a probability distribution for a pair of random variables.
Monotone increasing	A function is said to be monotone increasing if its value always increases as the input value increases.
Joint p.d.f.	A joint probability density function (joint p.d.f.) is a function that describes the probability distribution of two or more continuous random variables. It is the derivative of the joint cumulative distribution function with respect to both variables.
Limit	A limit of a function is the value that the function approaches as the input value approaches a certain point.
Joint Cumulative Distribution Function (c.d.f.)	The joint c.d.f. of two random variables X and Y is a function F(x, y) that represents the probability that both X is less than or equal to x and Y is less than or equal to y.
Joint Probability Density Function (p.d.f.)	The joint p.d.f. of two continuous random variables is a non-negative function f(x, y) such that the probability of the pair (X, Y) being in a set C is the integral of f(x, y) over the set C, if the integral exists. The joint p.d.f. is also the second mixed partial derivative of the joint c.d.f. with respect to both variables.
Probability of a Pair	The probability of a pair (X, Y) being in a set C, denoted as Pr (X ≤ x and Y ≤ y), is the integral of the joint p.d.f. over the set C.
Joint Probability Function (p.f.)	The joint p.f. of two discrete random variables is a non-negative function f(x, y) such that the probability of the pair (X, Y) being in a set C is the sum of f(x, y) over all points in C. A joint p.f. can be strictly positive at countably many pairs (x, y) at most.
Mixed Partial Derivative	The second mixed partial derivative of a function with respect to two variables is the derivative with respect to one variable, and then the derivative with respect to the other variable.
Joint probability density function (j.p.d.f.)	A function that describes the probability distribution of a pair of random variables (X, Y) over a two-dimensional region.
Constant marginal density	A density function that is constant over a specific region of the xy -plane, where (X, Y) is the pair of random variables.
Rectangle	A two-dimensional region in the xy -plane, defined by constraints on the values of X and Y.
Probability Mass Function (p.m.f.)	A function that assigns a probability to each possible value of a discrete random variable.
Probability of Intersection	The probability that two events or regions, in this case (X, Y), both occur.
Discrete Joint Distribution	A distribution of a pair of random variables (X, Y) where the joint probability density function is defined over a finite set of values.
Continuous Joint Distribution	A distribution of a pair of random variables (X, Y) where the joint probability density function is defined over a continuous range of values.
Joint Probability Function (j.p.f.)	A function that describes the probability distribution of a pair of random variables (X, Y) over a two-dimensional region.
Region	A subset of the xy -plane, used to define the boundaries or constraints of the joint probability density function.
Corner	A specific point on the boundary of a region in the xy -plane.
Pr (X ≤1/4)	The probability that the random variable X is less than or equal to 1/4.
Pr (X +Y≤1)	The probability that the sum of the random variables X and Y is less than or equal to 1.
Pr (1≤X≤2 and 1 ≤Y≤2)	The probability that the random variable X lies between 1 and 2, and the random variable Y lies between 1 and 2.
Pr (2≤X≤4 and 2 ≤Y≤4)	The probability that the random variable X lies between 2 and 4, and the random variable Y lies between 2 and 4.
The joint p.d.f. of XandY	The joint probability density function of the random variables X and Y.
Pr (Y ≤X)	The probability that the random variable Y is less than or equal to the random variable X.
f(x,y)	The joint probability density function of the random variables X and Y, given as a product of terms.
Pr (X =0)	The probability that the random variable X is equal to 0.
Table 3.3	A table containing the joint probability mass function of the random variables X and Y for a clinical trial of depression drugs.
Probability that a patient selected at random used Lithium (either alone or in combination with Imipramine) and did not relapse	The probability that a patient selected at random from the study used Lithium and did not relapse.
Probability that the patient had a relapse (without regard to the treatment group)	The probability that a patient selected at random from the study had a relapse, without regard to the treatment group.
Marginal Distribution	The marginal distribution of a random variable X, computed from a joint distribution, is also known as the distribution of one variable from the joint distribution. Each random variable will have a marginal cumulative distribution function (c.d.f.), marginal probability function (p.f.), and marginal probability density function (p.d.f.).
Marginal Cumulative Distribution Function (c.d.f.)	The marginal c.d.f. of X derived from a joint c.d.f. F of two random variables X and Y, obtained by Theorem 3.4.5.
Marginal Probability Function (p.f.)	The p.f. of X associated with the marginal c.d.f. of X.
Marginal Probability Density Function (p.d.f.)	The p.d.f. of X associated with the marginal c.d.f. of X, for continuous distribution.
Theorem 3.5.1	If X and Y have a discrete joint distribution with joint p.f. f, then the marginal p.f. of X is f1(x) = ∑ Allyf(x, y), and the marginal p.f. of Y is f2(y) = ∑ Allxf(x, y).
Marginal Probability Function (Marginal p.f.)	The marginal probability function f1(x) or f2(y) is a function that gives the probability of a random variable taking on a particular value, calculated by summing the joint probability function over all possible values of the other random variable.
Marginal Probability Density Function (p.d.f.)	The marginal p.d.f. of a random variable X is a function that gives the probability density of X, marginalized over the other random variable in a joint distribution.
Marginal Distributions	A marginal distribution is a distribution that describes the probability of a random variable without considering the values of other random variables. It is obtained by integrating or summing a joint distribution.
Joint Distribution	A joint distribution is a distribution that describes the probability of two or more random variables. It is obtained by integrating or summing the products of the marginal distributions.
Independent Random Variables	Independent random variables are random variables whose values are not correlated with each other. The product of their marginal distributions is equal to their joint distribution.
Marginal Probability Density Function (p.d.f.)	A marginal p.d.f. is a function that describes the probability distribution of a random variable without considering the values of other random variables.
Joint Probability Distribution Function (p.d.f.)	A joint p.d.f. is a function that describes the probability distribution of two or more random variables.
Correlation	Correlation refers to the relationship between two or more random variables. Independent random variables have zero correlation.
Distributions	The concept of distributions refers to the representation of random variables using probability density functions (p.d.f.s) or probability mass functions (p.m.f.s) that model the behavior of the variables.
Independent Discrete Random Variables	Discrete random variables X and Y are considered independent if learning the value of Y does not change any of the probabilities associated with X, or vice versa.
Conditional Distribution	A conditional distribution is a probability distribution that describes the probability of an event occurring based on the occurrence of another event or set of events.
Marginal Distribution	The marginal distribution of a bivariate distribution is the distribution of one of the variables, ignoring the other variable.
Joint Distribution	The joint distribution of two or more random variables is the probability distribution that describes the joint probability of all outcomes of the variables.
Uniform Distribution	A uniform distribution is a type of probability distribution in which every possible outcome has an equal probability of occurring.
Binomial Distribution	A binomial distribution is a type of probability distribution that describes the number of successes in a fixed number of independent trials, where each trial has a constant probability of success.
Independent Random Variables	Two random variables are independent if the joint probability distribution of the two variables can be expressed as the product of their individual probability distributions.
Joint Probability Function (p.f.)	The joint probability function of two random variables represents the probability distribution of the two variables jointly occurring.
Marginal Distributions	Marginal distributions are the probability distributions of individual random variables, obtained by summing or integrating the joint probability function over the other random variable.
Factorization of Joint Probability Function	The joint probability function of two independent random variables can be factored into the product of their individual probability distributions.
Proportional Marginals	Two joint probability distributions are said to have proportional marginals if the rows or columns of the table specifying their joint probability function are proportional to each other.
Independent Random Variables	Random variables X and Y are independent if their joint probability density function (p.d.f.) can be factored into the product of the marginal p.d.f.'s of X and Y, i.e., f(x,y) = f1(x) \* f2(y), where f1 and f2 are non-negative functions of x and y, respectively.
Marginal Probability Density Function (p.d.f.)	The probability density function (p.d.f.) of a random variable marginalized over another random variable, giving the probability distribution of the individual variable.
Independent Random Variables	Two random variables are said to be independent if their joint probability density function (p.d.f.) can be factored into the product of their individual marginal p.d.f.'s.
Joint Probability Density Function (p.d.f.)	A probability density function (p.d.f.) that describes the joint probability distribution of two or more random variables.
Discrete Joint Distribution	A joint probability distribution of two or more random variables with a finite or countable number of possible values.
Continuous Joint Distribution	A joint probability distribution of two or more random variables with a continuous range of possible values.
Sufficient Condition for Independence	A condition that ensures two continuous random variables are independent, namely that the region where the joint p.d.f. is positive is a rectangle with sides parallel to the coordinate axes and the joint p.d.f. factors into separate functions of x and y in this region.
Joint Probability Density Function (j.p.d.f.)	A function that describes the joint distribution of two or more random variables, providing the probability density at each point in the joint space.
Marginal Probability Density Function (m.p.d.f.)	A function that describes the distribution of a single random variable, obtained by integrating the joint probability density function over all values of the other variables.
Independence of Random Variables	A property of two or more random variables that states that the probability of their joint occurrence is equal to the product of their individual probabilities.
Conditional Distribution	A probability distribution that provides the probability density or mass function of a random variable given that another random variable has taken a specific value.
Rectangle	A geometric shape defined by four points (x1, y1), (x1, y2), (x2, y1), and (x2, y2), where x1 ≤ x2 and y1 ≤ y2.
Circle	A geometric shapedefined by the set of all points (x, y) that are equidistant from a fixed central point.
Uniform Distribution	A probability distribution where each value in the support has an equal probability.
Exponential Distribution	A probability distribution with a probability density function that is proportional to the exponential function, often used to model wait times or failure times.
Conditional Distribution	The conditional distribution of a random variable X given another random variable Y is the distribution that would be used for X after learning the value of Y. In other words, it is the probability distribution of X that is adjusted based on the information provided by Y.
Conditional Distribution	A discrete distribution whose probability mass function (p.f.) is a function of a conditioning variable, g1(x|y) = f(x,y) / f2(y), which represents the probability of a random variable X taking a particular value x given that the conditioning variable Y has taken a particular value y.
Conditional Probability	The probability of a random variable X taking a particular value x given that a conditioning variable Y has taken a particular value y, denoted as Pr(X = x | Y = y) = Pr(X = x and Y = y) / Pr(Y = y) = f(x,y) / f2(y).
Joint Distribution	A distribution that describes the probability of two or more random variables occurring together, denoted as f(x,y), where f(x,y) is the joint probability mass function (p.f.).
Marginal Distribution	A distribution that describes the probability of a single random variable, denoted as f1(x) or f2(y), where f1(x) and f2(y) are the marginal probability mass functions (p.f.s) of X and Y, respectively.
Conditional Distribution	The conditional distribution of a random variable X given a random variable Y is a probability distribution that describes the distribution of X given that Y has a specific value y, often denoted as X|Y=y.
Conditional Probability Density Function (p.d.f.)	The conditional probability density function of a continuous random variable X given a continuous random variable Y is a function g1(x|y) that describes the distribution of X given that Y has a specific value y, often denoted as X|Y=y. It is defined as the ratio of the joint p.d.f. of X and Y to the marginal p.d.f. of Y, i.e., g1(x|y) = f(x,y) / f2(y).
Conditional p.d.f. of X given Y=y	The conditional p.d.f. of X given Y=y, denoted as g1(x|y), is a function that describes the distribution of X given that Y has a specific value y. It is defined as the ratio of the joint p.d.f. of X and Y to the marginal p.d.f. of Y, i.e., g1(x|y) = f(x,y) / f2(y).
Theorem 3.6.1	Theorem 3.6.1 states that for each value y, the conditional p.d.f. g1(x|y) defined in Definition 3.6.2 is a probability density function (p.d.f.) as a function of x.
Conditional Probability Density Function (p.d.f.)	The conditional probability density function of a random variable X given Y, denoted as g1(x |y), is a function that describes the probability distribution of X when Y is known to have a specific value y. It is proportional to the joint probability density function f(x,y) of X and Y, with a constant factor required to ensure the conditional distribution integrates to unity over all values of x.
Conditional p.d.f.	The conditional probability density function (p.d.f.) of a random variable X given another random variable Y, denoted as g2(y|x), represents the probability density of X when Y takes a specific value y.
Conditional Probability Distribution	A probability distribution that defines the probability of an event given that another event has occurred.
Conditional Probability Mass Function (p.f.)	A function that defines the probability of a discrete random variable given that another variable has taken on a specific value.
Conditional Probability Density Function (p.d.f.)	A function that defines the probability density of a continuous random variable given that another variable has taken on a specific value.
Mixed Distribution	A distribution that combines a discrete random variable with a continuous random variable.
Joint Distribution	A distribution that defines the probability of a set of random variables at the same time.
Multiplication Rule for Distributions	A rule that states the probability of two events occurring together is the product of the probability of one event and the probability of the other event given the first event.
Conditional Distribution	A distribution that defines the probability of a random variable given that another random variable has taken on a specific value.
Conditioning	The process of finding the conditional probability density function or mass function of one random variable given the value of another random variable.
Joint Probability Density Function (j.p.d.f.)	A function that describes the joint distribution of two or more random variables, giving the probability density of each possible combination of values.
Conditional Probability Density Function (c.p.d.f.)	A function that describes the distribution of one random variable given the value of another random variable, giving the probability density of each possible value of the conditional random variable.
Law of Total Probability	A mathematical formula that allows us to find the probability of an event by summing the probabilities of each possible outcome, given a set of random variables.
Bayes' Theorem	A mathematical formula that allows us to update the probability of an event given new information or an additional random variable.
Law of Total Probability for Random Variables	A theorem that states that the marginal probability distribution function (p.d.f.) of a random variable X is the sum of the conditional p.d.f. of X given Y times the marginal p.d.f. of Y, where Y is another random variable.
Bayes’ Theorem for Random Variables	A theorem that states that the conditional p.d.f. of Y given X is the product of the conditional p.d.f. of X given Y and the marginal p.d.f. of Y, divided by the marginal p.d.f. of X.
Uniform Distribution	A probability distribution that assigns equal probability to every interval of equal length.
Marginal Probability Distribution Function (p.d.f.)	A probability distribution function that describes the probability distribution of a random variable X without considering any other random variable.
Conditional Probability Distribution Function (p.d.f.)	A probability distribution function that describes the probability distribution of a random variable X given the value of another random variable Y.
Joint Probability Distribution Function (p.d.f.)	A probability distribution function that describes the probability distribution of two or more random variables together.
Independence	A mathematical property of random variables X and Y, where X and Y are independent if and only if their joint probability density function (p.d.f) can be factored into the product of their individual p.d.f's, or if for every value of y such that f2(y) > 0 and every value of x, the conditional p.d.f. of X given Y = y is equal to the marginal p.d.f. of X.
Conditional Distribution	A statistical concept that describes the probability distribution of a random variable X given an observed value y of another random variable Y.
Bayes' Theorem	A mathematical formula used to calculate the conditional probability of an event given another event, often represented as P(A|B) = P(B|A) \* P(A) / P(B) or P(A|B) = P(B|A) \* P(A) / ∫P(B|x) \* P(x).
Conditional Distribution	The conditional distribution of a random variable X given that another random variable Y=y is the distribution of X when Y is known to be y.
Conditional Probability Function (p.f.)	The conditional probability function of a random variable X given that another random variable Y=y is the probability density function of X when Y is known to be y, represented as g1(x |y) = f(x,y) / f2(y), where f2 is the marginal probability function or density function of Y.
Independence of Random Variables	Two random variables X and Y are independent if the conditional probability function or density of X given Y=y is the same as the marginal probability function or density of X for all y such that f2(y) > 0. Equivalently, X and Y are independent if the conditional probability function of Y given X=x is the same as the marginal probability function or density of Y for all x such that f1(x) > 0.
Conditional Probability Density Function (Conditional p.d.f.)	The probability density function of a random variable Y given that a value X is known.
Joint Probability Density Function (Joint p.d.f.)	A function that describes the joint distribution of two or more random variables.
Marginal Probability Density Function (Marginal p.d.f.)	The probability density function of a random variable, ignoring the relationship between it and another random variable.
Random Variable	A variable whose possible values are numerical outcomes of a random phenomenon.
Probability	A measure of the likelihood of occurrence of an event, ranging from 0 (impossible) to 1 (certain).
Conditional Probability Density Function (c.p.d.f.)	A probability density function that describes the probability distribution of a random variable X given that another random variable Y has a specific value Y.
Joint Distribution Function (c.d.f.)	The joint c.d.f. of n random variables X1,...,X n is a function F(x1,...,x n) that specifies the probability that all the variables are less than or equal to specific values x1, x2,...,x n, and is defined as F(x1,...,x n) = Pr(X1 ≤ x1, X2 ≤ x2,...,X n ≤ x n).
Joint Discrete Distribution (p.f.)	A joint discrete distribution of n random variables X1,...,X n is a distribution where the random vector (X1,...,X n) can have only a finite number or an infinite sequence of different possible values (x1,...,x n) in R n, and the joint probability function (p.f.) f(x1,...,x n) is defined as f(x1,...,x n) = Pr(X1 = x1,...,X n = xn).
Discrete Joint Distribution	A discrete joint distribution is a probability distribution of a random vector X where each element of X is a discrete random variable, and its probability mass function (p.f.) is specified at every point x in the support of X.
Joint p.d.f. (Probability Density Function)	A function f that represents the joint distribution of random variables, some of which have continuous distributions and some of which have discrete distributions.
Mixed Distribution	A distribution that combines both continuous and discrete distributions, represented by a joint p.d.f.
Single-Server Queue	A queueing system where a single server provides service to customers arriving at a single server.
Joint Distribution	A distribution that describes the joint behavior of multiple random variables.
Service Rate	The rate at which customers are served in a queueing system.
Arrival Rate	The rate at which customers arrive at a queueing system.
Service Time	The time it takes for a server to serve a customer in a queueing system.
Arrival Time	The time at which a customer arrives at a queueing system.
Random Variables	A random variable is a measurable function from a probability space to the real numbers, which assigns a numerical value to each outcome in the sample space.
Probability Density Function (p.d.f.)	A probability density function is a non-negative function that integrates to 1 over all possible values of the random variable, and represents the relative likelihood of each value.
Joint Probability Density Function (p.d.f.)	A joint probability density function is a function that represents the probability distribution of multiple random variables, and gives the probability density of each possible combination of values.
Marginal Distribution	A marginal distribution is the probability distribution of a single random variable when the distributions of the other random variables are marginalized or integrated out.
Marginal Distribution	The distribution of a subset of random variables from a joint distribution, obtained by integrating or summing over the other variables.
Joint Cumulative Distribution Function (c.d.f.)	The joint c.d.f. of four random variables X1, X2, X3, and X4 specifiying the joint probability distribution from which a sample can be drawn.
Independent Random Variables	A set of n random variables X1, ..., Xn are independent if the probability of a set of events is equal to the product of the probabilities of each event.
Joint Cumulative Distribution Function (c.d.f.) and Marginal Univariate Cumulative Distribution Function (c.d.f.)	The joint c.d.f. of X1, ..., Xn is the product of its n individual marginal c.d.f.'s if and only if the random variables are independent.
Product of Probability Density Functions (p.d.f.s)	The probability density function of n independent random variables is the product of their individual probability density functions.
Random Samples/Independently and Identically Distributed (i.i.d.) Sample Size	A random sample of a given size drawn from a probability distribution is said to be i.i.d. if each element of the sample is independently drawn from the same distribution.
Independent and Identically Distributed (i.i.d.)	Random variables that are both independent and have the same marginal probability distribution, specified as f(x).
Random Sample	A set of random variables X1, ..., Xn that form a sample from a population distribution, where n is the sample size.
Joint Probability Density Function (p.d.f.)	The probability density function of a set of random variables X1, ..., Xn, specified as g(x1, ..., xn).
Product Distribution	A joint probability density function of independent and identically distributed random variables X1, ..., Xn, specified as the product of their individual marginal density functions, g(x1, ..., xn) = ∏f(xi).
Exponential Distribution	A continuous probability distribution, often denoted as f(x) = λe^(-λx), where λ is the rate parameter.
Joint Probability Density Function of i.i.d. Random Variables	The joint probability density function of a set of independent and identically distributed random variables X1, ..., Xn, specified as the product of their individual density functions, g(x1, ..., xn) = (∏xi) e^(-∑xi).
Conditional Distributions	The concept of conditional distributions refers to the probability distribution of a subset of random variables given that other variables take on specific values. Specifically, it is the conditional probability density or frequency function of a random vector divided into two subvectors, given that the second subvector takes on a specific value.
Marginal Distribution	A marginal distribution is a probability distribution of a subset of random variables, ignoring the values of the other variables. In the context of conditional distributions, the marginal distribution is the probability distribution of the variables not conditioned upon.
Conditional Probability Density Function (CPDF)	A conditional probability density function is a probability density function that takes into account the values of some variables while conditioning on the values of other variables. It is defined as the ratio of the joint probability density function to the marginal probability density function of the conditioned variables.
Independent and Identically Distributed (i.i.d.)	Independent and identically distributed random variables are random variables that are independent of each other and have the same probability distribution. In the context of the given text, i.i.d. random variables X1, ..., Xn refer to random variables that have a continuous joint distribution and are equally likely to be the smallest or largest of the n lifetimes.
Multi-Variable Distributions	A statistical concept that describes the probability distribution of multiple variables, which can be paired or unpaired, to model and analyze complex relationships between them.
Multivariate Law of Total Probability	A mathematical formula that generalizes the law of total probability to multiple random variables, allowing us to compute the marginal probability distribution of one variable given the values of other variables.
Bayes' Theorem	A mathematical formula that relates the conditional probability of one random variable given the values of another random variable, to the joint probability distribution of the two variables and the marginal probability distribution of the second variable.
Conditional Distribution	The probability distribution of a random variable given the value of another random variable, also known as the conditional probability density function (pdf).
Joint Distribution	A probability distribution that describes the probability of different combinations of values of two or more random variables.
Marginal Distribution	The probability distribution of a random variable when the values of other random variables are taken into account, also known as the marginal probability density function (pdf).
Law of Total Probability	A mathematical principle that states that the probability of a random event can be computed by summing the probabilities of the event given different possible values of another random variable, weighted by the probability of each value.
Bayes' Theorem (Multivariate)	An extension of Bayes' theorem that involves multiple random variables and allows us to compute the conditional probability of one variable given the values of other variables.
Conditionally Independent Random Variables	Random variables X1, ..., Xn are conditionally independent given Z if, for all z such that the joint probability density function or mass function f0(z) > 0, the conditional multivariate probability density function or mass function g( x|z) of X given Z=z is equal to the product of the conditional univariate probability density functions or mass functions gi(x i|z) of Xi given Z=z for i = 1, 2, ..., n.
Conditional Version of Bayes' Theorem	A formula for calculating the conditional probability density function (p.d.f.) of a random variable given a set of conditioning variables, denoted as g2(z|y,w) = g1(y|z,w) \* f2(z|w) / f1(y|w), where g1(y|z,w) is the conditional p.d.f. of the conditioning variables given the random variable and the conditioning variables, f2(z|w) is the conditional p.d.f. of the random variable given the conditioning variables, and f1(y|w) is the conditional p.d.f. of the conditioning variables given the random variable.
Conditional Independence	A property of random variables where the conditional distribution of two or more random variables given a third random variable is independent. In other words, if X1, ..., Xn are independent random variables and W is a constant random variable, then X1, ..., Xn are also conditionally independent given W=c.
Conditionally Independent Random Variables	Random variables that are independent given some conditioning event.
Conditionally i.i.d. Random Variables	Random variables that are independent and identically distributed given some conditioning event.
Histogram	A graphical display of a collection of numbers, particularly useful for displaying the observed values of a collection of random variables that have been modeled as conditionally i.i.d.
Histogram	A graphical display of a large set of numbers, divided into subintervals, and often used to visualize the distribution of a random variable.
Rate (r)	A factor used to determine the height or area of each bar in the histogram, which can be set to various values, such as r=1, n, or n(b-a)/k, depending on the desired representation.
Joint Distribution	A description of the probability behavior of more than one random variable together, including cdf, pdf, and pmf for continuous, continuous, and discrete random vectors, respectively.
Random Vector	A mathematical object consisting of multiple interdependent random variables, often represented as a single entity.
Independent Random Variables	A set of random variables are independent if the joint probability density function or probability mass function can be expressed as a product of the individual probability density functions or probability mass functions.
Conditional Independence	A set of random variables are conditionally independent given another random variable if the conditional probability density function or probability mass function can be expressed as a product of the individual probability density functions or probability mass functions.
Joint Distribution	A mathematical function that describes the probability distribution of multiple random variables.
Marginal Distribution	A probability distribution of a subset of random variables.
Conditional Distribution	A probability distribution of a random variable given the values of one or more other random variables.
Bayes' Theorem	A mathematical formula that describes the relationship between conditional and unconditional probabilities.
Law of Total Probability	A mathematical formula that describes the relationship between unconditional probability and conditional probabilities.
Random Variables	Random variables are mathematical constructs that can take on different values according to some underlying probability distribution.
Conditional Probability Density Function (p.d.f.)	The conditional p.d.f. of a random variable given another random variable is the probability density function of the first variable, conditional on the value of the second variable.
Independence	Two or more random variables are said to be independent if the probability of one variable taking on a certain value does not depend on the value of another variable.
Series and Parallel Connections	In a system composed of independent components, the components are said to be connected in series if the system functions properly if and only if all components function properly. In contrast, the components are said to be connected in parallel if the system functions properly if and only if at least one component functions properly.
Reliability	The probability that a system or component functions properly is called its reliability.
Discrete Distribution	A discrete distribution is a probability distribution that assigns non-zero probabilities to distinct values.
Continuous Distribution	A continuous distribution is a probability distribution that assigns zero probability to distinct values and has a probability density function.
Joint Distribution	The joint distribution of two or more random variables is the probability distribution that describes their combined behavior.
Marginal Distribution	The marginal distribution of a joint probability distribution is the probability distribution of a single random variable, ignoring the other variables.
Conditional Probability Density Function (c.p.d.f.)	The conditional probability density function of a random variable given another random variable is a probability distribution function that describes the probability density of the variable of interest given the state of the conditioning variable.
Conditional Independence	Two or more random variables are conditionally independent given a third random variable if their joint probability density function can be factorized into the product of the marginal probability density functions of each variable, given the state of the conditioning variable.
Conditional Probability Density Function of a Function of a Random Variable	The conditional probability density function of a function of a random variable is the probability distribution function that describes the probability density of the function given another random variable.
Discrete Random Variable	A random variable that takes on a finite or countable set of values, and has a probability density function that is a sum of discrete probabilities at each value.
Function of a Random Variable	A function that maps the values of a random variable to a new variable, and is often used to transform the original variable into a more meaningful or useful form.
Marginal Probability Density Function	The probability density function of a random variable, without conditioning on any other variable.
Probability Density Function (p.d.f.)	A function that describes the probability density of a random variable, and is often used to describe the probability distribution of a continuous or discrete random variable.
Continuous Distribution	A probability distribution where the possible values of a random variable are continuous, meaning they can have any value within a given interval or range.
Function of a Random Variable	A new random variable derived from another random variable by applying a mathematical function to it.
Continuous Random Variable	A random variable that has a continuous distribution, meaning it can take on any value within a given interval or range.
Deriving the Probability Distribution of a Function of a Continuous Random Variable	A method for finding the probability distribution of a function of a continuous random variable, which involves integrating the probability density function of the original random variable over the range of the function.
Median or Median Waiting Time	Not found in the given text.
Direct Calculation	A method for finding the probability distribution of a function of a continuous random variable by directly evaluating the probability of each possible outcome.
Example 3.8.3	An example of finding the probability distribution of a function of a continuous random variable, where the original random variable is the rate at which customers are served in a queue, and the new random variable is the average waiting time.
Example 3.8.4	An example of finding the probability distribution of a function of a continuous random variable, where the original random variable has a uniform distribution and the new random variable is the square of the original random variable.
Uniform Distribution	A probability distribution where each possible value within a given interval or range has an equal probability of being chosen.
Linear Function	A mathematical function that is a multiple of a variable, such as f(x) = ax + b for some constants a and b.
Probability Integral Transformation	The process of transforming a continuous random variable X with a cumulative distribution function F into a new random variable Y, such that Y has the uniform distribution on the interval [0, 1].
Algorithm	A set of rules or procedures used to compute or generate a specific output or result, often used to produce pseudo-random numbers.
Corollary 3.8.1	A statement or consequence that follows from or is implied by a previously established theorem or principle, used to describe the relationship between a random variable and its quantile function.
Continuous Cumulative Distribution Function (c.d.f.)	A mathematical function that describes the probability that a random variable taking values in a given interval, and is used to generate values of a random variable with a specified distribution.
Independent Random Variables	A set of random variables that are not related or correlated with each other, often used to describe the properties of pseudo-random number generators.
Interval	A set of values or points that are within a specific range or bounds, often used to describe the interval [0,1] in which pseudo-random numbers are generated.
Pseudo-Random Numbers	Numbers that appear to have certain properties of random numbers, but are actually generated by deterministic algorithms, often used in statistical analyses.
Quantile Function	A mathematical function that maps a cumulative distribution function to the corresponding quantile or percentage point, often used to transform pseudo-random numbers to values with a specified distribution.
Random Sample	A set of data values or observations that are representative of the population from which they were drawn, often used to describe the values of a random variable with a specified distribution.
Random Variable	A mathematical representation of a variable or value that can take on different values, often used to describe the outcome of an experiment or observation.
Uniform Distribution	A probability distribution in which all values in a specific interval have the same probability, often used to describe the distribution of pseudo-random numbers.
Quantile Function	A statistical function that maps a uniform random variable to a random variable with a given cumulative distribution function (c.d.f.).
Inverse Function	A mathematical function that undoes the action of another function, in this case, the cumulative distribution function (c.d.f.); i.e., G(y) = x and G−1(x) = y.
Continuous Distribution	A probability distribution where all possible values have a non-zero probability, resulting in a continuous range of outcomes.
Discontinuous Distribution	A probability distribution where some or all values have zero probability, resulting in a distribution that is not continuous.
Waiting Time	The time it takes for an event or a random variable to occur or reach a certain value.
Exponential Growth	A popular model for populations of microscopic organisms in large environments, where the rate of growth is unknown and has a continuous distribution with a probability density function (p.d.f.).
Uniform Distribution	A probability distribution in which all values within a given interval are equally likely to occur.
Probability Density Function (p.d.f.)	A function that describes the probability distribution of a continuous random variable, where the area under the curve between two points represents the probability that the random variable falls within that range.
Cumulative Distribution Function (c.d.f.)	A function that describes the cumulative probability distribution of a random variable, where the probability that the random variable falls below a certain value is equal to the area under the curve up to that point.
Quantile Function	A function that maps a probability level to the corresponding quantile of a distribution, providing a way to "invert" the cumulative distribution function.
Transformations	The process of applying a mathematical function to a random variable to create a new random variable, which can have its own probability distribution.
Inverse Transform Sampling	A method for generating random variables from a given distribution, where a uniform random variable is transformed using the inverse of the distribution's cumulative distribution function.
Y	The amount that the insurance company has to pay on a claim, which depends on the dollar amount X of a claim.
Joint Probability Function (Joint p.f.)	The joint probability function of two or more discrete random variables is a function that specifies the probability of a particular combination of values occurring simultaneously.
Set A (Definition of Set A)	Set A is the set of all points (x1, ..., xn) such that r1(x1, ..., xn) = y1, r2(x1, ..., xn) = y2, ..., rm(x1, ..., xn) = YM, where Y1, ..., Ym are random variables and r1, ..., rm are functions of the random variables.
Counting Argument	A counting argument is a method used to determine the number of points in a set (such as Set A) by applying probability theories and mathematical operations.
Binomial Distribution	A binomial distribution is a discrete probability distribution that models the number of successes in a sequence of independent trials, where each trial has a constant probability of success.
Bernoulli Distribution	A Bernoulli distribution is a discrete probability distribution that models a binary random variable that takes the value 0 or 1, with probability p assigned to the value 1 and probability 1-p assigned to the value 0.
Independent and Identically Distributed (i.i.d.)	A set of random variables are independent and identically distributed (i.i.d.) if each variable has the same probability distribution and if the variables are independent of each other.
Random Variables	Independent or dependent variables that take on specific values with given probability distributions.
Binomial Distribution	A discrete probability distribution that models the number of successes in a fixed number of independent trials, where each trial is associated with a binary outcome (success or failure) with a fixed probability.
Joint Distribution	A probability distribution that describes the joint behavior of two or more random variables.
Joint Probability Function (j.p.f.)	A function that specifies the probability of a specific combination of values for two or more random variables.
Marginal Distributions	The probability distributions of individual random variables, as opposed to the joint distribution of multiple random variables.
Total Service Time	The time it takes to serve multiple customers or units in a queue or system.
Continuous Joint Distribution	A probability distribution that describes the joint behavior of two or more continuous random variables.
Cumulative Distribution Function (c.d.f.)	A function that gives the probability that a random variable takes on a value less than or equal to a given value.
Probability Density Function (p.d.f.)	A function that describes the relative likelihood of a random variable taking on a given value or range of values.
Brute-Force Method	A method for finding the distribution of a function of several random variables, where the joint probability density function (p.d.f.) is integrated over the set of points satisfying the function's condition.
Theorem 3.9.3	A theorem stating that the cumulative distribution function (c.d.f.) of a function Y=r(X) of random variables X=(X1, …, Xn) is equal to the integral of the joint p.d.f. of X over the set of points satisfying r(x) ≤ y.
Theorem 3.9.4	A theorem applicable to the case where Y=a1X1+a2X2+b, stating that the continuous distribution of Y has a p.d.f. given by the integral of the joint p.d.f. of X1 and X2 over the set of points satisfying a1x1+a2x2+b ≤ y.
Convolution	The distribution of the sum of two independent continuous random variables, Y = X1 + X2, also referred to as the probability density function (p.d.f.) of Y, which is the result of combining the p.d.f.'s of X1 and X2.
Random Sample	A set of independent and identically distributed (i.i.d.) random variables, denoted by X1, ..., Xn, drawn from a distribution with probability density function (p.d.f.) f and cumulative distribution function (c.d.f.) F.
Largest Value (Maximum)	The largest value in a random sample, denoted by Yn, which is defined as max{X1, ..., Xn}.
Smallest Value (Minimum)	The smallest value in a random sample, denoted by Y1, which is defined as min{X1, ..., Xn}.
Cumulative Distribution Function (c.d.f.)	A function G(y) that gives the probability that a random variable X is less than or equal to y, written as Pr(X ≤ y) = G(y).
Probability Density Function (p.d.f.)	A function f(y) that gives the probability that a random variable X is in a small interval [y, y + dy] around y, written as Pr(X ∈ [y, y + dy]) = f(y)dy.
Joint Cumulative Distribution Function (bivariate c.d.f.)	A function G(y1, yn) that gives the probability that two random variables Y1 and Yn are less than or equal to y1 and yn respectively, written as Pr(Y1 ≤ y1 and Yn ≤ yn) = G(y1, yn).
Pr (Y n≤ynandY1> y 1)	The probability that one random variable Y is less than or equal to yn and another random variable Y1 is greater than y1.
Joint p.d.f. of Y1 and Yn	The joint probability density function of two random variables Y1 and Yn.
Range of the random sample	The distance from the minimum to the maximum value of a random sample.
Theorem 3.9.4	A theorem used to find the probability density function of a transformed random variable.
g(y 1, y n)	The joint probability density function of Y1 and Yn.
Theorem 3.9.5	A theorem that generalizes Theorem 3.8.4 to the case of several random variables.
Unknown Term	No definition available.
One-to-One Relation	A transformation between two sets of random variables, S and T, where each point in S corresponds to a unique point in T, and vice versa.
Conditional Probability Density Function (p.d.f.)	A probability distribution of a random variable given that another random variable has a specific value.
Joint Probability Density Function (p.d.f.)	A probability distribution of two or more random variables.
Inverse Transformation	A transformation that reverses the effect of another transformation, especially in the context of changing from one variable to another.
Marginal Probability Density Function (p.d.f.)	A probability distribution of a single random variable, obtained by integrating a joint probability distribution over all other variables.
Rate	A measure of the frequency or intensity of a process or event, typically measured as a quantity per unit of time.
Transformation	A change of one or more variables from one form to another, often used to simplify or analyze a problem.
Quotient	The result of dividing one number by another in a mathematical operation.
Product	The result of multiplying two or more numbers together in a mathematical operation.
Linear Transformation	A transformation of multiple random variables that is a linear combination of the original variables.
Joint Distribution	The distribution of multiple random variables that assigns a probability to each possible combination of values.
Marginal Distribution	The distribution of a single random variable that is obtained by integrating out the other variables in the joint distribution.
Determinant	A scalar value that is derived from the elements of a square matrix and is used to define the Jacobian of a transformation.
Jacobian	A matrix that describes the transformation from one set of coordinates to another, used to compute the joint probability density function.
Random Variable	A mathematical concept that represents a value or a set of values that are subject to uncertainty or randomness.
Joint Distribution	A probability distribution that describes the joint behavior of two or more random variables.
Conditional Distribution	A probability distribution that describes the behavior of a random variable given specific values or conditions.
Independent Random Variables	Random variables that have a joint probability distribution in which the probability of any combination of outcomes does not depend on the outcomes of the other variables.
Uniform Distribution	A continuous probability distribution where every possible value within a given interval has an equal probability of being observed.
Continuous Random Variable	A random variable that can take on any value within a given interval, with no limits on the number of possible values.
Function of Random Variables	A mathematical operation applied to one or more random variables, resulting in a new random variable.
Probability Density Function (p.d.f.)	A mathematical function that describes the probability distribution of a continuous random variable, with the area under the curve representing the probability of a given event.
Cumulative Distribution Function (c.d.f.)	A mathematical function that describes the probability distribution of a random variable, with the function value representing the probability that the variable takes on a value less than or equal to a given point.
Range of a Random Sample	The difference between the maximum and minimum values of a random sample of observations from a given distribution.
Independent Random Variables	Random variables X1, X2, ..., Xn are independent if any subset of these variables is independent of every other subset with at least one variable not in the first subset.
Conditional Distribution	The conditional distribution of a random variable Y given X = x is the probability distribution of Y when X takes on the value x.
Markov Chain	A Markov chain is a sequence of random variables, one for each time, where each random variable gives the state of the system, and the conditional distribution of each future state given the past states and the present state depends only on the present state.
Stochastic Process	A stochastic process is a sequence of random variables, one for each time, where each random variable gives the state of the system, and the behavior of the system is uncertain and random.
Occupational Telephone Lines	The number of telephone lines being used at regular intervals of 2 minutes in a certain business office, observed at the beginning of the period, the second time, and in general, for n = 1, 2, ..., let Xn denote the number of lines being used when they are observed for the nth time.
Stochastic Process	A sequence of random variables, X1, X2, ..., called a stochastic process or random process with discrete time parameter, where each random variable has a marginal distribution and the entire process has a joint distribution.
Markov Chain	A stochastic process with discrete time parameter is a Markov chain if the conditional distributions of all Xn+j for j>=1 given X1, ..., Xn depend only on Xn and not on the earlier states X1, ..., Xn-1.
k	The number of possible states of a general finite Markov chain.
Transition Distributions	The probability of moving from one state at time n to another state at time n+1.
Stationary Transition Distributions	The probability of moving from one state at time n to another state at time n+1, which does not depend on n.
Stationary Transition Distributions	The transition distributions of a Markov chain are said to be stationary when they are the same for every time n, meaning that the probability of moving from state i to state j remains constant over time.
Transition Matrix	A square matrix for a finite Markov chain with stationary transition probabilities, where each element pij represents the probability that the system will be in state j at the next time step given that it is currently in state i.
Markov Chain	A mathematical system that undergoes transitions from one state to another, where the probability of transitioning from one state to another is dependent only on the current state, not on any of its previous states.
Markov Chain	A mathematical system that describes a probability distribution for a process that undergoes transitions from one state to another, where the distribution of the next state depends on the current state.
Transition Distribution	A probability distribution that specifies the probability of transitioning from one state to another in a Markov chain.
Offspring	A descendant or result of reproduction, where the offspring's genetic makeup is determined by the combination of alleles inherited from its parents.
Allele	A variant of a gene that occupies the same position on a chromosome.
Genotype	The genetic makeup of an organism, comprising the specific combination of alleles inherited from its parents.
Population Dynamics	The study of the size, growth, and distribution of populations, often incorporating concepts from probability theory and Markov chains.
Transition Matrix	A matrix representing the transition probabilities of a Markov chain, where each row represents a current state and each column represents a possible future state.
Markov Chain	A mathematical system that undergoes transitions from one state to another, between a finite or countable number of possible states, in a way that depends on the current state.
Conditional Probability	The probability of an event occurring given that another event has occurred.
Stationary Transition Distributions	A set of transition probabilities that do not change over time in a Markov chain.
k-Step Transition Probability	The probability of being in a particular state in k time periods into the future, given the current state.
p(2)ij	The k-step transition probability, denoting the probability of being in state j in two time periods into the future, given the current state i.
Multiple Step Transition	The probability that a Markov chain will move from a state i to a state j in m steps, as described by the m-step transition matrix.
Multiple Step Transition Matrix	The m-step transition matrix is the matrix Pm, where Pm has in row i and column j the probability p(m)ij that the chain will move from state i to state j in m steps, as described by Theorem 3.10.2.
Stationary Transition Distributions	A set of probabilities that describes the long-term behavior of a Markov chain, according to the conditions of Theorem 3.10.2.
Pm (m-step transition matrix)	The m-step transition matrix, which has in row i and column j the probability p(m)ij that the chain will move from state i to state j in m steps.
Absorbing State	In a Markov chain, an absorbing state is a state where the probability of staying in that state is 1, meaning once the chain enters that state, it cannot transition out of it.
Absorbing State	A state in a Markov chain from which the chain has a positive probability of getting into in two steps no matter where the chain starts.
Initial Distribution/Probability Vector	A vector consisting of nonnegative numbers that add to 1, which specifies the probabilities that a Markov chain will be in each of its states at time 1.
Markov Chain, Joint Distribution	The entire distribution of a Markov chain, which can be constructed from the initial probability vector and the transition matrix.
Marginal Distributions	The probability distributions of the states at times later than 1, which can be found from the joint distribution.
Stationary Distribution	A probability vector v that satisfies vP = v is called a stationary distribution for a Markov chain.
Stationary Distribution	A stationary distribution for a Markov chain is a distribution that remains the same at all times, meaning that the probabilities of being in each state remain constant, even as the chain transitions between states. In other words, the distribution of the chain's state does not change over time.
Ergodicness	The property of a Markov chain where every element of a matrix P (transition matrix) is strictly positive, ensuring that the chain has a unique stationary distribution and converges to it asymptotically, regardless of the starting distribution.
Stationary Distribution	The limiting distribution of a Markov chain, which is a vector v such that the product Pv equals the vector v, where P is the transition matrix.
Inverse Matrix	A square matrix G that satisfies GG −1=G−1G=I, where G is a matrix satisfying vG=(0, . . . , 0,1), and v is the stationary distribution.
Singular Matrix	A matrix G that does not have an inverse matrix, typically indicating that the matrix is not one-to-one or has dependent rows, which can occur when a Markov chain has multiple stationary distributions.
Ergodic Theorem	A theorem stating that if every element of a matrix P is strictly positive, then the Markov chain has a unique stationary distribution, and its distribution after n steps converges to the stationary distribution as n approaches infinity.
Markov Chain	A mathematical system that undergoes transitions from one state to another, where the probability of transitioning from one state to another is based on the current state.
Transition Matrix	A matrix that represents the probabilities of transitioning from one state to another in a Markov chain.
Stationary Distribution	The distribution of states in a Markov chain that remains unchanged over time, assuming that the chain has converged.
Alternating Chain	A Markov chain whose transition matrix alternates between two states.
Gambler's Ruin Problem	A problem in which a gambler wins or loses money in a game, and the sequence of amounts held by the gambler forms a Markov chain.
Markov Chain	A stochastic process where the conditional distribution of the state at the next time given all of the past states depends on the past states only through the most recent state.
Stationary Distribution	A probability vector v such that vP = v, where P is the transition matrix and v is the initial probability vector.
Transition Matrix (P)	A matrix giving the probabilities of transition from the state indexing the row to the state indexing the column.
Initial Probability Vector (v)	The distribution of the state at time 1, used to calculate all probabilities associated with the Markov chain.
Finite Markov Chain	A Markov chain with a finite number of states.
Stationary Transition Distributions	When a Markov chain has a stationary distribution, the distribution of the chain after a large number of transitions converges to the stationary distribution.
Markov Chain	A stochastic process that undergoes transitions from one state to another based on certain probabilities (stationary transition probabilities).
Stationary Transition Probabilities	A set of probabilities that describe the likelihood of transitioning from one state to another in a Markov chain, and remain constant over time.
Sunny	A weather condition characterized by pleasant and warm weather, often accompanied by clear skies.
Cloudy	A weather condition characterized by overcast skies, often accompanied by precipitation or reduced sunshine.
Conditional Probability	The probability of an event occurring given that another event has occurred.
Transition Probability	The probability of transitioning from one state to another in a Markov chain.
State	A specific condition or situation in a Markov chain, characterized by a set of probabilities.
Transition Matrix	A square matrix used to represent the transition probabilities between different states in a Markov chain.
Time n+2	The future point in time, n+2, being considered.
Transition Matrix	A matrix that describes the probabilities of transitioning from one state to another in a Markov chain.
Stationary Transition Probabilities	Probabilities that do not change over time in a Markov chain, where the probability of transitioning from one state to another is constant.
Markov Chain	A mathematical system that undergoes transitions from one state to another, with the future state depending on the current state and the transition probabilities.
Brand A	One of two brands of toothpaste being considered.
Brand B	One of two brands of toothpaste being considered.
Probability	A measure of the likelihood of an event occurring, expressed as a value between 0 and 1.
Boy A	One of three boys throwing a ball to each other.
Boy B	One of three boys throwing a ball to each other.
Boy C	One of three boys throwing a ball to each other.
Coin	A circular object used for flipping, with heads and tails being equally likely to appear on any given toss.
Probability of Transition	The likelihood of transitioning from one state to another in a Markov chain, expressed as a value between 0 and 1.
State of the Process	The current situation or condition of the process being considered, which may change over time.
Markov Chain	A stochastic process that undergoes transitions from one state to another, where the probability of transitioning from one state to another is constant and does not depend on the specific sequence of events that have occurred before.
Stationary Transition Probabilities	The probabilities of transitioning from one state to another in a Markov chain do not change over time, and the probability of being in a particular state at a given time does not depend on the initial state.
Transition Matrix	A matrix that describes the transition probabilities of a Markov chain, where the entry in the ith row and jth column represents the probability of transitioning from state i to state j.
Conditional Distribution	The probability distribution of a random variable given that certain events have occurred, in this case, the probability distribution of the state at a given time given the state at a later time.
Absorbing State	A state in a Markov chain that cannot be left once entered, meaning that the transition probabilities to other states are zero.
Continuous distribution	A probability distribution where the probability of a random variable X taking on any particular value is zero, but the probability of it falling within a specific range is greater than zero.
p.d.f. (probability density function)	A function that describes the probability density of a continuous random variable at a given point. The area under the p.d.f. between two points gives the probability that the random variable falls within that range.
c.d.f. (cumulative distribution function)	A function that describes the cumulative probability of a continuous random variable up to a given point. The c.d.f. is the integral of the p.d.f. over the range of the random variable.
i.i.d. (independent and identically distributed)	A set of random variables that are both independent (i.e., each random variable can take on any value independently of the others) and identically distributed (i.e., each random variable follows the same probability distribution).
p.f. (probability function)	A function that describes the probability of a discrete random variable taking on a specific value. The p.f. is analogous to the p.d.f. for continuous random variables.
Uniform distribution	A continuous probability distribution where every possible value within a given range has the same probability.
Event Space (Independent Intervals)	The set of disjoint intervals, Ij, where each interval represents the probability that none of the random variables X1, ..., X12 will contain more than one of the variables.
Independence of Random Variables	The condition where two or more random variables, X and Y, are independent, meaning that their joint distribution is equal to the product of their marginal distributions.
Joint Distribution of Random Variables	A probability distribution that describes the joint behavior of two or more random variables, X and Y.
Marginal Distribution of a Random Variable	The distribution of a random variable, X, ignoring its relationship with another random variable, Y, and considering it as a standalone variable.
Conditional Probability Distribution of a Random Variable	The probability distribution of a random variable, X, given that another random variable, Y, has a specific value.
Joint Probability Density Function (j.p.d.f.)	A function that describes the joint probability distribution of two or more random variables, X and Y, as a function of the underlying random variables.
Univariate Marginal Probability Density Function (p.d.f.)	The marginal probability density function of a random variable, X or Y, describing its distribution without considering its relationship with another random variable.
F (x) = 0 for x ≤ 0, 1 - e^(-x) for x > 0	The cumulative distribution function (c.d.f.) of X, with X being a random variable.
X and Y are i.i.d. random variables with f(x) = e^(-x) for x > 0, 0 otherwise	X and Y are independent and identically distributed (i.i.d.) random variables with probability density function (p.d.f.) f(x) = e^(-x) for x > 0, 0 otherwise.
Joint p.d.f. of U and V	The joint probability density function (p.d.f.) of U and V, where U = X / (X + Y) and V = X + Y, given the p.d.f. of X and Y.
Independence of X and Y	Whether X and Y are independent, given their joint p.d.f. and the question of whether they are independent.
Joint p.d.f. of U and V	The joint probability density function (p.d.f.) of U and V, where U = X / Y and V = Y, given the joint p.d.f. of X and Y.
Independence of X and Y	Whether X and Y are independent, given their joint p.d.f. and the question of whether they are independent.
Independence of U and V	Whether U and V are independent, given their joint p.d.f. and the question of whether they are independent.
Conditional p.d.f. of Y1 given Yn = yn	The conditional probability density function (p.d.f.) of Y1 given Yn = yn, where Y1 = min{X1, ..., Xn} and Yn = max{X1, ..., Xn}, given the c.d.f. of X.
P.d.f. of the range of the sample	The probability density function (p.d.f.) of the range of a random sample of three observations from a distribution with p.d.f. f(x) = 2x for 0 < x < 1, 0 otherwise.
Approximation of Pr(y - ǫ < Y ≤ y + ǫ)	An approximate calculation of the probability Pr(y - ǫ < Y ≤ y + ǫ) using the approximation ∫b ar(t)dt ≈ (b - a)r / ((b+a)/2).
Approximation of Pr(X ≤ x and y - ǫ < Y ≤ y + ǫ)	An approximate calculation of the probability Pr(X ≤ x and y - ǫ < Y ≤ y + ǫ) using the approximation ∫b ar(t)dt ≈ (b - a)r / ((b+a)/2).
Ratio of approximations	The ratio of the approximation in part (b) to the approximation in part (a) without explanation.
Joint p.d.f. of X1 and Z	The joint probability density function (p.d.f.) of X1 and Z, where Z = X1 - X2.
Conditional p.d.f. of X1 given Z = 0	The conditional probability density function (p.d.f.) of X1 given Z = 0, given the p.d.f. of X1 and X2.
Joint p.d.f. of X1 and W	The joint probability density function (p.d.f.) of X1 and W, where W = X1 / X2.
Conditional p.d.f. of X1 given W = 1	The conditional probability density function (p.d.f.) of X1 given W = 1, given the p.d.f. of X1 and X2.
Borel paradox	The discrepancy between the conditional distributions of X1 given Z = 0 and X1 given W = 1, despite {Z = 0} = {W = 1}.
O-sets	A set of points (x1, x2) in a two-dimensional space, where |x1 - x2| < ǫ and/or |x1/x2 - 1| < ǫ, representing a specific pattern or structure.
Expected Value	The weighted average of the possible values of a random variable with the weights equal to the probabilities, often denoted as E(X) or MX.
Expected Value	The expected value of a random variable is a weighted average of its possible values, where the weights are determined by the probability density function (p.d.f.) of the variable.
Mean of X	The mean of a random variable X is a measure of its central tendency, defined as the expected value of X.
Distribution	In the context of probability theory, a distribution refers to the set of possible values and their corresponding probabilities of occurrence for a random variable.
Expected Value for a Continuous Distribution	The expected value of a continuous random variable is defined as the weighted average of its possible values, where the weights are determined by the probability density function (p.d.f.) of the variable, and the average is calculated using an integral instead of a sum.
Mean of Bounded Continuous Random Variable	The mean of a bounded continuous random variable is defined as the expected value of the variable, which is calculated using the probability density function (p.d.f.) of the variable and an integral.
Expected Failure Time	The expected failure time is the expected value of a random variable that represents the time until a certain event occurs, in this case, the failure of an appliance.
Mean of a General Continuous Random Variable	The mean, expectation, or expected value of a continuous random variable X, denoted as E(X), is defined as the integral of the product of x and f(x) where f(x) is the probability density function of X, provided at least one of the integrals ∫∞ 0xf(x)dx and ∫0 −∞xf(x)dx is finite. If both integrals are infinite, then E(X) does not exist.
Mean of a Distribution	The mean of a distribution is a measure of the center of the distribution. It is calculated as the weighted average of all possible values of the random variable, where the weights are determined by the probability density function (p.d.f.) of the distribution.
Symmetric Distribution	A distribution is said to be symmetric with respect to a given point x0 on the x-axis if the probability density function (p.d.f.) is equal to the p.d.f. mirroring x0, i.e., f(x0+δ) = f(x0-δ) for all values of δ.
Existence of Mean	The mean of a distribution is said to exist if it is a finite number. In other words, the mean exists if the sum of the product of each possible value of the random variable and its corresponding probability is finite.
Cauchy Distribution	The Cauchy distribution is a probability distribution characterized by a probability density function (p.d.f.) that is symmetric with respect to the point x=0 and has no mean. The p.d.f. of the Cauchy distribution is denoted by Eq. (4.1.7).
Failure Rate	The failure rate is the rate at which appliances fail per year, where failure is defined as the event of an appliance breaking or malfunctioning. The failure rate is modeled as a random variable X, whose mean is used to predict the time to failure.
Time to Failure	Time to failure refers to the length of time an appliance will last before failing or malfunctioning. It is related to the failure rate, and its distribution is modeled using the probability density function (p.d.f.) of the random variable X.
Expectation	A measure of the central tendency of a random variable, calculated by multiplying each possible value by its probability and summing the products.
Law of the Unconscious Statistician	A mathematical result stating that the expectation of a function of a random variable is equal to the integral of the function times the probability density function, if the random variable is continuous, or the sum of the function times the probability mass function, if the random variable is discrete.
Expectation of a Random Variable	A measure of the average value of a random variable, calculated using the probability distribution of the variable.
Probability Density Function (p.d.f.)	A function that describes the probability distribution of a continuous random variable, giving the probability density at each point in the support of the variable.
Probability Mass Function (p.f.)	A function that describes the probability distribution of a discrete random variable, giving the probability of each possible value of the variable.
Expectation	The average value of a random variable, which represents the mean value of its possible outcomes.
Law of the Unconscious Statistician	A phenomenon where people neglect the definition of the mean of a new variable and only consider the definition of the expectation of that variable.
Failure Rate and Time to Failure	The rate at which a system or component fails, typically measured by the time it takes for a failure to occur.
Determining the Expectation of X1/2	The process of calculating the expectation of a random variable that is a function of another random variable.
Linear Function	A function that can be represented by the equation y = ax + b, where a and b are constants.
Option Pricing	The process of determining the value of an option contract, which gives the holder the right to buy or sell an underlying asset at a specified price.
Present Value	The value of a future cash flow or income stream, discounted to its present value using a discount rate.
Expectation of a Random Variable	The expected value, or mean, of a random variable, denoted by E(Y), is a measure of the average value that the random variable is expected to take on, or the long-run average value that the random variable will tend towards.
Law of the Unconscious Statistician (LUS)	The Law of the Unconscious Statistician is a theorem stating that the expected value of a function of multiple random variables, denoted by E(Y), can be determined directly from the joint probability distribution of the variables, without first finding the distribution of the function.
Expectation	The expectation, expected value, or mean of a random variable is a summary of its distribution.
Expected Value	The expected value or mean of a random variable is a summary of its distribution.
Mean	The mean of a random variable is the center of mass of its distribution.
Random Variable	A random variable is a quantity that takes on different values according to a probability distribution.
Joint Probability Density Function (p.d.f.)	The joint p.d.f. of two random variables is a function that gives the probability density at each point in their joint distribution.
Center of Mass	The center of mass of a probability distribution is its mean.
Random Variable	A random variable is a mathematical representation of a set of possible values and their associated probabilities, used to model random phenomena.
Uniform Distribution	The uniform distribution is a probability distribution where every value in a given range has an equal probability of being selected.
Cauchy Distribution	The Cauchy distribution is a continuous probability distribution that has a probability density function that is a scaled version of the Lorentzian function.
Random Sample	A random sample is a set of observations drawn randomly from a population, where each observation is an instance of the variable of interest.
Continuous Distribution	A continuous distribution is a type of probability distribution where the random variable can take on any value within a given range or interval.
Compound Option	A compound option is a financial derivative that gives the holder the right to buy or sell another option at a specified price before its expiration date.
Risk-Neutral Price	The risk-neutral price of a financial derivative is the price that makes the expected value of the derivative zero under the risk-neutral measure.
Risk-Free Portfolio	A risk-free portfolio is a combination of assets and derivatives that has the same expected return as the market portfolio, but is completely uncorrelated with the market.
Option Price	The price of an option to buy or sell a share of stock at a specified price (call or put option) on or before a certain date.
Risk-Neutral Price	The expected price of an option to sell one share of stock at a particular price on a particular date, considering the same distribution and risk-free interest rate as the stock.
Continuous Distribution	A probability distribution for which the probability density function (p.d.f.) is defined for all real numbers.
Linear Function	A mathematical function of the form y = ax + b, where a and b are finite constants, and x is a random variable.
Expectation	A measure of the long-run average value of a random variable, denoted by E(X).
Investor's Net Worth	The gain or loss an investor may incur from an investment, due to changes in the stock price, minus the cost of the investment.
Expected return	The mean of the return on an investment, calculated by multiplying the number of shares purchased by the mean of the random variable representing the rate of return.
Expectation	The expected value of a random variable, denoted as E(X), which represents the long-run average value of the random variable.
Mutually Exclusive Events	Events that cannot occur simultaneously, meaning that if one event occurs, the other event cannot occur.
Independence	A condition where the occurrence of one event does not affect the probability of another event, meaning that the events are not related.
Random Variable	A variable that takes on different values depending on the outcome of a random experiment, often represented by a letter such as X.
Discrete Distribution	A type of probability distribution where the random variable can only take on a finite or countable number of distinct values.
Continuous Distribution	A type of probability distribution where the random variable can take on any value within a given interval or range.
Joint Distribution	A probability distribution that describes the joint behavior of two or more random variables.
Marginal Distribution	A probability distribution that describes the behavior of a single random variable, often obtained by integrating out the other variables in a joint distribution.
Portfolio	A combination of investments, such as stocks or assets, that an investor holds at a given time.
Sampling without Replacement	Sampling without replacement is a process where a fixed number of items are selected from a population, and once an item is selected, it is removed from the population and cannot be selected again.
Mean Return	The mean return of a portfolio is the weighted average of the returns of individual assets, with the weights being the proportion of the portfolio invested in each asset.
Convex Functions	A function is convex if, for every α within the range (0,1), and for every x and y, the value of the function at the point αx + (1-α)y is greater than or equal to α times the value of the function at x plus (1-α) times the value of the function at y.
Jensen's Inequality	Jensen's Inequality states that if a function is convex, and X is a random vector with finite mean, then the expected value of the function of X is greater than or equal to the function of the expected value of X.
Binomial Distribution	A probability distribution that models the number of successes in n independent trials, each with a constant probability of success p.
Expected Value	The average value of a random variable, denoted by E(X), which represents the long-term average outcome of a system or experiment.
Independent Random Variables	Random variables that are not related or correlated with each other, meaning that the outcome of one variable does not affect the outcome of the other variables.
Mean of Binomial Distribution	The expected value of a binomially distributed random variable, denoted by E(X) = np, where n is the number of trials and p is the probability of success in each trial.
Product of Independent Random Variables	The product of multiple independent random variables, denoted by E/parenleftBiggn/productdisplayi=1Xi/parenrightBigg= n/productdisplayi=1E(Xi), where each Xi is a random variable with finite expectation.
Joint Probability Density Function (JPDF)	The joint probability density function f(x1, …, xn) represents the probability distribution of a set of n random variables X1, …, Xn.
Marginal Probability Density Function (MPDF)	The marginal probability density function fidenotes the marginal distribution of a single random variable Xi, where i = 1, …, n, and is derived by integrating the joint probability density function f(x1, …, xn) over all variables except Xi.
Independence of Random Variables	If the random variables X1, …, Xn are independent, the joint probability density function f(x1, …, xn) can be expressed as the product of their marginal probability density functions f(x1), …, f(xn).
Law of Total Expectation	The law of total expectation states that if the random variables X1, …, Xn are independent, then the expectation of the sum of a group of random variables is equal to the sum of their individual expectations, i.e., E(Σ Xi) = Σ E(Xi).
Product of Independent Random Variables	The expectation of the product of a group of independent random variables is not always equal to the product of their individual expectations, unless the random variables are also identically distributed and independent.
Expectation of a Combination of Random Variables	The expectation of a combination of random variables can be calculated using the law of total expectation, provided the random variables are independent.
Filtering Process	A filtration process is a method of removing random proportions of particulates from a sample, resulting in a new proportion of remaining particulates.
Mean of a Random Variable	The mean of a random variable Y is denoted as µ(Y) and represents the expected value of the random variable, i.e., µ(Y) = E(Y).
Integer-Valued Random Variables	Random variables that can take only non-negative integer values, such as 0, 1, 2, ...
Theorem 4.2.7	A theorem that states the relationship between the expected value of an integer-valued random variable and the sum of the probabilities of the variable being greater than or equal to each value.
Nonnegative Distributions	A class of random variables that can take only non-negative values, including integer-valued random variables.
Theorem 4.2.8	A theorem that generalizes Theorem 4.2.7 to apply to all non-negative random variables, with a formula for the expected value in terms of their cumulative distribution function.
Expected Waiting Time	The average time a customer spends waiting for service in a queue, given its cumulative distribution function.
Linear Function of a Random Vector	A function that is a linear combination of the components of a random vector, whose mean is the linear combination of the means of the components.
Mean	The mean of a set of values is the average of those values, often denoted as E(x). It is calculated by summing up all the values and dividing by the number of values.
Product of means	The product of means is a mathematical expression where the mean of one set of values is multiplied by the mean of another set of values or by a constant.
Expected value	The expected value of a random variable is the long-term average value that the variable is expected to take on. It is often denoted as E(x) and is calculated as the sum of the product of each possible value of the variable and its corresponding probability.
Random sample	A random sample is a subset of data chosen from a larger population in such a way that each member of the population has a known non-zero probability of being chosen.
Uniform distribution	The uniform distribution is a probability distribution where each possible value within a given range has an equal probability of occurring.
Independent variables	Two random variables are said to be independent if the probability of one variable taking on a particular value does not depend on the value of the other variable.
Continuous distribution	A continuous distribution is a probability distribution where the random variable can take on any value within a given range, rather than being limited to a countable set of values.
Probability distribution function (p.d.f.)	The probability distribution function (p.d.f.) is a mathematical function that describes the probability of a given value of a continuous random variable occurring.
Probability	Probability is a measure of the likelihood of an event occurring. In statistics, probability is often represented as a number between 0 and 1, where 0 indicates an impossible event and 1 indicates a certain event.
Expectation of the number of observations	The expectation of the number of observations in a sample that fall within a specified interval is a measure of the average number of observations that are expected to occur within that interval.
Gambler's fortune	The gambler's fortune refers to the amount of money or resources possessed by a gambler, which can fluctuate as a result of their wins and losses.
Binomial probability	Binomial probability refers to the probability of a certain number of successes or failures in a fixed number of independent trials, where the probability of success is constant throughout.
Expected number of tosses required	The expected number of times a fair coin must be tossed in order to obtain a specific number of heads, such as k heads.
Expected number of tails required	The expected number of tails that will be obtained before the first head is obtained when a fair coin is tossed repeatedly.
Variance	A measure of how spread out a distribution of a random variable is, defined as the average of the squared differences from the mean.
Variance	The variance of a random variable X, denoted by Var(X), is defined as the expected value of the squared difference between X and its mean, i.e., Var(X) = E[(X - µ)²], where µ is the mean of X.
Standard Deviation	The standard deviation of a random variable X is the non-negative square root of its variance, i.e., it is the square root of Var(X), denoted by σX. If X has infinite mean or if the mean of X does not exist, the standard deviation of X is said to be infinite.
Expectation	The expectation of a random variable X, denoted by E(X), is the average value that X is expected to take, calculated by integrating the probability distribution function (p.d.f.) of X over the entire range of possible values of X.
Standard Deviation of a Discrete Distribution	The standard deviation of a discrete distribution X is a measure of the spread or dispersion of the distribution around its mean µ, calculated as the square root of the variance of X.
Variance of a Discrete Distribution	The variance of a discrete distribution X is a measure of the spread or dispersion of the distribution around its mean µ, calculated as the expected value of the squared difference between each value of X and its mean.
Alternative Method for Calculating the Variance	The alternative method for calculating the variance of a random variable X, which is often easier to use, states that the variance is equal to the expected value of X squared minus the square of the expected value of X.
Expectation	The expectation of a random variable X, denoted as E(X), is a measure of the long-run average value of X, calculated as the sum of the product of each value of X and its corresponding probability.
Standard Deviation	The standard deviation of a distribution is a measure of its spread or dispersion around its mean, calculated as the square root of its variance.
Spread or Dispersion	The spread or dispersion of a distribution around its mean is a measure of how spread out the values of the distribution are from its mean.
Variance	The variance of a random variable X is a measure of the spread or dispersion of the values of X from its mean value. It is calculated as the mean of the squared differences between each value of X and its mean, and is often denoted as Var(X).
Property of Variance	A property of the variance that states that the variance of a random variable X is always non-negative, or Var(X) ≥ 0. If X is a bounded random variable, then the variance not only exists but is also finite.
Theorem 4.3.3	A theorem that states that the variance of a random variable X can only be zero if the entire probability distribution of X is concentrated at a single point. In other words, Var(X) = 0 if and only if there exists a constant c such that Pr(X = c) = 1.
Translation Invariance of Variance	A property of the variance that states that shifting the entire distribution of a random variable X by a constant value b does not affect the variance, or Var(X + b) = Var(X) for every constant b.
Reflection Invariance of Variance	A property of the variance that states that reflecting the distribution of a random variable X around the origin does not affect the variance, or Var(-X) = Var(X).
Reflection of a distribution	The process of creating a new distribution by multiplying a random variable by -1, which changes the sign of the distribution but does not affect its spread.
Linear function	A function that is the result of multiplying a random variable by a constant and then shifting the result by an additive constant.
Variance of a linear function	The measure of the spread of a linear function of a random variable, calculated as the product of the coefficient of the linear function with the variance of the random variable.
Theorem 4.3.4	A theorem stating that the variance of a linear function of a random variable is the square of the coefficient of the linear function multiplied by the variance of the random variable.
Standard deviation	A measure of the spread of a random variable, calculated as the square root of the variance.
Theorem 4.3.5	A theorem stating that the variance of the sum of independent random variables is the sum of the variances of the individual random variables.
Corollary 4.3.1	A corollary stating that the variance of a linear combination of independent random variables is the sum of the products of the coefficients with the variances of the individual random variables.
Efficient Portfolio	A portfolio that provides the highest expected return for a given level of risk, or the lowest risk for a given level of expected return.
Variance	A measure of the distribution's spread or diversity from the mean value, describing how spread out the data is.
Mean Return	The average return on investment, calculated by summing all returns and dividing by the number of returns.
Risk	The possibility of incurring losses or a deviation from expected outcomes, often associated with volatility and uncertainty.
Investment Portfolios	A collection of investments, such as stocks, bonds, or other securities, combined to create a single investment product.
Binomial Distribution	A probability distribution that models the number of successes in a fixed number of independent trials, where each trial has only two outcomes (success or failure).
Binomial Distribution	A probability distribution that models the number of successes (X) in a fixed number (n) of independent Bernoulli trials, where the probability of success (p) is constant for each trial.
Variance of Binomial Distribution	The variance of a binomial distribution is equal to np(1-p), where n is the number of trials and p is the probability of success in each trial.
Interquartile Range (IQR)	A measure of spread that describes the length of the interval that contains the middle half of the data, regardless of whether the distribution has a mean or variance. It is defined as the difference between the 75th percentile and the 25th percentile of the data.
Cauchy Distribution	A type of continuous probability distribution characterized by its probability density function (p.d.f.) F(x) = 1 / (π(1 + y^2)), where y = (x - a) / b, for some constants a and b.
Interquartile Range (IQR)	A measure of spread that is the difference between the 0.75 and 0.25 quantiles, or the point where the distribution reaches its 75th percentile minus the point where it reaches its 25th percentile.
Variance	A measure of the spread or dispersion of a probability distribution, denoted by Var(X), and is defined as the expected value of the squared difference between the random variable and its mean.
Standard Deviation	The square root of the variance, which is a measure of the spread or dispersion of a probability distribution.
Quantile Function	A function F^(-1)(p) that maps a probability p to the value of the random variable X that is exceeded by that probability.
Independent Random Variables	Random variables X and Y are said to be independent if the event that X takes on a certain value does not affect the probability that Y takes on a certain value.
Intervals	The points at which the mean and variance determine the intervals [a1,b1] and [a2,b2].
Value-at-Risk (VaR)	The estimated worst loss of a portfolio at a given probability level, such as 0.97 in this example.
Interquartile Range (IQR)	A measure of the spread of a distribution, specifically the difference between the 75th percentile and the 25th percentile.
Moments	The expected value of a random variable raised to a power, with the mean being the first moment and the variance being the second moment.
Moment Generating Function	A related tool to aid in deriving distributions of sums of independent random variables and limiting properties of distributions.
Existence of Moments	The requirement that the absolute value of a random variable raised to a power be integrable or summable, with the existence of higher-order moments implying the existence of lower-order moments.
Theorem 4.4.1	A mathematical statement that proves that if the kth moment of a random variable exists, then all moments of lower order must also exist.
Central Moments	The expectation E[(X −µ)k] of a random variable X, where k is a positive integer, is called the kth central moment of X or the kth moment of X about the mean.
Skewness	The skewness of a random variable X with mean µ, standard deviation σ, and finite third moment is defined as E[(X −µ)3]/σ3.
Moment Generating Function (MGF)	The Moment Generating Function (MGF) of a random variable X is a function ψ(t) defined as the expected value of e^( tx), and can be denoted as ψ(t) = E(e^(tx)).
Skewness	Skewness is a measure of the asymmetry of a probability distribution, and is calculated as the third moment of the distribution divided by the standard deviation to the power of 3.
Binomial Distribution	A Binomial Distribution is a discrete probability distribution that models the number of successes in a fixed number of independent trials, where each trial has a constant probability of success.
Moment Generating Function (m.g.f.)	A function that encodes all the moments of a random variable, defined as the expected value of the exponential transformation of the random variable.
Exponential Transformation	A mathematical operation that transforms a random variable into an exponential function, often used in the definition of a moment generating function.
Random Variable	A mathematical object that has a set of possible values, called outcomes, and a probability distribution that assigns a probability to each outcome.
Independent Random Variables	Random variables that have no correlation or dependence on each other, meaning that the value of one variable does not affect the value of another variable.
Binomial Distribution	A discrete probability distribution that models the number of successes in a fixed number of independent trials, where the probability of success is constant in each trial.
Expectation	A mathematical concept that measures the average value of a random variable or a function of a random variable, often denoted as E(X).
Variance	A measure of the spread or dispersion of a random variable, often denoted as Var(X), which represents the average of the squared differences from the mean.
Moment Generating Function (m.g.f.)	A function that gives the expected value of e^(tx) for a random variable X, used to characterize the distribution of a random variable.
Binomial Distribution	A discrete probability distribution that models the number of successes in a fixed number of independent trials, each with a constant probability of success.
Theorem 4.4.4	A theorem stating that the moment generating function of the sum of n independent and identically distributed random variables is the product of their individual moment generating functions.
Theorem 4.4.5	A theorem stating that if the moment generating functions of two random variables are identical, then their probability distributions must also be identical.
Theorem 4.4.6	A theorem stating that the sum of two independent binomial random variables with the same second parameter is also a binomial random variable.
Additive Property of the Binomial Distribution	A property of the binomial distribution that states that the sum of two independent binomial random variables with the same second parameter is also a binomial random variable.
Moment Generating Function (m.g.f.)	The moment generating function (m.g.f.) of a random variable X is a function ψ(t) that is defined as the expected value of e^(tX), where X is a random variable.
Moments	Moments of a random variable X are the expected values of X raised to the power of n, for n = 1, 2, 3, ..., where X is a random variable.
Characterization of Distribution	A random variable characterizes its distribution if all random variables that have the same moment generating function (m.g.f.) have the same distribution.
Central Moments	Central moments of a random variable X are the moments of X around its mean, i.e., (X-E(X))^n, where X is a random variable.
Variance	The variance of a random variable X is the expected value of the squared difference between X and its mean, i.e., var(X) = E((X-E(X))^2), where X is a random variable.
Uniform Distribution	A random variable X has a uniform distribution on an interval [a, b] if its probability density function is 1/(b-a) for x in [a, b] and 0 otherwise, where X is a random variable.
Mean	A measure of central location for a probability distribution, which can be regarded as the center of gravity of that distribution.
Median	A measure of central location for a probability distribution, which is a point that divides the total probability into two equal parts (1/2 probability to the left and 1/2 probability to the right).
Moment Generating Function (m.g.f.)	A function that gives the expectation of a function of a random variable, which is an exponential function of the variable, often denoted as ψ(t).
Median	A number m in the distribution of a random variable X is called a median if the probability that X is less than or equal to m is greater than or equal to 1/2, and the probability that X is greater than or equal to m is also greater than or equal to 1/2.
Multiple Medians	Some distributions have multiple medians, where every number in some interval is a median of the distribution. In such cases, the 1/2 quantile is the minimum of the set of all medians.
Median	A measure of the middle of a distribution, based on the middle value(s) in the dataset; it is often used to summarize the "middle" of a distribution.
Annual Incomes	The amount of money a family earns in a year, with a certain community in the example having a mean annual income of $30,000.
Median	A value in a dataset where at least half of the values are the same or less, and at least half are the same or more (in the example, at least one-half of the families must have incomes of $30,000 or more).
One-to-One Function	A function that maps each input to a unique output, and is either increasing or decreasing over a specified interval (in the theorem, a one-to-one function is defined on an interval I).
Median	A resistant measure of central tendency, which has the convenient property that at least one-half of the values are the same or more (in the theorem, the median of a random variable X is used).
Mean Squared Error (M.S.E.)	The expected value of the square of the error between an observed value and a predicted value (in the theorem, the M.S.E. is used to select the optimal prediction of a random variable X).
Prediction	An estimate or forecast of a value in an experiment or situation (in the text, the value of a random variable X is to be predicted before it can be observed).
Variance	A measure of the spread or dispersion of a random variable, defined as the expected value of the square of the deviation from the mean.
Mean Absolute Error	The mean absolute error (M.A.E.) is the average difference between a predicted value and the actual value of a random variable, calculated as E(|X−d|).
Median	A median is a value of a distribution of a random variable such that at least half of the data points are less than or equal to that value, and at least half are greater than or equal to that value.
Discrete Uniform Distribution	A probability distribution where each of the discrete values has an equal probability of occurrence, where the probability is equally distributed over a finite range of values.
Median	A value that separates the higher half of a dataset from the lower half, such that at least half of the data points are below the median and at least half are above it.
Quantile	A value that divides a dataset into equal parts, such that a certain percentage of the data points fall below the quantile and the remaining percentage fall above it.
Discrete distribution	A probability distribution that can only take on specific, distinct values, with a probability mass function (PMF) that assigns a non-zero probability to each value.
Continuous distribution	A probability distribution that can take on any value within a given range, with a probability density function (PDF) that assigns a probability to each point in the range.
Mean	The average value of a dataset, calculated as the sum of the values divided by the number of observations.
Median (as predicted value)	The middle value of a dataset when it is sorted in order, which is used as a prediction for new observations.
symmetric distribution	A probability distribution that is unchanged when reflected about a particular point, in this case 0.
Prediction (MSE or MAE)	A value that aims to minimize the difference between the predicted and actual values of a random variable, where MSE stands for Mean Squared Error and MAE stands for Mean Absolute Error.
Expected value	The long-run average value of a random variable, calculated as the sum of the possible values multiplied by their probabilities.
Probability density function (PDF)	A function that describes the probability distribution of a continuous random variable, with the PDF values summing up to 1 over the entire range.
Distance problem	A mathematical problem that aims to find the point along a straight road where a store should be located to minimize the sum of distances from n houses to the store.
Binomial distribution	A probability distribution that models the number of successes in a fixed number of independent trials, where each trial has a constant probability of success.
Mean Squared Error (MSE)	A measure of the average squared difference between a predicted value and its actual value in a regression problem.
Mean Absolute Error (MAE)	A measure of the average absolute difference between a predicted value and its actual value in a regression problem.
Median	The middle value of a dataset when it is sorted in order; 50% of the values are below the median and 50% are above.
Cauchy distribution	A continuous probability distribution that has a tail that decays slowly, resulting in a large number of extreme values.
Cumulative Distribution Function (c.d.f.)	A function that describes the probability that a random variable takes on a value less than or equal to a given value.
Quantile	A value that divides a dataset into two halves, with a certain percentage of values below it and the remaining percentage above it.
Covariance	A measure of the variation in the relationship between two random variables, describing how much they tend to move together.
Covariance	The covariance of two random variables X and Y (denoted by Cov(X,Y)) is a measure of their joint distribution that reflects the degree to which they tend to vary together. It is defined as Cov(X,Y) = E[(X - µX)(Y - µY)], where µX and µY are the means of X and Y, respectively.
Correlation	(Implicit) Although not explicitly defined in the provided text, the concept of correlation is often associated with the covariance of two random variables. Correlation measures the strength and direction of the linear relationship between two variables.
Covariance calculation	The calculation of the covariance of two random variables involves evaluating the expectation of the product of their deviations from their means. For example, the covariance Cov(X,Y) = E[(X - µX)(Y - µY)] can be computed using the joint probability density function (p.d.f.) of the variables.
Theorem 4.6.1	Theorem 4.6.1 states that for random variables X and Y with finite variances, the covariance Cov(X,Y) can be calculated using the formula Cov(X,Y) = E(XY) - E(X)E(Y). This result simplifies the calculation of covariance.
Covariance	Covariance between two random variables X and Y is a measure of the degree to which they tend to be large or small at the same time, and is defined as Cov(X,Y) = E(XY) - µX E(Y) - µY E(X) + µX µY.
Correlation	Correlation between two random variables X and Y is a measure of association between them that is not driven by arbitrary changes in the scales of one or the other random variable, and is defined as ρ(X,Y) = Cov(X,Y) / (σXσY).
Cauchy-Schwarz Inequality	A mathematical theorem stating that [Cov(X,Y)]^2 ≤ σ^2_X * σ^2_Y, where X and Y are random variables with finite variance.
Covariance	A measure of the linear relationship between two random variables X and Y, defined as Cov(X,Y) = E[(X-μ_X)(Y-μ_Y)].
Correlation	A measure of the linear relationship between two random variables X and Y, defined as ρ(X,Y) = Cov(X,Y) / (σ_X * σ_Y), where σ_X and σ_Y are the standard deviations of X and Y, respectively. It ranges between -1 (perfect negative correlation) and 1 (perfect positive correlation), with 0 indicating no correlation.
Positively/Negatively Correlated/Uncorrelated	A definition stating that two random variables X and Y are positively correlated if ρ(X,Y) > 0, negatively correlated if ρ(X,Y) < 0, and uncorrelated if ρ(X,Y) = 0.
Unrelated Random Variables	Although dependent random variables can be uncorrelated, meaning that the covariance between them is zero, they are not necessarily unrelated. Two random variables can be dependent and correlated or independent and uncorrelated.
Covariance	A measure of how two random variables vary in relation to each other. It is defined as the expected value of the product of their deviations from their respective means.
Covariance	The covariance of two random variables X and Y, which is the expected value of the product of their deviations from their means, calculated as E{[X−E(X)][ Y−E(Y)]}.
Correlation	A measure of the linear relationship between two random variables X and Y, calculated as the covariance of X and Y divided by the product of their variances, denoted as ρ(X,Y) = Cov(X,Y)/[Var(X)Var(Y)]1/2.
Uncorrelated Random Variables	Random variables that have a zero covariance, meaning that their deviations from their means are not related.
Independent Random Variables	Random variables that have zero covariance and are also statistically independent, meaning that the probability of one event does not affect the probability of another event.
Theorem 4.6.7	A theorem that states that the variance of a sum of random variables can be expressed as the sum of the variances plus two times the sum of their covariances.
Corollary 4.6.2	A corollary to Theorem 4.6.7 that states that if the random variables are uncorrelated, then the variance of the sum is equal to the sum of the variances.
Variance of a Sum	The sum of the variances of random variables plus two times the sum of their covariances, which is calculated using Theorem 4.6.7.
Covariances	The covariance between two random variables X and Y is a measure of how they change together, represented as Cov(X, Y) = E[(X - E(X))(Y - E(Y))].
Var (X 1+...+Xn)	The sum of multiple random variables X1 to Xn.
Correlation (-1)	The correlation coefficient between two random variables R1 and R2, which is equal to -1.
Conditional Mean	The expected value of a random variable Y given a value of X, denoted by E(Y|x).
Conditional Expectation	The expectation of the conditional distribution of Y given X=x.
Law of Total Probability	A theorem that extends the concept of probability to expectations, allowing for the prediction of one random variable using a function of another random variable.
Law of Total Variance	Not explicitly defined, but implied to be the extension of the law of total probability to variances.
Covariance (Cov(X,Y))	A measure of the covariance between two random variables X and Y.
Portfolio	An investment strategy that combines different assets (such as stocks) to achieve a desired return or risk profile.
Conditional Expectation	The expected value of a random variable Y given a value x of a related random variable X.
Conditional Means as Random Variables	A concept where a function of X is computed before X is observed, leading to the idea of E(Y |X) as a random variable, denoted as the conditional mean of Y given X.
Conditional Mean	A random variable that represents the expected value of Y given a specific value of X, denoted as E(Y |x).
Expectation	The concept of expectation represents the long-term average or expected value of a random variable or a function of a random variable, often denoted as E(X) or E[Y | X].
Conditional Mean	The conditional mean of a random variable Y given a random variable X is the expected value of Y when X is fixed, denoted as E(Y | X).
Law of Total Probability for Expectations	The Law of Total Probability for Expectations, stated by Theorem 4.7.1, asserts that the mean of the random variable E(Y | X) is equal to the mean of Y, formally written as E[E(Y | X)] = E(Y).
Conditional Expectation	The expected value of a random variable Y given a random variable X, denoted as E(Y | X), is a measure of the expected value of Y when X takes on a specific value.
Marginal Expected Value	The expected value of a conditional expectation E(X | P) is called the marginal expected value, which is the average of the conditional expected values over all possible values of the conditioning random variable P.
Conditional Mean	The expected value of a random variable X given a random variable P, denoted as E(X | P), represents the average value of X when P takes on a specific value.
Joint Distribution	A joint distribution is a probability distribution that assigns a probability to each possible pair of values of the random variables X and Y.
Conditional Distribution	The conditional distribution of a random variable Z given X = x, denoted as P(Z = z | X = x), is a probability distribution that describes the distribution of Z when X takes on a specific value x.
Theorem 4.7.2	This theorem states that the conditional distribution of Z given X = x is the same as the conditional distribution of r(x, Y) given X = x, where r is a function and X and Y are random variables.
Theorem 4.7.1	This theorem implies that E{E[Z | X]} = E[Z] for two arbitrary random variables X and Y.
Conditional Expectation	A conditional expectation is a random variable E(Y |X) that represents the expected value of Y given that X is fixed or conditioned on X.
Conditional Variance	A conditional variance is a measure of the spread or dispersion of the conditional distribution of Y given that X is fixed, denoted as Var (Y |x) or simply Var (Y |X).
Prediction	Prediction is the process of finding a predicted value of Y, denoted as d(X), based on the observed value of X, with the goal of minimizing the mean squared error E{[Y−d(X)]2}.
Theorem 4.7.3 (Prediction)	The theorem states that the prediction d(X) that minimizes the mean squared error E{[Y−d(X)]2} is equal to the conditional expectation of Y given X, i.e., d(X) = E(Y |X).
Mean Squared Error (MSE)	The expected value of the squared difference between a predicted value and the actual value.
Conditional Distribution	The probability distribution of a random variable Y given a specific value of another random variable X, denoted as P(Y | X).
Law of Total Probability for Variances	A mathematical result stating that the total variance of a random variable Y is equal to the expected value of its conditional variance given X plus the variance of its conditional expectation given X (Var(Y) = E[Var(Y | X)] + Var[E(Y | X)]).
Conditional Mean Squared Error	The expected value of the squared difference between a predicted value and the actual value, conditioned on a specific value of X.
Total Mean Squared Error	The expected value of the squared difference between a predicted value and the actual value, averaged over all possible values of X.
Conditional Variance	The variance of a random variable Y given a specific value of X.
Overall Mean Squared Error	The expected value of the squared difference between a predicted value and the actual value, considering all observations of X.
Reduction in Mean Squared Error	The difference between the total mean squared error and the conditional mean squared error, representing the reduction in error due to the use of additional information about X.
Conditional Mean	The expected value of a random variable conditional on the value of another random variable.
Conditional Probability Distribution	The probability distribution of a random variable given the value of another random variable.
Bayes' Theorem	A formula for updating the probability distribution of a random variable based on new information or evidence.
Inverse Formula for Beta Distribution	A formula for computing the probability distribution of a beta random variable, given as Eq. (4.7.11) in the text.
Conditional Variance	The variance of a random variable conditional on the value of another random variable.
Conditional Expectation	The expected value of a random variable Y given a specific value X, represented as E(Y|x).
Conditional Variance	The variance of a random variable Y given a specific value X, represented as Var(Y|x).
Mean Squared Error (MSE)	The average squared difference between the predicted and actual values of a random variable.
Mean Absolute Error (MAE)	The average absolute difference between the predicted and actual values of a random variable.
Median	The middle value of a dataset when it is sorted in order, where half the values are above and half are below the median.
Law of Total Probability	The expected value of a random variable Y can be calculated by taking the weighted average of the expected values of Y given different values of X.
What is the term for outcomes in an event B that also belong to an event A?	No image available
What are the intersections of set A with events B1 through B5 in the context of a partition in a proof?	No image available
Conditional Probability	The revised probability of an event A after learning that another event B has occurred, considering the set of outcomes in B that also result in the occurrence of A.
Conditional Probability	The probability of an event A occurring given that event B has occurred, denoted as Pr(A | B). It is computed as the proportion of the total probability Pr(B) represented by Pr(A ∩ B), intuitively the proportion of B that is also part of A.
Probability of the Event Given that the Event has Occurred	A probability value that represents the proportion of repetitions in an experimental process where event B occurs, given that event B has occurred.
Frequency Interpretation of Conditional Probability	According to this interpretation, if an experimental process is repeated a large number of times, the proportion of repetitions in which event B occurs is approximately Pr(B), and the proportion of repetitions in which both events A and B occur is approximately Pr(A ∩ B). Therefore, among those repetitions in which event B occurs, the proportion of repetitions in which event A also occurs is approximately equal to Pr(A | B) = Pr(A ∩ B) / Pr(B).
Conditional Probability	The probability of an event occurring given that another event has occurred, denoted as P(A | B) and read as "the probability of A given B".
Event	A set of outcomes from a sample space of a random experiment.
Independence	Two events are independent if the occurrence of one event does not affect the probability of another event.
Intersection	The set of outcomes which belong to at least two events, denoted as A ∩ B.
Probability	A measure of the likelihood of an event occurring, denoted as P(A) and ranging from 0 to 1.
Multiplication Rule for Conditional Probabilities	A theorem that states that if Pr(B) > 0, then Pr(A ∩ B) = Pr(B) × Pr(A | B), and if Pr(A) > 0, then Pr(A ∩ B) = Pr(A) × Pr(B | A), allowing for the calculation of the probability that two events occur together.
Conditional Probability	The probability of an event occurring given that another event has occurred.
Partition	A set of k events B1, ..., Bk in a sample space S such that B1, ..., Bk are disjoint and their union k i=1 Bi=S, where each event reduces uncertainty in the problem if it is determined which event has occurred.
Law of Total Probability	A theorem stating that if the events B1, ..., Bk form a partition of the sample space S and Pr(Bj) > 0 for j=1, ..., k, then for every event A in S, Pr(A) = summationdisplay j=1 Pr(Bj)Pr(A|BJ).
Conditional Probability	The probability of an event A occurring given that event B has occurred, denoted as Pr(A|B).
Conditional Probability	The probability of an event occurring given that another event has occurred. It is denoted as Pr(A|B) and is calculated as the probability of A and B occurring together divided by the probability of B.
Composition of a collection of patients	The different possible proportions of successful outcomes among patients who might receive a treatment, such as a depression study.
Uncertainty in patient composition	The unknown proportion of successful outcomes among patients who might receive a treatment, which introduces uncertainty in the treatment's effectiveness.
Event Bj	The event that the sample was drawn from a collection with a specific proportion of successes, denoted by (j - 1)/10.
Event Ej	The event that the jth patient in the imipramine group has a success.
Prior probability	The initial probability believed to exist before starting the trial, which is assumed to be 1/11 for each possible composition of patients.
Augmented Experiment	An experiment that can be augmented to include the potential or hypothetical observation of as much additional information as needed to help calculate any desired probabilities.
Event	A set of outcomes in which the limit of a proportion or value is a specific value (e.g., 1/10, 2/10, etc.).
Augmented Experiment	An experiment that includes extra information or outcomes beyond what is typically observed, allowing for the determination of additional quantities (e.g., p in Example 2.1.13).
Independent Events	Events in which the outcome of one event does not affect the outcome of another event (not explicitly mentioned in this text, but implied by the concept of conditional probability).
Conditional Probability	The probability of an event occurring based on the occurrence of another event or set of events (e.g., Pr(W|Bi) in the text).
Craps	A popular gambling game played with two dice, where the sum of the two numbers determines the outcome of the game.
Sample Space	The set of all possible outcomes or sequences of outcomes in an experiment (e.g., all possible sequences of sums from the rolls of dice).
Event Partition	The division of an event into smaller, more manageable subsets based on a specific characteristic or attribute (e.g., the sum on the first roll in the text).
Theorem 2.1.4	A mathematical theorem providing a formula for computing the probability of an event in terms of conditional probabilities and the probabilities of events in a partition (not explicitly defined in this text).
Conditional Probability	The probability of an event A occurring given that event B has occurred, denoted by Pr(A|B), computed as Pr(A ∩ B) / Pr(B).
Multiplication Rule for Conditional Probabilities	A rule for computing the probability of two events occurring given a third event, denoted by Pr(A ∩ B) = Pr(B) Pr(A |B).
Partition	A collection of disjoint events whose union is the whole sample space, chosen so that an important source of uncertainty is reduced if we learn which one of the partition events occurs.
Law of Total Probability	A rule for combining the conditional probabilities of an event A given each event in a partition to get Pr(A).
Independent Events	Two or more events that do not affect each other's probability of occurring.
Brand A and Brand B	Two distinct brands or options.
Random Selection	The process of choosing an item or outcome randomly and without bias from a set or population.
Conditional Probability	The probability of an event occurring given that another event has occurred.
Box Model	A statistical model where a box contains multiple items, and items are selected and returned to the box to analyze probabilities.
Event Intersection	The intersection of two or more events, where the occurrence of one event affects the probability of the other event(s).
Inclusive Events	Events that occur at least once, such as at least one of three events occurring.
Exclusive Events	Events that do not occur simultaneously, such as selecting two distinct cards from a set.
Rolling Dice	The process of randomly generating numbers using multiple dice, often used in game theory and probability analysis.
Tree Diagram	A visual representation of conditional probabilities and events, used to analyze and understand complex probability scenarios.
Bayes' Theorem	A mathematical formula used to update the probability of an event based on new information or evidence.
Union of Events	The combination of two or more events, where the occurrence of one event does not affect the probability of the others.
Conditional Probability	The likelihood of an event occurring given that another event has occurred. In this case, Pr(A ∩ B|D) represents the probability of event A occurring given that event B has occurred and event D is true.
Probability of a Head	The likelihood of obtaining a head when a coin is tossed randomly once from a box containing coins with a head on each side. The probability is calculated by dividing the number of coins with heads by the total number of coins.
Compound Probability	The probability of the machine producing a defective part given its state of repair. It involves calculating the probability of individual events and combining them to find the total probability.
Probability of Defective Parts	The likelihood of a randomly selected part being defective given the machine's state of repair. It involves calculating the probability of the machine being in good working order, wearing down, or needing maintenance, and then combining these probabilities to find the total probability of production of defective parts.
What is the probability of an event occurring given that another specific event has occurred?	No image available
What is the meaning of the intersection of a set A with events B1 to B5 in a partition, as depicted in the proof of a mathematical theorem?	No image available
Conditional Probability	The updated probability of an event A after learning that event B has occurred, considering the set of outcomes in B that also result in the occurrence of A.
Definition of Conditional Probability	The concept of updating probabilities when certain events are observed, considering the event that has occurred, and calculating the new probability of another event taking into account that information.
Independent Events	Events that can occur independently of each other, meaning that the occurrence of one event does not affect the probability of the other event occurring.
Bayes' Theorem	A mathematical formula used to update conditional probability information based on new data, by combining prior knowledge with new data to produce a revised probability estimate.
Gambler's Ruin Problem	A problem in probability that involves calculating the probability of a gambler's fortune reaching a certain point or running out of funds, given a set of conditions.
Supplementary Exercises	Additional exercises or problems that provide further practice and challenge in applying conditional probability concepts.
Conditional Probability	The probability of an event A given that event B has occurred, denoted as Pr(A|B), which is computed as the proportion of the total probability Pr(B) that is represented by Pr(A∩B), or the proportion of B that is also part of A.
Conditional Probability	The probability of an event A occurring given that another event B has occurred, denoted as Pr(A|B). It is calculated as the probability of both events occurring, Pr(A∩B), divided by the probability of event B, Pr(B).
Event	A set of outcomes from a sample space, denoted as A, B, or C, that may or may not occur.
Probability	A measure of the likelihood of an event occurring, denoted as Pr(A) or Pr(B), and calculated as the number of favorable outcomes divided by the total number of possible outcomes.
Intersection (Set Theory)	The set of outcomes that are common to two or more events, denoted as A∩B.
Placebo	A treatment that is intended to have no therapeutic effect, administered as a control in a clinical trial to compare its effects with those of a real treatment.
Clinical Trial	A study in which a new treatment or intervention is tested in a controlled environment to assess its safety and effectiveness, typically involving a control group and an experimental group.
Multiplication Rule for Conditional Probabilities	A rule stating that the probability of two events occurring (P(A ∩ B)) is equal to the probability of one event (P(B)) times the conditional probability of the other event given the first event (P(A | B)) or vice versa, as long as the probability of the event is not zero.
Conditional Probability	The probability of an event occurring given that another event has occurred, denoted as P(A|B).
The Multiplication Rule	A rule for calculating the probability of the intersection of multiple events, denoted as P(A1∩A2∩...∩An), which states that P(A1∩A2∩...∩An) = P(A1)P(A2|A1)P(A3|A1∩A2)...P(An|A1∩A2∩...∩An-1).
Conditional Independence	A property of two events A and B, where P(A|C) = P(A|C∩B) for all events C, meaning that the conditionality of A with respect to C is equivalent to the conditionality of A with respect to C∩B.
Theorem 2.1.3	A theorem stating that if A1, A2, ..., An, B are events such that P(B) > 0 and P(A1∩A2∩...∩An-1|B) > 0, then P(A1∩A2∩...∩An|B) = P(A1|B)P(A2|A1∩B)...P(An|A1∩A2∩...∩An-1∩B).
Partition	A set of disjoint events B1, ..., Bk that partition the sample space S, such that their union is the entire sample space (B1 ∪ ... ∪ Bk = S).
Law of Total Probability	A theorem that states that the probability of an event A in a sample space S can be calculated as the sum of the probabilities of A occurring given each event Bj in a partition of S, weighted by the probability of Bj.
Conditional Version of Law of Total Probability	The law of total probability has an analog conditional on another event C, which states that Pr(A |C) = k/summationdisplayj=1Pr(Bj|C)Pr(A|Bj∩C), where k is a normalizing constant.
Augmented Experiment	An experiment in which the initial description does not clearly show a partition that will facilitate the calculation of probabilities, but may be modified to include additional structure to facilitate probability calculations.
Mixed Experiment	An experiment where a single outcome is possible, but the outcome can be classified into different categories or sub-events, and each sub-event has a probability of occurring.
Bayes' Theorem	The probability of an event conditional on another event, where the probability of the conditioned event is initially unknown or uncertain.
Event Bj	An event that represents the collection of all available patients being drawn from a collection with proportion (j−1)/10 of successes, or equivalently, having probability p=(j−1)/10.
Event Bj	An event that represents the event {p=(j−1)/10}.
Event E1	An event that represents the first patient in the imipramine group having a success.
Conditional Probability	The probability of an event occurring given that another event has occurred, denoted as Pr(E1|Bj).
Prior Probability	The probability believed to be true before any new evidence is taken into account, denoted as Pr(Bj) in this example.
Sample Space	The set of all possible outcomes or states of a random experiment, which is partitioned into mutually exclusive and exhaustive events in this example.
Unknown Composition	The state of not knowing the proportion of successes among all patients who might receive a treatment, leading to uncertainty about the patients in a sample.
Augmented Experiment	An experiment that can be expanded to include the potential or hypothetical observation of as much additional information as is necessary to help calculate any desired probabilities.
Sample Space	The set of all possible outcomes or states in an experiment.
Event	A set of outcomes or states in a sample space that satisfies a particular condition or rule.
Proportion of Successes	The ratio of successful outcomes to the total number of outcomes in an experiment.
Conditional Probability	The probability of an event occurring, given that another event has already occurred.
Conditional Probability	The probability of an event A occurring, given that another event B has occurred, denoted by Pr(A | B). This is calculated as Pr(A ∩ B) / Pr(B) or as Pr(B) Pr(A | B) using the multiplication rule for conditional probabilities.
Brand A	One of the brands being considered, where the probability of a purchase or outcome is described in terms of this brand.
Independent Events	Events where the occurrence of one event does not affect the probability of the other event, as seen in exercises 5 and 6, where balls are selected randomly and without replacement.
Conditional Probability	The probability of an event occurring given that another event has occurred, as seen in exercises 7-9, where the probability of an event is conditional on the occurrence of another event.
Craps (Game)	A game of chance involving the roll of two dice, where the probability of winning or losing is described in exercise 10.
Probability Theory (Formula)	A formula or theorem describing the relationship between events and probabilities, as seen in exercises 11 and 12, where the probability of an event is calculated or described in terms of other events.
Independent Events	Events that occur independently and are not affected by each other's occurrence or non-occurrence.
Conditional Probability	The probability of an event occurring given that another event has occurred.
Coin Flip	A random experiment in which a coin is tossed and lands on either heads or tails.
Probability	The expected value or outcome of an event, often represented as a numerical value between 0 (impossible) and 1 (certain).
Compound Event	An event that is the combination of multiple simpler events.
Machine Probability	The probability of a machine producing defective parts, which varies depending on its state of repair.
Random Event	An event that has multiple possible outcomes and is not predictable, often represented by chance or probability.
What are the outcomes common to both Event A and Event B?	No image available
What is the typical representation of a partition in the context of a mathematical theorem?	No image available
